{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2ed10a-10d1-4734-bbdf-1d54a99fd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTHON\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pyspark\n",
    "\n",
    "#SPARK\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3096f2-39bc-4025-9c2c-a216255bdf3f",
   "metadata": {},
   "source": [
    "Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb6d6aa-4b8d-4c65-8dc1-a61c5d94018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/01 11:00:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Spark Session - 2 CORE - imporvement CONFIGURATION!!\n",
    "spark_session = SparkSession.builder\\\n",
    "    .master(\"local[2]\")\\\n",
    "    .appName(\"pseudo_spark_nora\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"INFO\")\n",
    "\n",
    "sqlContext = SQLContext(spark_session.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44667861-4446-4677-adf9-5acf17142cc4",
   "metadata": {},
   "source": [
    "File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b6e7bf-1a3a-4aab-af97-04bad4192b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_roots = ['hdfs://130.238.29.183:9000//input/millionsongsubset.tar.gz']\n",
    "directory_path = \"data/B/A/A\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333b60e-ef05-446a-86ff-7c0cbcd0a7c7",
   "metadata": {},
   "source": [
    "# 1) Saving the paths to the HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b92eef-ad69-49de-8bb6-fc9907ff326f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 filenames gathered (PYTHON)\n"
     ]
    }
   ],
   "source": [
    "# GET .H5 FILENAMES\n",
    "h5_file_paths = []\n",
    "for root, _, files in os.walk(directory_path):\n",
    "    for name in files:\n",
    "        h5_file_paths.append(os.path.join(root, name))\n",
    "\n",
    "print('{} filenames gathered (PYTHON)'.format(len(h5_file_paths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa39ddd-8486-42f6-86fd-dc7741270519",
   "metadata": {},
   "source": [
    "# 2) List h5_file_paths is converted to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a776fd3f-ad71-4b9e-96a0-0d37cc0aad7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/B/A/A/TRBAASG128F92FBB7C.h5',\n",
       " 'data/B/A/A/TRBAAEJ128F4263AB7.h5',\n",
       " 'data/B/A/A/TRBAAUV128F429553C.h5',\n",
       " 'data/B/A/A/TRBAAGQ128F148D17E.h5',\n",
       " 'data/B/A/A/TRBAAOC128F42A6BEA.h5',\n",
       " 'data/B/A/A/TRBAAOT128F4261A18.h5',\n",
       " 'data/B/A/A/TRBAADN128F426B7A4.h5',\n",
       " 'data/B/A/A/TRBAAXL12903CE78EE.h5',\n",
       " 'data/B/A/A/TRBAAJQ128F4273A42.h5']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd1391e3-6cfe-4623-bc8e-d21ac0212b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[12] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = spark_context.parallelize(h5_file_paths)\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc5b8a3d-e994-4f86-8feb-e66f5105e5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 11:15:46 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/01 11:15:46 INFO DAGScheduler: Got job 11 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/01 11:15:46 INFO DAGScheduler: Final stage: ResultStage 11 (runJob at PythonRDD.scala:181)\n",
      "24/03/01 11:15:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/01 11:15:46 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/01 11:15:46 INFO DAGScheduler: Submitting ResultStage 11 (PythonRDD[13] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/01 11:15:47 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 5.8 KiB, free 413.9 MiB)\n",
      "24/03/01 11:15:47 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 413.9 MiB)\n",
      "24/03/01 11:15:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on host-192-168-2-34-de1:36397 (size: 3.8 KiB, free: 413.9 MiB)\n",
      "24/03/01 11:15:47 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/01 11:15:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (PythonRDD[13] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/01 11:15:47 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "24/03/01 11:15:47 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (host-192-168-2-34-de1, executor driver, partition 0, PROCESS_LOCAL, 7761 bytes) \n",
      "24/03/01 11:15:47 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)\n",
      "24/03/01 11:15:47 INFO PythonRunner: Times: total = 14, boot = 10, init = 4, finish = 0\n",
      "24/03/01 11:15:47 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 1399 bytes result sent to driver\n",
      "24/03/01 11:15:47 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 33 ms on host-192-168-2-34-de1 (executor driver) (1/1)\n",
      "24/03/01 11:15:47 INFO DAGScheduler: ResultStage 11 (runJob at PythonRDD.scala:181) finished in 0.071 s\n",
      "24/03/01 11:15:47 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/01 11:15:47 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/03/01 11:15:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/03/01 11:15:47 INFO DAGScheduler: Job 11 finished: runJob at PythonRDD.scala:181, took 0.088696 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/B/A/A/TRBAASG128F92FBB7C.h5'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 11:15:47 INFO BlockManagerInfo: Removed broadcast_10_piece0 on host-192-168-2-34-de1:36397 in memory (size: 4.5 KiB, free: 413.9 MiB)\n"
     ]
    }
   ],
   "source": [
    "file_first = file_paths.first()\n",
    "file_first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f78bb-e817-474d-acf5-c0c33bed8740",
   "metadata": {},
   "source": [
    "# 3) H5Py file object file creation from the H5 file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b6da36c-82e1-4874-af26-f0d9bcbb5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(file_path):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        # Informationen extrahieren\n",
    "        song = file['analysis']['songs']\n",
    "        np_song = np.array(song[0])\n",
    "        selected_features = np_song[['danceability', 'duration', 'tempo', 'energy', 'loudness']].ravel()\n",
    "        return selected_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1ffc9696-8ccd-476e-8fe4-867491f32e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = h5py.File(file_first)\n",
    "files_infos_rdd = file_paths.map(extract_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7e29e487-ef1d-46c0-829f-708203dafb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 11:33:23 INFO SparkContext: Starting job: collect at /tmp/ipykernel_53857/3743736607.py:1\n",
      "24/03/01 11:33:23 INFO DAGScheduler: Got job 28 (collect at /tmp/ipykernel_53857/3743736607.py:1) with 2 output partitions\n",
      "24/03/01 11:33:23 INFO DAGScheduler: Final stage: ResultStage 28 (collect at /tmp/ipykernel_53857/3743736607.py:1)\n",
      "24/03/01 11:33:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/01 11:33:23 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/01 11:33:23 INFO DAGScheduler: Submitting ResultStage 28 (PythonRDD[48] at collect at /tmp/ipykernel_53857/3743736607.py:1), which has no missing parents\n",
      "24/03/01 11:33:23 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.9 KiB, free 413.9 MiB)\n",
      "24/03/01 11:33:23 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 413.9 MiB)\n",
      "24/03/01 11:33:23 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on host-192-168-2-34-de1:36397 (size: 4.0 KiB, free: 413.9 MiB)\n",
      "24/03/01 11:33:23 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/01 11:33:23 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 28 (PythonRDD[48] at collect at /tmp/ipykernel_53857/3743736607.py:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/01 11:33:23 INFO TaskSchedulerImpl: Adding task set 28.0 with 2 tasks resource profile 0\n",
      "24/03/01 11:33:23 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 29) (host-192-168-2-34-de1, executor driver, partition 0, PROCESS_LOCAL, 7761 bytes) \n",
      "24/03/01 11:33:23 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 30) (host-192-168-2-34-de1, executor driver, partition 1, PROCESS_LOCAL, 7821 bytes) \n",
      "24/03/01 11:33:23 INFO Executor: Running task 0.0 in stage 28.0 (TID 29)\n",
      "24/03/01 11:33:23 INFO Executor: Running task 1.0 in stage 28.0 (TID 30)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0.0, 210.75546, 150.03, 0.0, -4.188)], [(0.0, 319.4771, 96.031, 0.0, -11.519)], [(0.0, 207.20281, 100.175, 0.0, -14.233)], [(0.0, 243.17342, 152.468, 0.0, -6.149)], [(0.0, 85.96853, 110.064, 0.0, -19.67)], [(0.0, 364.64281, 60.882, 0.0, -17.67)], [(0.0, 251.76771, 127.926, 0.0, -2.62)], [(0.0, 116.1922, 103.971, 0.0, -14.616)], [(0.0, 343.562, 131.991, 0.0, -10.751)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 11:33:23 INFO PythonRunner: Times: total = 158, boot = 29, init = 100, finish = 29\n",
      "24/03/01 11:33:23 INFO Executor: Finished task 0.0 in stage 28.0 (TID 29). 1579 bytes result sent to driver\n",
      "24/03/01 11:33:23 INFO PythonRunner: Times: total = 176, boot = 23, init = 117, finish = 36\n",
      "24/03/01 11:33:23 INFO Executor: Finished task 1.0 in stage 28.0 (TID 30). 1631 bytes result sent to driver\n",
      "24/03/01 11:33:23 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 30) in 192 ms on host-192-168-2-34-de1 (executor driver) (1/2)\n",
      "24/03/01 11:33:23 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 29) in 201 ms on host-192-168-2-34-de1 (executor driver) (2/2)\n",
      "24/03/01 11:33:23 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "24/03/01 11:33:23 INFO DAGScheduler: ResultStage 28 (collect at /tmp/ipykernel_53857/3743736607.py:1) finished in 0.225 s\n",
      "24/03/01 11:33:23 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/01 11:33:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "24/03/01 11:33:23 INFO DAGScheduler: Job 28 finished: collect at /tmp/ipykernel_53857/3743736607.py:1, took 0.237895 s\n"
     ]
    }
   ],
   "source": [
    "print(files_infos_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897df25a-b182-41a3-92a1-f61f9492ddbe",
   "metadata": {},
   "source": [
    "# 4) Create Spark Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb6348bf-0f3e-455e-af23-1bf886a2e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere das Schema für den DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"danceability\", FloatType(), True),\n",
    "    StructField(\"duration\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"energy\", FloatType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e51be1c-caa7-4443-8ecc-e0dece7f6603",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = sqlContext.createDataFrame(files_infos_rdd.flatMap(lambda x: x), schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0da17ea8-b186-406f-bf62-f36affa66f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 11:37:13 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/01 11:37:13 INFO DAGScheduler: Got job 29 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/01 11:37:13 INFO DAGScheduler: Final stage: ResultStage 29 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/01 11:37:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/01 11:37:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/01 11:37:13 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[54] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/01 11:37:13 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 22.3 KiB, free 413.9 MiB)\n",
      "24/03/01 11:37:13 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 413.9 MiB)\n",
      "24/03/01 11:37:13 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on host-192-168-2-34-de1:36397 (size: 10.4 KiB, free: 413.9 MiB)\n",
      "24/03/01 11:37:13 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/01 11:37:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[54] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/01 11:37:13 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "24/03/01 11:37:13 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 31) (host-192-168-2-34-de1, executor driver, partition 0, PROCESS_LOCAL, 7761 bytes) \n",
      "24/03/01 11:37:13 INFO Executor: Running task 0.0 in stage 29.0 (TID 31)\n",
      "24/03/01 11:37:14 INFO PythonRunner: Times: total = 103, boot = 16, init = 74, finish = 13\n",
      "24/03/01 11:37:14 INFO Executor: Finished task 0.0 in stage 29.0 (TID 31). 2113 bytes result sent to driver\n",
      "24/03/01 11:37:14 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 31) in 153 ms on host-192-168-2-34-de1 (executor driver) (1/1)\n",
      "24/03/01 11:37:14 INFO DAGScheduler: ResultStage 29 (showString at NativeMethodAccessorImpl.java:0) finished in 0.180 s\n",
      "24/03/01 11:37:14 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/01 11:37:14 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "24/03/01 11:37:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished\n",
      "24/03/01 11:37:14 INFO DAGScheduler: Job 29 finished: showString at NativeMethodAccessorImpl.java:0, took 0.198001 s\n",
      "24/03/01 11:37:14 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/01 11:37:14 INFO DAGScheduler: Got job 30 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/01 11:37:14 INFO DAGScheduler: Final stage: ResultStage 30 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/01 11:37:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/01 11:37:14 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/01 11:37:14 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[54] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/01 11:37:14 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 22.3 KiB, free 413.9 MiB)\n",
      "24/03/01 11:37:14 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 413.9 MiB)\n",
      "24/03/01 11:37:14 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on host-192-168-2-34-de1:36397 (size: 10.4 KiB, free: 413.9 MiB)\n",
      "24/03/01 11:37:14 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/01 11:37:14 INFO BlockManagerInfo: Removed broadcast_28_piece0 on host-192-168-2-34-de1:36397 in memory (size: 4.0 KiB, free: 413.9 MiB)\n",
      "24/03/01 11:37:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[54] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
      "24/03/01 11:37:14 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0\n",
      "24/03/01 11:37:14 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 32) (host-192-168-2-34-de1, executor driver, partition 1, PROCESS_LOCAL, 7821 bytes) \n",
      "24/03/01 11:37:14 INFO Executor: Running task 0.0 in stage 30.0 (TID 32)\n",
      "24/03/01 11:37:14 INFO BlockManagerInfo: Removed broadcast_29_piece0 on host-192-168-2-34-de1:36397 in memory (size: 10.4 KiB, free: 413.9 MiB)\n",
      "24/03/01 11:37:14 INFO PythonRunner: Times: total = 53, boot = 24, init = 2, finish = 27\n",
      "24/03/01 11:37:14 INFO Executor: Finished task 0.0 in stage 30.0 (TID 32). 2125 bytes result sent to driver\n",
      "24/03/01 11:37:14 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 32) in 87 ms on host-192-168-2-34-de1 (executor driver) (1/1)\n",
      "24/03/01 11:37:14 INFO DAGScheduler: ResultStage 30 (showString at NativeMethodAccessorImpl.java:0) finished in 0.123 s\n",
      "24/03/01 11:37:14 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/01 11:37:14 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "24/03/01 11:37:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished\n",
      "24/03/01 11:37:14 INFO DAGScheduler: Job 30 finished: showString at NativeMethodAccessorImpl.java:0, took 0.135590 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------+------+--------+\n",
      "|danceability| duration|  tempo|energy|loudness|\n",
      "+------------+---------+-------+------+--------+\n",
      "|         0.0|210.75546| 150.03|   0.0|  -4.188|\n",
      "|         0.0| 319.4771| 96.031|   0.0| -11.519|\n",
      "|         0.0| 207.2028|100.175|   0.0| -14.233|\n",
      "|         0.0|243.17342|152.468|   0.0|  -6.149|\n",
      "|         0.0| 85.96853|110.064|   0.0|  -19.67|\n",
      "|         0.0|364.64282| 60.882|   0.0|  -17.67|\n",
      "|         0.0|251.76772|127.926|   0.0|   -2.62|\n",
      "|         0.0| 116.1922|103.971|   0.0| -14.616|\n",
      "|         0.0|  343.562|131.991|   0.0| -10.751|\n",
      "+------------+---------+-------+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 11:37:14 INFO CodeGenerator: Code generated in 83.256434 ms\n"
     ]
    }
   ],
   "source": [
    "# Zeige den DataFrame-Inhalt\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75edaf8-3e1f-4173-8f65-b8852bb3b27e",
   "metadata": {},
   "source": [
    "# Calculation (Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f17d4735-196d-40f3-87be-b41dcdabfafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 11:37:44 INFO CodeGenerator: Code generated in 170.718637 ms\n",
      "24/03/01 11:37:45 INFO DAGScheduler: Registering RDD 56 (collect at /tmp/ipykernel_53857/2557781333.py:2) as input to shuffle 0\n",
      "24/03/01 11:37:45 INFO DAGScheduler: Got map stage job 31 (collect at /tmp/ipykernel_53857/2557781333.py:2) with 2 output partitions\n",
      "24/03/01 11:37:45 INFO DAGScheduler: Final stage: ShuffleMapStage 31 (collect at /tmp/ipykernel_53857/2557781333.py:2)\n",
      "24/03/01 11:37:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/01 11:37:45 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/01 11:37:45 INFO DAGScheduler: Submitting ShuffleMapStage 31 (MapPartitionsRDD[56] at collect at /tmp/ipykernel_53857/2557781333.py:2), which has no missing parents\n",
      "24/03/01 11:37:45 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 36.3 KiB, free 413.9 MiB)\n",
      "24/03/01 11:37:45 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 15.5 KiB, free 413.8 MiB)\n",
      "24/03/01 11:37:45 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on host-192-168-2-34-de1:36397 (size: 15.5 KiB, free: 413.9 MiB)\n",
      "24/03/01 11:37:45 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/01 11:37:45 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 31 (MapPartitionsRDD[56] at collect at /tmp/ipykernel_53857/2557781333.py:2) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/01 11:37:45 INFO BlockManagerInfo: Removed broadcast_30_piece0 on host-192-168-2-34-de1:36397 in memory (size: 10.4 KiB, free: 413.9 MiB)\n",
      "24/03/01 11:37:45 INFO TaskSchedulerImpl: Adding task set 31.0 with 2 tasks resource profile 0\n",
      "24/03/01 11:37:45 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 33) (host-192-168-2-34-de1, executor driver, partition 0, PROCESS_LOCAL, 7750 bytes) \n",
      "24/03/01 11:37:45 INFO TaskSetManager: Starting task 1.0 in stage 31.0 (TID 34) (host-192-168-2-34-de1, executor driver, partition 1, PROCESS_LOCAL, 7810 bytes) \n",
      "24/03/01 11:37:45 INFO Executor: Running task 0.0 in stage 31.0 (TID 33)\n",
      "24/03/01 11:37:45 INFO Executor: Running task 1.0 in stage 31.0 (TID 34)\n",
      "24/03/01 11:37:45 INFO CodeGenerator: Code generated in 260.355134 ms\n",
      "24/03/01 11:37:45 INFO PythonRunner: Times: total = 79, boot = -31026, init = 31041, finish = 64\n",
      "24/03/01 11:37:45 INFO PythonRunner: Times: total = 394, boot = 39, init = 314, finish = 41\n",
      "24/03/01 11:37:45 INFO Executor: Finished task 0.0 in stage 31.0 (TID 33). 2328 bytes result sent to driver\n",
      "24/03/01 11:37:45 INFO Executor: Finished task 1.0 in stage 31.0 (TID 34). 2285 bytes result sent to driver\n",
      "24/03/01 11:37:45 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 33) in 547 ms on host-192-168-2-34-de1 (executor driver) (1/2)\n",
      "24/03/01 11:37:45 INFO TaskSetManager: Finished task 1.0 in stage 31.0 (TID 34) in 551 ms on host-192-168-2-34-de1 (executor driver) (2/2)\n",
      "24/03/01 11:37:45 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "24/03/01 11:37:45 INFO DAGScheduler: ShuffleMapStage 31 (collect at /tmp/ipykernel_53857/2557781333.py:2) finished in 0.688 s\n",
      "24/03/01 11:37:45 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/01 11:37:45 INFO DAGScheduler: running: Set()\n",
      "24/03/01 11:37:45 INFO DAGScheduler: waiting: Set()\n",
      "24/03/01 11:37:45 INFO DAGScheduler: failed: Set()\n",
      "24/03/01 11:37:45 INFO CodeGenerator: Code generated in 96.016395 ms\n",
      "24/03/01 11:37:46 INFO SparkContext: Starting job: collect at /tmp/ipykernel_53857/2557781333.py:2\n",
      "24/03/01 11:37:46 INFO DAGScheduler: Got job 32 (collect at /tmp/ipykernel_53857/2557781333.py:2) with 1 output partitions\n",
      "24/03/01 11:37:46 INFO DAGScheduler: Final stage: ResultStage 33 (collect at /tmp/ipykernel_53857/2557781333.py:2)\n",
      "24/03/01 11:37:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\n",
      "24/03/01 11:37:46 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/01 11:37:46 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[59] at collect at /tmp/ipykernel_53857/2557781333.py:2), which has no missing parents\n",
      "24/03/01 11:37:46 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 23.4 KiB, free 413.9 MiB)\n",
      "24/03/01 11:37:46 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 8.8 KiB, free 413.8 MiB)\n",
      "24/03/01 11:37:46 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on host-192-168-2-34-de1:36397 (size: 8.8 KiB, free: 413.9 MiB)\n",
      "24/03/01 11:37:46 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580\n",
      "24/03/01 11:37:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[59] at collect at /tmp/ipykernel_53857/2557781333.py:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/01 11:37:46 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0\n",
      "24/03/01 11:37:46 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 35) (host-192-168-2-34-de1, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/03/01 11:37:46 INFO Executor: Running task 0.0 in stage 33.0 (TID 35)\n",
      "24/03/01 11:37:46 INFO BlockManagerInfo: Removed broadcast_31_piece0 on host-192-168-2-34-de1:36397 in memory (size: 15.5 KiB, free: 413.9 MiB)\n",
      "24/03/01 11:37:46 INFO ShuffleBlockFetcherIterator: Getting 2 (194.0 B) non-empty blocks including 2 (194.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/03/01 11:37:46 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 27 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittswerte:\n",
      "avg(duration): 238.08245256212024\n",
      "avg(tempo): 114.83755620320638\n",
      "avg(energy): 0.0\n",
      "avg(danceability): 0.0\n",
      "avg(loudness): -11.268444538116455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 11:37:46 INFO CodeGenerator: Code generated in 103.975145 ms\n",
      "24/03/01 11:37:46 INFO Executor: Finished task 0.0 in stage 33.0 (TID 35). 4058 bytes result sent to driver\n",
      "24/03/01 11:37:46 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 35) in 292 ms on host-192-168-2-34-de1 (executor driver) (1/1)\n",
      "24/03/01 11:37:46 INFO DAGScheduler: ResultStage 33 (collect at /tmp/ipykernel_53857/2557781333.py:2) finished in 0.322 s\n",
      "24/03/01 11:37:46 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/01 11:37:46 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "24/03/01 11:37:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished\n",
      "24/03/01 11:37:46 INFO DAGScheduler: Job 32 finished: collect at /tmp/ipykernel_53857/2557781333.py:2, took 0.357979 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "average_values = spark_df.agg({'danceability': 'avg', 'duration': 'avg', 'tempo': 'avg', 'energy': 'avg', 'loudness': 'avg'}).collect()[0]\n",
    "print(\"Average Values::\")\n",
    "for col_name, avg_value in average_values.asDict().items():\n",
    "    print(f\"{col_name}: {avg_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ca167-67d5-4b4e-857c-b11709b1698e",
   "metadata": {},
   "source": [
    "# STOP SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c23825d7-4cce-458e-82bc-b4dc6995cf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/01 11:39:43 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/03/01 11:39:43 INFO SparkUI: Stopped Spark web UI at http://host-192-168-2-34-de1:4040\n",
      "24/03/01 11:39:43 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/03/01 11:39:43 INFO MemoryStore: MemoryStore cleared\n",
      "24/03/01 11:39:43 INFO BlockManager: BlockManager stopped\n",
      "24/03/01 11:39:43 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/03/01 11:39:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/03/01 11:39:43 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# release the cores\n",
    "spark_context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
