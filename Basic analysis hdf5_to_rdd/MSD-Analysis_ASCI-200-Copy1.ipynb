{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a2ed10a-10d1-4734-bbdf-1d54a99fd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTHON\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pyspark\n",
    "\n",
    "#SPARK\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "\n",
    "import sys\n",
    "\n",
    "# Append the path to Pydoop to sys.path\n",
    "#sys.path.append(\"/usr/local/lib/python3.8/dist-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3096f2-39bc-4025-9c2c-a216255bdf3f",
   "metadata": {},
   "source": [
    "Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fb6d6aa-4b8d-4c65-8dc1-a61c5d94018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session - 2 CORE - imporvement CONFIGURATION!!\n",
    "# spark_session = SparkSession.builder\\\n",
    "#     .master(\"local[2]\")\\\n",
    "#     .appName(\"pseudo_spark_nora\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Set Hadoop configuration directory\n",
    "#os.environ['HADOOP_CONF_DIR'] = '/path/to/hadoop/conf'\n",
    "#spark_session.stop()\n",
    "#os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"HDFS_Connection_Test_nora\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# num executors \n",
    "# executor memory RAM \n",
    "\n",
    "\n",
    "# spark_session = SparkSession.builder \\\n",
    "#     .appName(\"HDFS_Connection_Test_nora\") \\\n",
    "#     .master(\"yarn\") \\\n",
    "#     .config(\"spark.executor.instances\", \"2\") \\\n",
    "#     .config(\"spark.hadoop.fs.default.name\", \"hdfs://master-node:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://master-node:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.dfs.server.namenode.class\", \"org.apache.hadoop.hdfs.server.namenode.NameNode\") \\\n",
    "#     .config(\"spark.hadoop.conf\", \"org.apache.hadoop.hdfs.HdfsConfiguration\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "#.config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "#.config(\"spark.cores.max\", 4)\\\n",
    "\n",
    "\n",
    "#.config(\"spark.jars.packages\", \"LLNL:spark-hdf5:0.0.4\") \\\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"INFO\")\n",
    "\n",
    "sqlContext = SQLContext(spark_session.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44667861-4446-4677-adf9-5acf17142cc4",
   "metadata": {},
   "source": [
    "File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85b6e7bf-1a3a-4aab-af97-04bad4192b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_roots = ['hdfs://130.238.29.183:9000//input/millionsongsubset.tar.gz']\n",
    "directory_path = \"file://data/B/A/A\"\n",
    "#directory_path = \"data_AZ\"\n",
    "#Path300\n",
    "#directory_path = \"/user/hadoop/MillionSongSubset/A/A/\"\n",
    "#directory_path = \"MillionSongSubset/A/A\"\n",
    "path = 'hdfs://master-node:9000/user/hadoop/MillionSongSubset_ASCI_analysis_songs/TRAAAAW128F429D538_songs.asci'\n",
    "directory_path = \"hdfs://master-node:9000/user/hadoop/MillionSongSubset_ASCI_analysis_songs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "f72586dd-41ed-4710-8c15-6cbbe47debcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Spark to list files in HDFS directory\n",
    "# hdfs_files = spark_session._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
    "#     .listStatus(spark_session._jvm.org.apache.hadoop.fs.Path(directory_path))\n",
    "#path = 'hdfs://master-node:9000/user/hadoop/MillionSongSubset_ASCI_analysis_songs/TRAAAAW128F429D538_songs.asci'\n",
    "#path = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAAW128F429D538.asci'\n",
    "path = 'hdfs://master-node:9000/user/hadoop/MillionSongSubset_ASCI_analysis_songs/'\n",
    "max_files = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a18b7ee5-e775-4b03-b698-f04b07e67300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:47:10 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 344.9 KiB, free 362.0 MiB)\n",
      "24/03/15 01:47:10 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 362.0 MiB)\n",
      "24/03/15 01:47:10 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on master-node:37591 (size: 33.3 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:47:10 INFO SparkContext: Created broadcast 93 from wholeTextFiles at <unknown>:0\n",
      "24/03/15 01:47:10 INFO FileInputFormat: Total input files to process : 11\n",
      "24/03/15 01:47:10 INFO FileInputFormat: Total input files to process : 11\n",
      "24/03/15 01:47:10 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/15 01:47:10 INFO DAGScheduler: Got job 59 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/15 01:47:10 INFO DAGScheduler: Final stage: ResultStage 60 (runJob at PythonRDD.scala:181)\n",
      "24/03/15 01:47:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:47:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:47:10 INFO DAGScheduler: Submitting ResultStage 60 (PythonRDD[212] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/15 01:47:10 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 8.2 KiB, free 362.0 MiB)\n",
      "24/03/15 01:47:10 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 362.0 MiB)\n",
      "24/03/15 01:47:10 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on master-node:37591 (size: 5.1 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:47:10 INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:47:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (PythonRDD[212] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 01:47:10 INFO YarnScheduler: Adding task set 60.0 with 1 tasks resource profile 0\n",
      "24/03/15 01:47:10 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 111) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:47:10 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on worker-node-2:39595 (size: 5.1 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:47:10 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on worker-node-2:39595 (size: 33.3 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:47:10 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 111) in 95 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 01:47:10 INFO YarnScheduler: Removed TaskSet 60.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:47:10 INFO DAGScheduler: ResultStage 60 (runJob at PythonRDD.scala:181) finished in 0.105 s\n",
      "24/03/15 01:47:10 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 01:47:10 INFO YarnScheduler: Killing all running tasks in stage 60: Stage finished\n",
      "24/03/15 01:47:10 INFO DAGScheduler: Job 59 finished: runJob at PythonRDD.scala:181, took 0.114863 s\n"
     ]
    }
   ],
   "source": [
    "file_contents = spark_context.wholeTextFiles(path).map(lambda x: x[1].replace('\\n', '').replace('{', '').replace('}', '').replace(' ', '').split(', '))\n",
    "file_contents = file_contents.take(max_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "73413731-3fd0-4616-ae10-27d312e66938",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[229], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m split_file_contents \u001b[38;5;241m=\u001b[39m \u001b[43mfile_contents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "split_file_contents = file_contents.map(lambda x: x[0].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6bf4e196-d127-41c3-b062-be8829f49686",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_elements = split_file_contents.map(lambda x: [float(x[i]) for i in [3, 4, 26, 23, 27, 28, 29]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "24331246-eed0-4505-b070-76e87a885754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:38:09 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/15 01:38:09 INFO DAGScheduler: Got job 53 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/15 01:38:09 INFO DAGScheduler: Final stage: ResultStage 53 (runJob at PythonRDD.scala:181)\n",
      "24/03/15 01:38:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:38:09 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:38:09 INFO DAGScheduler: Submitting ResultStage 53 (PythonRDD[197] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/15 01:38:09 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 9.1 KiB, free 362.5 MiB)\n",
      "24/03/15 01:38:09 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 5.4 KiB, free 362.5 MiB)\n",
      "24/03/15 01:38:09 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on master-node:37591 (size: 5.4 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:38:09 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:38:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (PythonRDD[197] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 01:38:09 INFO YarnScheduler: Adding task set 53.0 with 1 tasks resource profile 0\n",
      "24/03/15 01:38:09 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 104) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:38:09 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on worker-node-2:39595 (size: 5.4 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:38:09 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 104) in 66 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 01:38:09 INFO YarnScheduler: Removed TaskSet 53.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:38:09 INFO DAGScheduler: ResultStage 53 (runJob at PythonRDD.scala:181) finished in 0.077 s\n",
      "24/03/15 01:38:09 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 01:38:09 INFO YarnScheduler: Killing all running tasks in stage 53: Stage finished\n",
      "24/03/15 01:38:09 INFO DAGScheduler: Job 53 finished: runJob at PythonRDD.scala:181, took 0.083547 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[218.932, 0.247, 218.932, -11.197, 92.198, 4.0, 0.778]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_elements.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "57bb06ef-8815-4fc5-84e7-5b7d530ae7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:38:30 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/15 01:38:30 INFO DAGScheduler: Got job 54 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/15 01:38:30 INFO DAGScheduler: Final stage: ResultStage 54 (runJob at PythonRDD.scala:181)\n",
      "24/03/15 01:38:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:38:30 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:38:30 INFO DAGScheduler: Submitting ResultStage 54 (PythonRDD[198] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/15 01:38:30 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 9.6 KiB, free 362.5 MiB)\n",
      "24/03/15 01:38:30 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 362.5 MiB)\n",
      "24/03/15 01:38:30 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on master-node:37591 (size: 5.6 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:38:30 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:38:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (PythonRDD[198] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 01:38:30 INFO YarnScheduler: Adding task set 54.0 with 1 tasks resource profile 0\n",
      "24/03/15 01:38:30 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 105) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:38:30 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on worker-node-2:39595 (size: 5.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:38:30 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 105) in 73 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 01:38:30 INFO YarnScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:38:30 INFO DAGScheduler: ResultStage 54 (runJob at PythonRDD.scala:181) finished in 0.084 s\n",
      "24/03/15 01:38:30 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 01:38:30 INFO YarnScheduler: Killing all running tasks in stage 54: Stage finished\n",
      "24/03/15 01:38:30 INFO DAGScheduler: Job 54 finished: runJob at PythonRDD.scala:181, took 0.087696 s\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(selected_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "8dce02ca-1e86-4451-8368-dd59fedc72b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:38:49 INFO CodeGenerator: Code generated in 26.376097 ms\n",
      "24/03/15 01:38:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Got job 55 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Final stage: ResultStage 55 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[204] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 01:38:49 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 20.0 KiB, free 362.5 MiB)\n",
      "24/03/15 01:38:49 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 9.5 KiB, free 362.5 MiB)\n",
      "24/03/15 01:38:49 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on master-node:37591 (size: 9.5 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:38:49 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[204] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 01:38:49 INFO YarnScheduler: Adding task set 55.0 with 1 tasks resource profile 0\n",
      "24/03/15 01:38:49 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 106) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:38:49 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on worker-node-2:39595 (size: 9.5 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:38:49 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 106) in 101 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 01:38:49 INFO YarnScheduler: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:38:49 INFO DAGScheduler: ResultStage 55 (showString at NativeMethodAccessorImpl.java:0) finished in 0.117 s\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 01:38:49 INFO YarnScheduler: Killing all running tasks in stage 55: Stage finished\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Job 55 finished: showString at NativeMethodAccessorImpl.java:0, took 0.122983 s\n",
      "24/03/15 01:38:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Got job 56 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Final stage: ResultStage 56 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[204] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 01:38:49 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 20.0 KiB, free 362.5 MiB)\n",
      "24/03/15 01:38:49 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 9.5 KiB, free 362.5 MiB)\n",
      "24/03/15 01:38:49 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on master-node:37591 (size: 9.5 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:38:49 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[204] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
      "24/03/15 01:38:49 INFO YarnScheduler: Adding task set 56.0 with 1 tasks resource profile 0\n",
      "24/03/15 01:38:49 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 107) (worker-node-2, executor 1, partition 1, PROCESS_LOCAL, 8304 bytes) \n",
      "24/03/15 01:38:49 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on worker-node-2:39595 (size: 9.5 KiB, free: 366.0 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------+-------+-------+---+-----+\n",
      "|     _1|   _2|     _3|     _4|     _5| _6|   _7|\n",
      "+-------+-----+-------+-------+-------+---+-----+\n",
      "|218.932|0.247|218.932|-11.197| 92.198|4.0|0.778|\n",
      "|148.035|0.148|137.915| -9.843|121.274|4.0|0.384|\n",
      "|177.475|0.282|172.304| -9.689| 100.07|1.0|  0.0|\n",
      "|233.404|  0.0|217.124| -9.013|119.293|4.0|  0.0|\n",
      "|209.606|0.066|198.699| -4.501|129.738|4.0|0.562|\n",
      "|267.702|2.264| 254.27| -9.323|147.782|3.0|0.454|\n",
      "|114.782|0.096|114.782|-17.302|111.787|1.0|  0.0|\n",
      "| 189.57|0.319|181.023|-11.642| 101.43|3.0|0.408|\n",
      "|269.818|  5.3| 258.99|-13.496| 86.643|4.0|0.487|\n",
      "|266.396|0.084|261.747| -6.697|114.041|4.0|0.878|\n",
      "|218.775|2.125|207.012|-10.021|146.765|1.0|  0.0|\n",
      "+-------+-----+-------+-------+-------+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:38:49 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 107) in 156 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 01:38:49 INFO YarnScheduler: Removed TaskSet 56.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:38:49 INFO DAGScheduler: ResultStage 56 (showString at NativeMethodAccessorImpl.java:0) finished in 0.169 s\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 01:38:49 INFO YarnScheduler: Killing all running tasks in stage 56: Stage finished\n",
      "24/03/15 01:38:49 INFO DAGScheduler: Job 56 finished: showString at NativeMethodAccessorImpl.java:0, took 0.172850 s\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "437cbcc0-cf67-4a19-8aa7-156e43c2a176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:44:07 INFO CodeGenerator: Code generated in 55.533127 ms\n",
      "24/03/15 01:44:07 INFO DAGScheduler: Registering RDD 206 (collect at /tmp/ipykernel_231309/3936526560.py:5) as input to shuffle 0\n",
      "24/03/15 01:44:07 INFO DAGScheduler: Got map stage job 57 (collect at /tmp/ipykernel_231309/3936526560.py:5) with 2 output partitions\n",
      "24/03/15 01:44:07 INFO DAGScheduler: Final stage: ShuffleMapStage 57 (collect at /tmp/ipykernel_231309/3936526560.py:5)\n",
      "24/03/15 01:44:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:44:07 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:44:07 INFO DAGScheduler: Submitting ShuffleMapStage 57 (MapPartitionsRDD[206] at collect at /tmp/ipykernel_231309/3936526560.py:5), which has no missing parents\n",
      "24/03/15 01:44:07 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 37.2 KiB, free 362.4 MiB)\n",
      "24/03/15 01:44:07 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 362.4 MiB)\n",
      "24/03/15 01:44:07 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on master-node:37591 (size: 15.3 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:44:07 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:44:07 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 57 (MapPartitionsRDD[206] at collect at /tmp/ipykernel_231309/3936526560.py:5) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/15 01:44:07 INFO YarnScheduler: Adding task set 57.0 with 2 tasks resource profile 0\n",
      "24/03/15 01:44:07 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 108) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8413 bytes) \n",
      "24/03/15 01:44:07 INFO TaskSetManager: Starting task 1.0 in stage 57.0 (TID 109) (worker-node-1, executor 2, partition 1, PROCESS_LOCAL, 8293 bytes) \n",
      "24/03/15 01:44:07 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on worker-node-2:39595 (size: 15.3 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:44:07 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on worker-node-1:34791 (size: 15.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:44:07 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on worker-node-1:34791 (size: 33.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:44:08 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 108) in 258 ms on worker-node-2 (executor 1) (1/2)\n",
      "24/03/15 01:44:08 INFO TaskSetManager: Finished task 1.0 in stage 57.0 (TID 109) in 303 ms on worker-node-1 (executor 2) (2/2)\n",
      "24/03/15 01:44:08 INFO YarnScheduler: Removed TaskSet 57.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:44:08 INFO DAGScheduler: ShuffleMapStage 57 (collect at /tmp/ipykernel_231309/3936526560.py:5) finished in 0.343 s\n",
      "24/03/15 01:44:08 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/15 01:44:08 INFO DAGScheduler: running: Set()\n",
      "24/03/15 01:44:08 INFO DAGScheduler: waiting: Set()\n",
      "24/03/15 01:44:08 INFO DAGScheduler: failed: Set()\n",
      "24/03/15 01:44:08 INFO CodeGenerator: Code generated in 45.568846 ms\n",
      "24/03/15 01:44:08 INFO SparkContext: Starting job: collect at /tmp/ipykernel_231309/3936526560.py:5\n",
      "24/03/15 01:44:08 INFO DAGScheduler: Got job 58 (collect at /tmp/ipykernel_231309/3936526560.py:5) with 1 output partitions\n",
      "24/03/15 01:44:08 INFO DAGScheduler: Final stage: ResultStage 59 (collect at /tmp/ipykernel_231309/3936526560.py:5)\n",
      "24/03/15 01:44:08 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 58)\n",
      "24/03/15 01:44:08 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:44:08 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[209] at collect at /tmp/ipykernel_231309/3936526560.py:5), which has no missing parents\n",
      "24/03/15 01:44:08 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 28.1 KiB, free 362.4 MiB)\n",
      "24/03/15 01:44:08 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 9.9 KiB, free 362.4 MiB)\n",
      "24/03/15 01:44:08 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on master-node:37591 (size: 9.9 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:44:08 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:44:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[209] at collect at /tmp/ipykernel_231309/3936526560.py:5) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 01:44:08 INFO YarnScheduler: Adding task set 59.0 with 1 tasks resource profile 0\n",
      "24/03/15 01:44:08 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 110) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7626 bytes) \n",
      "24/03/15 01:44:08 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on worker-node-2:39595 (size: 9.9 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:44:08 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.40:47008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittswerte:\n",
      "avg(_3): 202.07254545454543\n",
      "avg(_7): 0.3591818181818182\n",
      "avg(_2): 0.9937272727272727\n",
      "avg(_6): 3.0\n",
      "avg(_1): 210.40863636363636\n",
      "avg(_4): -10.247636363636362\n",
      "avg(_5): 115.54736363636363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:44:08 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 110) in 227 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 01:44:08 INFO YarnScheduler: Removed TaskSet 59.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:44:08 INFO DAGScheduler: ResultStage 59 (collect at /tmp/ipykernel_231309/3936526560.py:5) finished in 0.239 s\n",
      "24/03/15 01:44:08 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 01:44:08 INFO YarnScheduler: Killing all running tasks in stage 59: Stage finished\n",
      "24/03/15 01:44:08 INFO DAGScheduler: Job 58 finished: collect at /tmp/ipykernel_231309/3936526560.py:5, took 0.255883 s\n"
     ]
    }
   ],
   "source": [
    "# Urspr√ºngliche Spaltennamen verwenden, wenn bekannt\n",
    "original_column_names = [\"column3\", \"column4\", \"column6\", \"column12\", \"column16\", \"column17\", \"column18\"]\n",
    "\n",
    "\n",
    "average_values = df.agg({'_1': 'avg', '_2': 'avg', '_3': 'avg', '_4': 'avg', '_5': 'avg', '_6': 'avg', '_7': 'avg'}).collect()[0]\n",
    "print(\"Durchschnittswerte:\")\n",
    "for col_name, avg_value in average_values.asDict().items():\n",
    "    print(f\"{col_name}: {avg_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "eaa79ed5-c09a-4f68-a520-27cf4f6c0eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22050',\n",
       " '\"a222795e07cd65b7a530f1346f520649\"',\n",
       " '0',\n",
       " '218.932',\n",
       " '0.247',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0.736',\n",
       " '-11.197',\n",
       " '0',\n",
       " '0.636',\n",
       " '218.932',\n",
       " '92.198',\n",
       " '4',\n",
       " '0.778',\n",
       " '\"TRAAAAW128F429D538\"']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first[0].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c4d0294e-2d89-4dfa-9792-d2a9e0707a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = file_contents.map(lambda data: (\n",
    "    float(float(data[0][3])),  # column3\n",
    "    float(float(data[0][4])),  # column4\n",
    "    float(float(data[0][5])), # column6\n",
    "    float(float(data[0][23])), # column12 (gleicher Index wie column6)\n",
    "    float(float(data[0][25])), # column16\n",
    "    float(float(data[0][26])), # column17\n",
    "    float(float(data[0][27]))  # column18\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7f7bdb7f-ef6c-4bee-b3a8-d0d791357dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:27:46 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/15 01:27:46 INFO DAGScheduler: Got job 46 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/15 01:27:46 INFO DAGScheduler: Final stage: ResultStage 46 (runJob at PythonRDD.scala:181)\n",
      "24/03/15 01:27:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:27:46 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:27:46 INFO DAGScheduler: Submitting ResultStage 46 (PythonRDD[171] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/15 01:27:46 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 8.7 KiB, free 363.6 MiB)\n",
      "24/03/15 01:27:46 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 363.6 MiB)\n",
      "24/03/15 01:27:46 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on master-node:37591 (size: 5.3 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:27:46 INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:27:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (PythonRDD[171] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 01:27:46 INFO YarnScheduler: Adding task set 46.0 with 1 tasks resource profile 0\n",
      "24/03/15 01:27:46 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 91) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:27:46 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on worker-node-2:39595 (size: 5.3 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:27:46 WARN TaskSetManager: Lost task 0.0 in stage 46.0 (TID 91) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/15 01:27:46 INFO TaskSetManager: Starting task 0.1 in stage 46.0 (TID 92) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:27:46 INFO TaskSetManager: Lost task 0.1 in stage 46.0 (TID 92) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      ") [duplicate 1]\n",
      "24/03/15 01:27:46 INFO TaskSetManager: Starting task 0.2 in stage 46.0 (TID 93) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:27:46 INFO TaskSetManager: Lost task 0.2 in stage 46.0 (TID 93) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      ") [duplicate 2]\n",
      "24/03/15 01:27:46 INFO TaskSetManager: Starting task 0.3 in stage 46.0 (TID 94) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:27:46 INFO TaskSetManager: Lost task 0.3 in stage 46.0 (TID 94) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      ") [duplicate 3]\n",
      "24/03/15 01:27:46 ERROR TaskSetManager: Task 0 in stage 46.0 failed 4 times; aborting job\n",
      "24/03/15 01:27:46 INFO YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:27:46 INFO YarnScheduler: Cancelling stage 46\n",
      "24/03/15 01:27:46 INFO YarnScheduler: Killing all running tasks in stage 46: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 46.0 failed 4 times, most recent failure: Lost task 0.3 in stage 46.0 (TID 94) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/15 01:27:46 INFO DAGScheduler: ResultStage 46 (runJob at PythonRDD.scala:181) failed in 0.211 s due to Job aborted due to stage failure: Task 0 in stage 46.0 failed 4 times, most recent failure: Lost task 0.3 in stage 46.0 (TID 94) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/15 01:27:46 INFO DAGScheduler: Job 46 failed: runJob at PythonRDD.scala:181, took 0.215557 s\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 4 times, most recent failure: Lost task 0.3 in stage 46.0 (TID 94) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\nValueError: could not convert string to float: ','\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\nValueError: could not convert string to float: ','\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[196], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mselected_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 4 times, most recent failure: Lost task 0.3 in stage 46.0 (TID 94) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\nValueError: could not convert string to float: ','\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:181)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2849, in takeUpToNumLeft\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\nValueError: could not convert string to float: ','\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:181)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "selected_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1a171901-11bd-443e-b8ba-370896e3a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"column3\", FloatType(), True),\n",
    "    StructField(\"column4\", FloatType(), True),\n",
    "    StructField(\"column6\", FloatType(), True),\n",
    "    StructField(\"column12\", FloatType(), True),\n",
    "    StructField(\"column16\", FloatType(), True),\n",
    "    StructField(\"column17\", FloatType(), True),\n",
    "    StructField(\"column18\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "184a25dc-ae6a-4466-bc9e-b066ed8214c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame(selected_data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "083d82e5-c329-416f-ba37-3cec4615110a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:36:32 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 01:36:32 INFO DAGScheduler: Got job 52 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 01:36:32 INFO DAGScheduler: Final stage: ResultStage 52 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 01:36:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:36:32 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:36:32 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[196] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 01:36:33 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 25.6 KiB, free 361.7 MiB)\n",
      "24/03/15 01:36:33 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 11.0 KiB, free 361.7 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on master-node:37591 (size: 11.0 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:36:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[196] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 01:36:33 INFO YarnScheduler: Adding task set 52.0 with 1 tasks resource profile 0\n",
      "24/03/15 01:36:33 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 100) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_73_piece0 on master-node:37591 in memory (size: 5.3 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_73_piece0 on worker-node-2:39595 in memory (size: 5.3 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on worker-node-2:39595 (size: 11.0 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_74_piece0 on master-node:37591 in memory (size: 5.3 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_74_piece0 on worker-node-2:39595 in memory (size: 5.3 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_84_piece0 on master-node:37591 in memory (size: 5.2 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_84_piece0 on worker-node-2:39595 in memory (size: 5.2 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_82_piece0 on master-node:37591 in memory (size: 5.1 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_82_piece0 on worker-node-2:39595 in memory (size: 5.1 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_75_piece0 on master-node:37591 in memory (size: 5.3 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_75_piece0 on worker-node-2:39595 in memory (size: 5.3 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_76_piece0 on master-node:37591 in memory (size: 5.1 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_76_piece0 on worker-node-2:39595 in memory (size: 5.1 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 WARN TaskSetManager: Lost task 0.0 in stage 52.0 (TID 100) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/15 01:36:33 INFO TaskSetManager: Starting task 0.1 in stage 52.0 (TID 101) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_72_piece0 on worker-node-2:39595 in memory (size: 5.3 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_72_piece0 on master-node:37591 in memory (size: 5.3 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_85_piece0 on master-node:37591 in memory (size: 5.4 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_85_piece0 on worker-node-2:39595 in memory (size: 5.4 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_80_piece0 on master-node:37591 in memory (size: 33.3 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_77_piece0 on master-node:37591 in memory (size: 5.1 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_77_piece0 on worker-node-2:39595 in memory (size: 5.1 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_81_piece0 on master-node:37591 in memory (size: 33.3 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:36:33 INFO BlockManagerInfo: Removed broadcast_81_piece0 on worker-node-2:39595 in memory (size: 33.3 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:36:33 INFO TaskSetManager: Lost task 0.1 in stage 52.0 (TID 101) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      ") [duplicate 1]\n",
      "24/03/15 01:36:33 INFO TaskSetManager: Starting task 0.2 in stage 52.0 (TID 102) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:36:33 INFO TaskSetManager: Lost task 0.2 in stage 52.0 (TID 102) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      ") [duplicate 2]\n",
      "24/03/15 01:36:33 INFO TaskSetManager: Starting task 0.3 in stage 52.0 (TID 103) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:36:33 INFO TaskSetManager: Lost task 0.3 in stage 52.0 (TID 103) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      ") [duplicate 3]\n",
      "24/03/15 01:36:33 ERROR TaskSetManager: Task 0 in stage 52.0 failed 4 times; aborting job\n",
      "24/03/15 01:36:33 INFO YarnScheduler: Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:36:33 INFO YarnScheduler: Cancelling stage 52\n",
      "24/03/15 01:36:33 INFO YarnScheduler: Killing all running tasks in stage 52: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 52.0 failed 4 times, most recent failure: Lost task 0.3 in stage 52.0 (TID 103) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/15 01:36:33 INFO DAGScheduler: ResultStage 52 (showString at NativeMethodAccessorImpl.java:0) failed in 0.243 s due to Job aborted due to stage failure: Task 0 in stage 52.0 failed 4 times, most recent failure: Lost task 0.3 in stage 52.0 (TID 103) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\n",
      "ValueError: could not convert string to float: ','\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/15 01:36:33 INFO DAGScheduler: Job 52 failed: showString at NativeMethodAccessorImpl.java:0, took 0.250462 s\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o21243.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 52.0 failed 4 times, most recent failure: Lost task 0.3 in stage 52.0 (TID 103) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\nValueError: could not convert string to float: ','\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\nValueError: could not convert string to float: ','\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[215], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o21243.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 52.0 failed 4 times, most recent failure: Lost task 0.3 in stage 52.0 (TID 103) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\nValueError: could not convert string to float: ','\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/2361289025.py\", line 4, in <lambda>\nValueError: could not convert string to float: ','\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e14ab731-249d-4a9a-a8a2-e76cc7e15b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:11:16 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 344.9 KiB, free 361.3 MiB)\n",
      "24/03/15 01:11:16 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 361.3 MiB)\n",
      "24/03/15 01:11:16 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on master-node:37591 (size: 33.3 KiB, free: 365.8 MiB)\n",
      "24/03/15 01:11:16 INFO SparkContext: Created broadcast 58 from wholeTextFiles at <unknown>:0\n"
     ]
    }
   ],
   "source": [
    "#file_paths = spark_context.wholeTextFiles(path).map(lambda x: x[1]).replace('\\n', '').replace(' ', '')\n",
    "file_paths = spark_context.wholeTextFiles(path).map(lambda x: x[1].replace('\\n', '').replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "cdb9f9ab-24d8-49cf-80db-eed0b4b79810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:12:58 INFO SparkContext: Starting job: collect at /tmp/ipykernel_231309/1529570411.py:1\n",
      "24/03/15 01:12:58 INFO DAGScheduler: Got job 35 (collect at /tmp/ipykernel_231309/1529570411.py:1) with 2 output partitions\n",
      "24/03/15 01:12:58 INFO DAGScheduler: Final stage: ResultStage 35 (collect at /tmp/ipykernel_231309/1529570411.py:1)\n",
      "24/03/15 01:12:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:12:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:12:58 INFO DAGScheduler: Submitting ResultStage 35 (PythonRDD[145] at collect at /tmp/ipykernel_231309/1529570411.py:1), which has no missing parents\n",
      "24/03/15 01:12:58 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 7.4 KiB, free 364.0 MiB)\n",
      "24/03/15 01:12:58 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 4.5 KiB, free 364.0 MiB)\n",
      "24/03/15 01:12:58 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on master-node:37591 (size: 4.5 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:58 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:12:58 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 35 (PythonRDD[145] at collect at /tmp/ipykernel_231309/1529570411.py:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/15 01:12:58 INFO YarnScheduler: Adding task set 35.0 with 2 tasks resource profile 0\n",
      "24/03/15 01:12:58 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 64) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8424 bytes) \n",
      "24/03/15 01:12:58 INFO TaskSetManager: Starting task 1.0 in stage 35.0 (TID 65) (worker-node-1, executor 2, partition 1, PROCESS_LOCAL, 8304 bytes) \n",
      "24/03/15 01:12:58 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on worker-node-2:39595 (size: 4.5 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:12:58 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on worker-node-1:34791 (size: 4.5 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:12:59 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on worker-node-1:34791 (size: 33.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:12:59 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 64) in 73 ms on worker-node-2 (executor 1) (1/2)\n",
      "24/03/15 01:12:59 INFO TaskSetManager: Finished task 1.0 in stage 35.0 (TID 65) in 179 ms on worker-node-1 (executor 2) (2/2)\n",
      "24/03/15 01:12:59 INFO YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:12:59 INFO DAGScheduler: ResultStage 35 (collect at /tmp/ipykernel_231309/1529570411.py:1) finished in 0.198 s\n",
      "24/03/15 01:12:59 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 01:12:59 INFO YarnScheduler: Killing all running tasks in stage 35: Stage finished\n",
      "24/03/15 01:12:59 INFO DAGScheduler: Job 35 finished: collect at /tmp/ipykernel_231309/1529570411.py:1, took 0.207772 s\n"
     ]
    }
   ],
   "source": [
    "first = file_paths.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8f86fcf3-c796-4451-b41c-a69e9a1a1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['22050,\"a222795e07cd65b7a530f1346f520649\",0,218.932,0.247,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0.736,-11.197,0,0.636,218.932,92.198,4,0.778,\"TRAAAAW128F429D538\"']\n"
     ]
    }
   ],
   "source": [
    "file_paths = file_paths.replace('\\n', '').replace('{', '').replace('}', '').replace(' ', '').split(', ')\n",
    "print(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "abf9c5ab-a6dc-4023-af55-c932ca197b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data = [first[0].split(',')[i] for i in [3, 4, 22, 22, 23, 26, 27]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "edeea4d1-b76e-4bb5-ba05-1b09a124923b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertiere die Daten in die entsprechenden Datentypen\n",
    "selected_data = [float(selected_data[0]), str(selected_data[1]), float(selected_data[2]), float(selected_data[3]), float(selected_data[4]), float(selected_data[5]), float(selected_data[6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1cfc7e2d-8073-49c6-82d3-90e3f53e4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"column3\", FloatType(), True),\n",
    "    StructField(\"column4\", FloatType(), True),\n",
    "    StructField(\"column6\", FloatType(), True),\n",
    "    StructField(\"column12\", FloatType(), True),\n",
    "    StructField(\"column16\", FloatType(), True),\n",
    "    StructField(\"column17\", FloatType(), True),\n",
    "    StructField(\"column18\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8e8563c7-70e0-4c90-b8a4-889538ff34e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_55_piece0 on master-node:37591 in memory (size: 33.3 KiB, free: 365.8 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_47_piece0 on master-node:37591 in memory (size: 5.0 KiB, free: 365.8 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_47_piece0 on worker-node-2:39595 in memory (size: 5.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_39_piece0 on master-node:37591 in memory (size: 6.5 KiB, free: 365.8 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_39_piece0 on worker-node-1:34791 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_46_piece0 on master-node:37591 in memory (size: 7.0 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_46_piece0 on worker-node-2:39595 in memory (size: 7.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_38_piece0 on master-node:37591 in memory (size: 6.5 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_38_piece0 on worker-node-2:39595 in memory (size: 6.5 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_45_piece0 on master-node:37591 in memory (size: 7.0 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_45_piece0 on worker-node-2:39595 in memory (size: 7.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_53_piece0 on master-node:37591 in memory (size: 33.4 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_56_piece0 on master-node:37591 in memory (size: 33.4 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_42_piece0 on master-node:37591 in memory (size: 5.0 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_42_piece0 on worker-node-2:39595 in memory (size: 5.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_48_piece0 on master-node:37591 in memory (size: 7.0 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_48_piece0 on worker-node-2:39595 in memory (size: 7.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_49_piece0 on master-node:37591 in memory (size: 7.0 KiB, free: 365.9 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_49_piece0 on worker-node-2:39595 in memory (size: 7.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_51_piece0 on master-node:37591 in memory (size: 33.4 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_50_piece0 on master-node:37591 in memory (size: 33.3 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_54_piece0 on master-node:37591 in memory (size: 33.4 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_43_piece0 on master-node:37591 in memory (size: 5.0 KiB, free: 366.0 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_43_piece0 on worker-node-2:39595 in memory (size: 5.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_41_piece0 on master-node:37591 in memory (size: 6.5 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_41_piece0 on worker-node-2:39595 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_40_piece0 on master-node:37591 in memory (size: 6.5 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_40_piece0 on worker-node-1:34791 in memory (size: 6.5 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_59_piece0 on master-node:37591 in memory (size: 5.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_59_piece0 on worker-node-2:39595 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_52_piece0 on master-node:37591 in memory (size: 33.4 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_44_piece0 on worker-node-2:39595 in memory (size: 5.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:12:03 INFO BlockManagerInfo: Removed broadcast_44_piece0 on master-node:37591 in memory (size: 5.0 KiB, free: 366.1 MiB)\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame([selected_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a685054b-9a34-45d5-b9da-158dde2a193a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:12:07 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Got job 33 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Final stage: ResultStage 33 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[144] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 01:12:07 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 14.5 KiB, free 364.0 MiB)\n",
      "24/03/15 01:12:07 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 364.0 MiB)\n",
      "24/03/15 01:12:07 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on master-node:37591 (size: 7.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:07 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[144] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 01:12:07 INFO YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\n",
      "24/03/15 01:12:07 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 62) (worker-node-2, executor 1, partition 0, PROCESS_LOCAL, 7606 bytes) \n",
      "24/03/15 01:12:07 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on worker-node-2:39595 (size: 7.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 01:12:07 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 62) in 58 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 01:12:07 INFO YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:12:07 INFO DAGScheduler: ResultStage 33 (showString at NativeMethodAccessorImpl.java:0) finished in 0.074 s\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 01:12:07 INFO YarnScheduler: Killing all running tasks in stage 33: Stage finished\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Job 33 finished: showString at NativeMethodAccessorImpl.java:0, took 0.088119 s\n",
      "24/03/15 01:12:07 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Got job 34 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Final stage: ResultStage 34 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[144] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 01:12:07 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 14.5 KiB, free 364.0 MiB)\n",
      "24/03/15 01:12:07 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 364.0 MiB)\n",
      "24/03/15 01:12:07 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on master-node:37591 (size: 7.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 01:12:07 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[144] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1))\n",
      "24/03/15 01:12:07 INFO YarnScheduler: Adding task set 34.0 with 1 tasks resource profile 0\n",
      "24/03/15 01:12:07 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 63) (worker-node-1, executor 2, partition 1, PROCESS_LOCAL, 7696 bytes) \n",
      "24/03/15 01:12:07 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on worker-node-1:34791 (size: 7.0 KiB, free: 366.2 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+-------+-------+------+\n",
      "|     _1|   _2|   _3|   _4|     _5|     _6|    _7|\n",
      "+-------+-----+-----+-----+-------+-------+------+\n",
      "|218.932|0.247|0.736|0.736|-11.197|218.932|92.198|\n",
      "+-------+-----+-----+-----+-------+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 01:12:07 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 63) in 142 ms on worker-node-1 (executor 2) (1/1)\n",
      "24/03/15 01:12:07 INFO YarnScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "24/03/15 01:12:07 INFO DAGScheduler: ResultStage 34 (showString at NativeMethodAccessorImpl.java:0) finished in 0.158 s\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 01:12:07 INFO YarnScheduler: Killing all running tasks in stage 34: Stage finished\n",
      "24/03/15 01:12:07 INFO DAGScheduler: Job 34 finished: showString at NativeMethodAccessorImpl.java:0, took 0.166256 s\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "00bbe06b-f160-4dae-be15-28535fedcba9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selected_data \u001b[38;5;241m=\u001b[39m [first[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m8\u001b[39m]]\n\u001b[1;32m      2\u001b[0m selected_data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(selected_data[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(selected_data[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(selected_data[\u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(selected_data[\u001b[38;5;241m3\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(selected_data[\u001b[38;5;241m4\u001b[39m]), \u001b[38;5;28mint\u001b[39m(selected_data[\u001b[38;5;241m5\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(selected_data[\u001b[38;5;241m6\u001b[39m])]\n\u001b[1;32m      4\u001b[0m selected_data\n",
      "Cell \u001b[0;32mIn[97], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selected_data \u001b[38;5;241m=\u001b[39m [\u001b[43mfirst\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m8\u001b[39m]]\n\u001b[1;32m      2\u001b[0m selected_data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(selected_data[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(selected_data[\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(selected_data[\u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(selected_data[\u001b[38;5;241m3\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(selected_data[\u001b[38;5;241m4\u001b[39m]), \u001b[38;5;28mint\u001b[39m(selected_data[\u001b[38;5;241m5\u001b[39m]), \u001b[38;5;28mfloat\u001b[39m(selected_data[\u001b[38;5;241m6\u001b[39m])]\n\u001b[1;32m      4\u001b[0m selected_data\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "selected_data = [first[i] for i in [2, 3, 7, 7, 6, 9, 8]]\n",
    "selected_data = [int(selected_data[0]), float(selected_data[1]), float(selected_data[2]), float(selected_data[3]), float(selected_data[4]), int(selected_data[5]), float(selected_data[6])]\n",
    "\n",
    "selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d00e8ef3-b42a-48ab-ab96-f2290e733fc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `        0` in type `str`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m      2\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn3\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn4\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn29\u001b[39m\u001b[38;5;124m\"\u001b[39m, FloatType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m ])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Erstelle das DataFrame\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_data\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m-> 1459\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:2160\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2151\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2152\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2157\u001b[0m             },\n\u001b[1;32m   2158\u001b[0m         )\n\u001b[1;32m   2159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 2160\u001b[0m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2162\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:2076\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_integer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_integer\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2075\u001b[0m     assert_acceptable_types(obj)\n\u001b[0;32m-> 2076\u001b[0m     \u001b[43mverify_acceptable_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2077\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2147483648\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2147483647\u001b[39m:\n\u001b[1;32m   2078\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2079\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVALUE_OUT_OF_BOUND\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2080\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2085\u001b[0m             },\n\u001b[1;32m   2086\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:2006\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2004\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[0;32m-> 2006\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   2007\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_ACCEPT_OBJECT_IN_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2008\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   2009\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(dataType),\n\u001b[1;32m   2010\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(obj),\n\u001b[1;32m   2011\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   2012\u001b[0m             },\n\u001b[1;32m   2013\u001b[0m         )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `IntegerType()` can not accept object `        0` in type `str`."
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"column3\", IntegerType(), True),\n",
    "    StructField(\"column4\", FloatType(), True),\n",
    "    StructField(\"column26\", FloatType(), True),\n",
    "    StructField(\"column23\", FloatType(), True),\n",
    "    StructField(\"column27\", FloatType(), True),\n",
    "    StructField(\"column28\", IntegerType(), True),\n",
    "    StructField(\"column29\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Erstelle das DataFrame\n",
    "df = spark_session.createDataFrame([selected_data], schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a50b8e3c-32d1-4b36-a2ba-c6f82ed504f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_rdd = file_paths.flatMap(lambda file_path: \n",
    "                               spark_session.read.text(file_path)\n",
    "                                    .rdd\n",
    "                                    .map(lambda row: row.value.replace('\\n', '').replace(' ', '').replace(',', '').split())\n",
    "                                    .map(lambda tokens: Row(column1=tokens[3], column2=tokens[6], column3=tokens[3], column4=tokens[4], column5=tokens[5], column6=tokens[9], column7=tokens[10], column8=tokens[11]))\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "24c1e0c8-967f-4899-a518-bb28a78ccbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 00:36:56 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/15 00:36:56 INFO FileInputFormat: Total input files to process : 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/serializers.py\", line 459, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/context.py\", line 466, in __getnewargs__\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/serializers.py:459\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:466\u001b[0m, in \u001b[0;36mSparkContext.__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getnewargs__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;66;03m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    467\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCONTEXT_ONLY_VALID_ON_DRIVER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    468\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    469\u001b[0m     )\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfile_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:2888\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2862\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   2863\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2864\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   2865\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2888\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   2890\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:2855\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   2852\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2854\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 2855\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2857\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   2858\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:2510\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   2508\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   2509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2510\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), \u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[1;32m   2511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:5470\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5468\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 5470\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   5472\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5474\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5475\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   5476\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   5477\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:5268\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5266\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5267\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 5268\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5269\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSimplePythonFunction(\n\u001b[1;32m   5271\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   5272\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5277\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   5278\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:5251\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   5249\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   5250\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 5251\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5252\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   5253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   5254\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/serializers.py:469\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    468\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 469\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
     ]
    }
   ],
   "source": [
    "file_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "002b236e-fd26-489b-a151-2ddbdf5f31a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the DataFrame\n",
    "schema = \"column1 STRING, column2 STRING, column3 STRING, column4 STRING, column5 STRING, column6 STRING, column7 STRING, column8 STRING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94cdfa31-8c2f-4d5e-8059-6e29ca41a778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 00:25:11 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 358.7 KiB, free 364.8 MiB)\n",
      "24/03/15 00:25:11 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 33.1 KiB, free 364.8 MiB)\n",
      "24/03/15 00:25:11 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on master-node:37591 (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:25:11 INFO SparkContext: Created broadcast 18 from textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "file_rdd = spark_context.textFile(path) \\\n",
    "    .map(lambda line: line.replace('\\n', '').replace(' ', '').replace(',', '').split()) #\\\n",
    "    #.map(lambda data: Row(column1=data[3], column2=data[6], column3=data[3], column4=data[4], column5=data[5], column6=data[9], column7=data[10], column8=data[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86c4814e-5e23-4e87-82e2-a4ca2375c744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 00:25:13 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/15 00:25:13 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/15 00:25:13 INFO DAGScheduler: Got job 9 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/15 00:25:13 INFO DAGScheduler: Final stage: ResultStage 9 (runJob at PythonRDD.scala:181)\n",
      "24/03/15 00:25:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 00:25:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 00:25:13 INFO DAGScheduler: Submitting ResultStage 9 (PythonRDD[40] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/15 00:25:13 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 8.5 KiB, free 364.7 MiB)\n",
      "24/03/15 00:25:13 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 364.7 MiB)\n",
      "24/03/15 00:25:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on master-node:37591 (size: 5.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:25:13 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 00:25:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (PythonRDD[40] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 00:25:13 INFO YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/03/15 00:25:13 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 35) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:25:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on worker-node-2:39595 (size: 5.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:25:13 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on worker-node-2:39595 (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:25:13 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 35) in 97 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 00:25:13 INFO YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/03/15 00:25:13 INFO DAGScheduler: ResultStage 9 (runJob at PythonRDD.scala:181) finished in 0.112 s\n",
      "24/03/15 00:25:13 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 00:25:13 INFO YarnScheduler: Killing all running tasks in stage 9: Stage finished\n",
      "24/03/15 00:25:13 INFO DAGScheduler: Job 9 finished: runJob at PythonRDD.scala:181, took 0.122703 s\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "befd9963-20c6-4eac-abfc-26acc7442c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 00:19:51 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 358.7 KiB, free 363.2 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_12_piece0 on master-node:37591 in memory (size: 35.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:19:51 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 33.1 KiB, free 363.5 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on master-node:37591 (size: 33.1 KiB, free: 366.0 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_11_piece0 on master-node:37591 in memory (size: 6.4 KiB, free: 366.0 MiB)\n",
      "24/03/15 00:19:51 INFO SparkContext: Created broadcast 14 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_11_piece0 on worker-node-2:39595 in memory (size: 6.4 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_6_piece0 on master-node:37591 in memory (size: 5.1 KiB, free: 366.0 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_6_piece0 on worker-node-2:39595 in memory (size: 5.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_6_piece0 on worker-node-1:34791 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_10_piece0 on master-node:37591 in memory (size: 35.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_10_piece0 on worker-node-2:39595 in memory (size: 35.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_9_piece0 on worker-node-1:34791 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_9_piece0 on worker-node-2:39595 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_9_piece0 on master-node:37591 in memory (size: 5.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_5_piece0 on worker-node-1:34791 in memory (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_5_piece0 on master-node:37591 in memory (size: 6.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_5_piece0 on worker-node-2:39595 in memory (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on worker-node-1:34791 in memory (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on worker-node-2:39595 in memory (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:19:51 INFO BlockManagerInfo: Removed broadcast_8_piece0 on master-node:37591 in memory (size: 6.0 KiB, free: 366.1 MiB)\n"
     ]
    }
   ],
   "source": [
    "file_rdd = spark_context.textFile(path) \\\n",
    "    .map(lambda line: line.split(\",\")) \\\n",
    "    .map(lambda x: Row(*x))\n",
    "\n",
    "cleaned_rdd = rdd.map(lambda line: line.replace('\\n', '').replace(' ', '').replace(',', '').split())\n",
    "\n",
    "    # Debugging-Anweisungen hinzuf√ºgen, um die Struktur der RDD-Elemente zu √ºberpr√ºfen\n",
    "    cleaned_rdd.foreach(lambda x: print(\"Debugging: \", x))\n",
    "    \n",
    "    # Zeilen f√ºr jede bereinigte Zeile im RDD erstellen\n",
    "    rows = cleaned_rdd.map(lambda data: Row(column1=data[3], column2=data[6], column3=data[3], column4=data[4], column5=data[5], column6=data[9], column7=data[10], column8=data[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77e0224d-2bd6-438b-9e48-a0addad007f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_df = spark_session.createDataFrame(file_rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3de78d1f-69d2-4385-9aa5-8d3028ed2fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 00:20:34 INFO CodeGenerator: Code generated in 25.505494 ms\n",
      "24/03/15 00:20:34 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/15 00:20:34 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 00:20:34 INFO DAGScheduler: Got job 7 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 00:20:34 INFO DAGScheduler: Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 00:20:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 00:20:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 00:20:34 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[34] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 00:20:34 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 22.9 KiB, free 364.0 MiB)\n",
      "24/03/15 00:20:34 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 10.6 KiB, free 364.0 MiB)\n",
      "24/03/15 00:20:34 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on master-node:37591 (size: 10.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:20:34 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 00:20:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[34] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 00:20:34 INFO YarnScheduler: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "24/03/15 00:20:34 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 27) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:20:34 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on worker-node-2:39595 (size: 10.6 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:20:34 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on worker-node-2:39595 (size: 33.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:20:34 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 27) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2187, in verify\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2150, in verify_struct\n",
      "pyspark.errors.exceptions.base.PySparkValueError: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 1 and 8.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/15 00:20:34 INFO TaskSetManager: Starting task 0.1 in stage 7.0 (TID 28) (worker-node-1, executor 2, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:20:34 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on worker-node-1:34791 (size: 10.6 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:20:34 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on worker-node-1:34791 (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:20:35 WARN TaskSetManager: Lost task 0.1 in stage 7.0 (TID 28) (worker-node-1 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2187, in verify\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2150, in verify_struct\n",
      "pyspark.errors.exceptions.base.PySparkValueError: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 1 and 8.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/15 00:20:35 INFO TaskSetManager: Starting task 0.2 in stage 7.0 (TID 29) (worker-node-1, executor 2, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:20:35 INFO TaskSetManager: Lost task 0.2 in stage 7.0 (TID 29) on worker-node-1, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2187, in verify\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2150, in verify_struct\n",
      "pyspark.errors.exceptions.base.PySparkValueError: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 1 and 8.\n",
      ") [duplicate 1]\n",
      "24/03/15 00:20:35 INFO TaskSetManager: Starting task 0.3 in stage 7.0 (TID 30) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:20:35 INFO TaskSetManager: Lost task 0.3 in stage 7.0 (TID 30) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2187, in verify\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2150, in verify_struct\n",
      "pyspark.errors.exceptions.base.PySparkValueError: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 1 and 8.\n",
      ") [duplicate 1]\n",
      "24/03/15 00:20:35 ERROR TaskSetManager: Task 0 in stage 7.0 failed 4 times; aborting job\n",
      "24/03/15 00:20:35 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/03/15 00:20:35 INFO YarnScheduler: Cancelling stage 7\n",
      "24/03/15 00:20:35 INFO YarnScheduler: Killing all running tasks in stage 7: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 30) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2187, in verify\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2150, in verify_struct\n",
      "pyspark.errors.exceptions.base.PySparkValueError: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 1 and 8.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/15 00:20:35 INFO DAGScheduler: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) failed in 1.108 s due to Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 30) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2187, in verify\n",
      "  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2150, in verify_struct\n",
      "pyspark.errors.exceptions.base.PySparkValueError: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 1 and 8.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/15 00:20:35 INFO DAGScheduler: Job 7 failed: showString at NativeMethodAccessorImpl.java:0, took 1.115633 s\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o20301.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 30) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2187, in verify\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2150, in verify_struct\npyspark.errors.exceptions.base.PySparkValueError: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 1 and 8.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2187, in verify\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2150, in verify_struct\npyspark.errors.exceptions.base.PySparkValueError: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 1 and 8.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msql_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o20301.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 30) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2187, in verify\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2150, in verify_struct\npyspark.errors.exceptions.base.PySparkValueError: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 1 and 8.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/session.py\", line 1459, in prepare\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2187, in verify\n  File \"/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/types.py\", line 2150, in verify_struct\npyspark.errors.exceptions.base.PySparkValueError: [LENGTH_SHOULD_BE_THE_SAME] obj and fields should be of the same length, got 1 and 8.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on master-node:37591 in memory (size: 10.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on worker-node-2:39595 in memory (size: 10.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on worker-node-1:34791 in memory (size: 10.6 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_4_piece0 on master-node:37591 in memory (size: 33.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_4_piece0 on worker-node-1:34791 in memory (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_4_piece0 on worker-node-2:39595 in memory (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_13_piece0 on master-node:37591 in memory (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on master-node:37591 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on worker-node-2:39595 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_3_piece0 on worker-node-1:34791 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on master-node:37591 in memory (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on worker-node-1:34791 in memory (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_2_piece0 on worker-node-2:39595 in memory (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on master-node:37591 in memory (size: 35.6 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:22:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on worker-node-2:39595 in memory (size: 35.6 KiB, free: 366.2 MiB)\n"
     ]
    }
   ],
   "source": [
    "sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5250cac-4c24-4c08-b1c7-6f265d09785e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 00:17:02 INFO InMemoryFileIndex: It took 5 ms to list leaf files for 1 paths.\n",
      "24/03/15 00:17:02 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "24/03/15 00:17:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/15 00:17:02 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#66, None)) > 0)\n",
      "24/03/15 00:17:02 INFO CodeGenerator: Code generated in 18.758086 ms\n",
      "24/03/15 00:17:02 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 353.8 KiB, free 364.4 MiB)\n",
      "24/03/15 00:17:02 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 364.3 MiB)\n",
      "24/03/15 00:17:02 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on master-node:37591 (size: 35.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:17:02 INFO SparkContext: Created broadcast 10 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 00:17:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/15 00:17:02 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 00:17:02 INFO DAGScheduler: Got job 6 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 00:17:02 INFO DAGScheduler: Final stage: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 00:17:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 00:17:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 00:17:02 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[18] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 00:17:02 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.5 KiB, free 364.3 MiB)\n",
      "24/03/15 00:17:02 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 364.3 MiB)\n",
      "24/03/15 00:17:02 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on master-node:37591 (size: 6.4 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:17:02 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 00:17:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[18] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 00:17:02 INFO YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/03/15 00:17:02 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 26) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 8257 bytes) \n",
      "24/03/15 00:17:02 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on worker-node-2:39595 (size: 6.4 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:17:02 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on worker-node-2:39595 (size: 35.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:17:02 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 26) in 116 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 00:17:02 INFO YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/03/15 00:17:02 INFO DAGScheduler: ResultStage 6 (csv at NativeMethodAccessorImpl.java:0) finished in 0.133 s\n",
      "24/03/15 00:17:03 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 00:17:03 INFO YarnScheduler: Killing all running tasks in stage 6: Stage finished\n",
      "24/03/15 00:17:03 INFO DAGScheduler: Job 6 finished: csv at NativeMethodAccessorImpl.java:0, took 0.143467 s\n",
      "24/03/15 00:17:03 INFO CodeGenerator: Code generated in 12.929188 ms\n",
      "24/03/15 00:17:03 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/15 00:17:03 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/03/15 00:17:03 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 353.8 KiB, free 364.0 MiB)\n",
      "24/03/15 00:17:03 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 363.9 MiB)\n",
      "24/03/15 00:17:03 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on master-node:37591 (size: 35.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:17:03 INFO SparkContext: Created broadcast 12 from csv at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 00:17:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.read.option(\"header\", \"false\").csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63edfe6a-7867-43d8-a5dc-b2869d0636e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:53:58 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/03/14 23:53:58 INFO SharedState: Warehouse path is 'file:/home/hadoop/MSD_Project/DataEngineeringProject/Basic%20analysis%20hdf5_to_rdd/spark-warehouse'.\n",
      "24/03/14 23:53:58 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/03/14 23:53:58 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/03/14 23:53:58 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/03/14 23:53:58 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/03/14 23:53:58 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/03/14 23:54:01 INFO InMemoryFileIndex: It took 1190 ms to list leaf files for 1 paths.\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.read.text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac4dbcf2-83e4-4d8b-ae14-849fbbef5c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daten bereinigen und Spalten extrahieren\n",
    "cleaned_df = df.withColumn('content', regexp_replace(col('value'), r'[\\n\\s,]', '')) \\\n",
    "    .selectExpr(\"split(content, '\\t') as data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a6adcf9-2d59-4d79-8769-d5dc61dd2d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = df.withColumn('content', regexp_replace(col('value'), r'[\\n\\s,]', '')) \\\n",
    "    .selectExpr(\"split(content, '\\t') as data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "310e2568-10f6-4907-a73a-a2b9d72962ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = cleaned_df.select(\n",
    "    col('data')[5].alias('column1'),\n",
    "    col('data')[6].alias('column2'),\n",
    "    col('data')[28].alias('column3'),\n",
    "    col('data')[25].alias('column4'),\n",
    "    col('data')[23].alias('column5'),\n",
    "    col('data')[29].alias('column6'),\n",
    "    col('data')[30].alias('column7'),\n",
    "    col('data')[31].alias('column8')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fde44dc-6610-484f-b001-cb5756775649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:56:26 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/03/14 23:56:26 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/03/14 23:56:27 INFO CodeGenerator: Code generated in 386.169797 ms\n",
      "24/03/14 23:56:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 353.5 KiB, free 366.0 MiB)\n",
      "24/03/14 23:56:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 35.6 KiB, free 365.9 MiB)\n",
      "24/03/14 23:56:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on master-node:37591 (size: 35.6 KiB, free: 366.3 MiB)\n",
      "24/03/14 23:56:27 INFO SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/14 23:56:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/03/14 23:56:27 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/14 23:56:27 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/14 23:56:27 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/14 23:56:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 23:56:27 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:56:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/14 23:56:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 20.6 KiB, free 365.9 MiB)\n",
      "24/03/14 23:56:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 365.9 MiB)\n",
      "24/03/14 23:56:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on master-node:37591 (size: 8.1 KiB, free: 366.3 MiB)\n",
      "24/03/14 23:56:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:56:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 23:56:28 INFO YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/03/14 23:56:28 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 12132 bytes) \n",
      "24/03/14 23:56:28 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on worker-node-2:39595 (size: 8.1 KiB, free: 366.3 MiB)\n",
      "24/03/14 23:56:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on worker-node-2:39595 (size: 35.6 KiB, free: 366.3 MiB)\n",
      "24/03/14 23:56:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2816 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/14 23:56:30 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/14 23:56:30 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 2.981 s\n",
      "24/03/14 23:56:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 23:56:30 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "24/03/14 23:56:30 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 3.070527 s\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|column1|column2|column3|column4|column5|column6|column7|column8|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|   NULL|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:56:32 INFO CodeGenerator: Code generated in 29.276926 ms\n",
      "24/03/14 23:56:32 INFO BlockManagerInfo: Removed broadcast_1_piece0 on master-node:37591 in memory (size: 8.1 KiB, free: 366.3 MiB)\n",
      "24/03/14 23:56:32 INFO BlockManagerInfo: Removed broadcast_1_piece0 on worker-node-2:39595 in memory (size: 8.1 KiB, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333b60e-ef05-446a-86ff-7c0cbcd0a7c7",
   "metadata": {},
   "source": [
    "# 1) Saving the paths to the HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82538b17-fdbc-4c52-9c59-23e67626d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h5_file_paths_hdfs(spark, hdfs_directory_path):\n",
    "    h5_file_paths = []\n",
    "\n",
    "    # Use Spark to list files in HDFS directory\n",
    "    hdfs_files = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
    "        .listStatus(spark._jvm.org.apache.hadoop.fs.Path(hdfs_directory_path))\n",
    "\n",
    "    # Extract file paths from HDFS file status\n",
    "    for hdfs_file_status in hdfs_files:\n",
    "        file_path = hdfs_file_status.getPath().toString()\n",
    "\n",
    "        if hdfs_file_status.isDirectory():\n",
    "            # If it's a directory, recursively gather paths\n",
    "            h5_file_paths.extend(get_h5_file_paths_hdfs(spark, file_path))\n",
    "        elif file_path.endswith(\".asci\"):\n",
    "            # If it's an HDF5 file, add to the list\n",
    "            h5_file_paths.append(file_path)\n",
    "\n",
    "    return h5_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1292d02-c77a-47b4-b0e1-b62f475f8679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 HDF5 file paths gathered (HDFS)\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAAW128F429D538.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAABD128F429CF47.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAADZ128F9348C2E.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAEF128F4273421.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAFD128F92F423A.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAMO128F1481E7F.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAMQ128F1460CD3.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAPK128E0786D96.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAARJ128F9320760.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAVG12903CFA543.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAVO128F93133D4.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABCL128F4286650.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABDL12903CAABBA.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABJL12903CDCF1A.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABJV128F1460C49.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABLR128F423B7E3.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABNV128F425CEE1.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABRB128F9306DD5.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABVM128F92CA9DC.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABXG128F9318EBD.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABYN12903CFD305.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAABYW128F4244559.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACCG128F92E8A55.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACER128F4290F96.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACFV128F935E50B.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACHN128F1489601.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACIW12903CC0F6D.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACLV128F427E123.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACNS128F14A2DF5.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACOW128F933E35F.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACPE128F421C1B9.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACQT128F9331780.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACSL128F93462F4.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACTB12903CAAF15.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACVS128E078BE39.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAACZK128F4243829.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADDS128F425C68B.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADED128F42741FD.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADEV128F9348C0A.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADHS12903CE70A9.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADIV12903CB15C1.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADLH12903CA70EE.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADLN128F14832E9.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADLR12903CF8D7E.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADLX128F1469250.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADMH128F9343E59.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADNA128F9331246.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADOF12903CAA1C1.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADQX128F422B4CF.asci\n",
      "hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAADRN128F932D607.asci\n"
     ]
    }
   ],
   "source": [
    "h5_file_paths = get_h5_file_paths_hdfs(spark_session, path)\n",
    "\n",
    "print('{} HDF5 file paths gathered (HDFS)'.format(len(h5_file_paths)))\n",
    "for file_path in h5_file_paths[:50]:\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa39ddd-8486-42f6-86fd-dc7741270519",
   "metadata": {},
   "source": [
    "# 2) List h5_file_paths is converted to RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a776fd3f-ad71-4b9e-96a0-0d37cc0aad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "part = h5_file_paths[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "667366d9-b3ca-46d8-8005-7651999e1aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 00:10:20 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 358.7 KiB, free 364.8 MiB)\n",
      "24/03/15 00:10:20 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 33.1 KiB, free 364.7 MiB)\n",
      "24/03/15 00:10:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on master-node:37591 (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:10:20 INFO SparkContext: Created broadcast 7 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 00:10:20 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/15 00:10:20 INFO SparkContext: Starting job: foreach at /tmp/ipykernel_231309/4209321439.py:12\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Got job 4 (foreach at /tmp/ipykernel_231309/4209321439.py:12) with 2 output partitions\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Final stage: ResultStage 4 (foreach at /tmp/ipykernel_231309/4209321439.py:12)\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[13] at foreach at /tmp/ipykernel_231309/4209321439.py:12), which has no missing parents\n",
      "24/03/15 00:10:20 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 10.4 KiB, free 364.7 MiB)\n",
      "24/03/15 00:10:20 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 364.7 MiB)\n",
      "24/03/15 00:10:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on master-node:37591 (size: 6.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:10:20 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 4 (PythonRDD[13] at foreach at /tmp/ipykernel_231309/4209321439.py:12) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/15 00:10:20 INFO YarnScheduler: Adding task set 4.0 with 2 tasks resource profile 0\n",
      "24/03/15 00:10:20 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 16) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:10:20 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 17) (worker-node-1, executor 2, partition 1, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:10:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on worker-node-2:39595 (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:10:20 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on worker-node-1:34791 (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:10:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on worker-node-2:39595 (size: 33.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:10:20 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on worker-node-1:34791 (size: 33.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:10:20 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 16) in 101 ms on worker-node-2 (executor 1) (1/2)\n",
      "24/03/15 00:10:20 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 17) in 124 ms on worker-node-1 (executor 2) (2/2)\n",
      "24/03/15 00:10:20 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/03/15 00:10:20 INFO DAGScheduler: ResultStage 4 (foreach at /tmp/ipykernel_231309/4209321439.py:12) finished in 0.149 s\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 00:10:20 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Job 4 finished: foreach at /tmp/ipykernel_231309/4209321439.py:12, took 0.154230 s\n",
      "24/03/15 00:10:20 INFO SparkContext: Starting job: collect at /tmp/ipykernel_231309/4209321439.py:18\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Got job 5 (collect at /tmp/ipykernel_231309/4209321439.py:18) with 2 output partitions\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /tmp/ipykernel_231309/4209321439.py:18)\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[14] at collect at /tmp/ipykernel_231309/4209321439.py:18), which has no missing parents\n",
      "24/03/15 00:10:20 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 8.3 KiB, free 364.7 MiB)\n",
      "24/03/15 00:10:20 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 364.7 MiB)\n",
      "24/03/15 00:10:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on master-node:37591 (size: 5.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:10:20 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (PythonRDD[14] at collect at /tmp/ipykernel_231309/4209321439.py:18) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/15 00:10:20 INFO YarnScheduler: Adding task set 5.0 with 2 tasks resource profile 0\n",
      "24/03/15 00:10:20 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 18) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:10:20 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 19) (worker-node-1, executor 2, partition 1, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:10:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on worker-node-2:39595 (size: 5.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:10:20 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on worker-node-1:34791 (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:10:20 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 18) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/15 00:10:20 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 20) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:10:20 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 19) (worker-node-1 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/15 00:10:20 INFO TaskSetManager: Starting task 1.1 in stage 5.0 (TID 21) (worker-node-1, executor 2, partition 1, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:10:20 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 20) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\n",
      "IndexError: list index out of range\n",
      ") [duplicate 1]\n",
      "24/03/15 00:10:20 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 22) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:10:20 INFO TaskSetManager: Lost task 1.1 in stage 5.0 (TID 21) on worker-node-1, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\n",
      "IndexError: list index out of range\n",
      ") [duplicate 1]\n",
      "24/03/15 00:10:20 INFO TaskSetManager: Starting task 1.2 in stage 5.0 (TID 23) (worker-node-1, executor 2, partition 1, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:10:20 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 22) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\n",
      "IndexError: list index out of range\n",
      ") [duplicate 2]\n",
      "24/03/15 00:10:20 INFO TaskSetManager: Starting task 0.3 in stage 5.0 (TID 24) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:10:20 INFO TaskSetManager: Lost task 1.2 in stage 5.0 (TID 23) on worker-node-1, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\n",
      "IndexError: list index out of range\n",
      ") [duplicate 2]\n",
      "24/03/15 00:10:20 INFO TaskSetManager: Starting task 1.3 in stage 5.0 (TID 25) (worker-node-1, executor 2, partition 1, NODE_LOCAL, 7710 bytes) \n",
      "24/03/15 00:10:20 INFO TaskSetManager: Lost task 0.3 in stage 5.0 (TID 24) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\n",
      "IndexError: list index out of range\n",
      ") [duplicate 3]\n",
      "24/03/15 00:10:20 ERROR TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job\n",
      "24/03/15 00:10:20 INFO YarnScheduler: Cancelling stage 5\n",
      "24/03/15 00:10:20 INFO YarnScheduler: Killing all running tasks in stage 5: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 24) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/15 00:10:20 INFO YarnScheduler: Stage 5 was cancelled\n",
      "24/03/15 00:10:20 INFO DAGScheduler: ResultStage 5 (collect at /tmp/ipykernel_231309/4209321439.py:18) failed in 0.156 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 24) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/15 00:10:20 INFO DAGScheduler: Job 5 failed: collect at /tmp/ipykernel_231309/4209321439.py:18, took 0.161560 s\n",
      "24/03/15 00:10:20 WARN TaskSetManager: Lost task 1.3 in stage 5.0 (TID 25) (worker-node-1 executor 2): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 24) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/03/15 00:10:20 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 24) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m rows \u001b[38;5;241m=\u001b[39m cleaned_rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m data: Row(column1\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m3\u001b[39m], column2\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m6\u001b[39m], column3\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m3\u001b[39m], column4\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m4\u001b[39m], column5\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m5\u001b[39m], column6\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m9\u001b[39m], column7\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m10\u001b[39m], column8\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m11\u001b[39m]))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Die Zeilen der gemeinsamen Liste hinzuf√ºgen\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m all_rows\u001b[38;5;241m.\u001b[39mextend(\u001b[43mrows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 24) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0014/container_1710430208522_0014_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_231309/4209321439.py\", line 15, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "\n",
    "# F√ºr jede Datei die Zeilen lesen, bereinigen und extrahieren\n",
    "for file_path in part:\n",
    "    # Datei lesen und Inhalt bereinigen\n",
    "    rdd = spark_context.textFile(file_path)\n",
    "    \n",
    "    # Transformationen anwenden, um den RDD-Inhalt zu bereinigen und in Teile zu splitten\n",
    "    cleaned_rdd = rdd.map(lambda line: line.replace('\\n', '').replace(' ', '').replace(',', '').split())\n",
    "\n",
    "    # Debugging-Anweisungen hinzuf√ºgen, um die Struktur der RDD-Elemente zu √ºberpr√ºfen\n",
    "    cleaned_rdd.foreach(lambda x: print(\"Debugging: \", x))\n",
    "    \n",
    "    # Zeilen f√ºr jede bereinigte Zeile im RDD erstellen\n",
    "    rows = cleaned_rdd.map(lambda data: Row(column1=data[3], column2=data[6], column3=data[3], column4=data[4], column5=data[5], column6=data[9], column7=data[10], column8=data[11]))\n",
    "    \n",
    "    # Die Zeilen der gemeinsamen Liste hinzuf√ºgen\n",
    "    all_rows.extend(rows.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1391e3-6cfe-4623-bc8e-d21ac0212b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = spark_context.parallelize(h5_file_paths)\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b8a3d-e994-4f86-8feb-e66f5105e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_first = file_paths.first()\n",
    "file_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5a877e2-6301-4dfe-8716-8dd0f4bd4d5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o23.wholeTextFiles. Trace:\npy4j.Py4JException: Method wholeTextFiles([class java.util.ArrayList, class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#file_paths = spark_context.wholeTextFiles(path).map(lambda x: x[0]).collect()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m file_paths \u001b[38;5;241m=\u001b[39m \u001b[43mspark_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwholeTextFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5_file_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/context.py:1070\u001b[0m, in \u001b[0;36mSparkContext.wholeTextFiles\u001b[0;34m(self, path, minPartitions, use_unicode)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;124;03mRead a directory of text files from HDFS, a local file system\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;124;03m(available on all nodes), or any  Hadoop-supported file system\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;124;03m[('.../1.txt', '123'), ('.../2.txt', 'xyz')]\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m minPartitions \u001b[38;5;241m=\u001b[39m minPartitions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaultMinPartitions\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m RDD(\n\u001b[0;32m-> 1070\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwholeTextFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminPartitions\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1072\u001b[0m     PairDeserializer(UTF8Deserializer(use_unicode), UTF8Deserializer(use_unicode)),\n\u001b[1;32m   1073\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o23.wholeTextFiles. Trace:\npy4j.Py4JException: Method wholeTextFiles([class java.util.ArrayList, class java.lang.Integer]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"
     ]
    }
   ],
   "source": [
    "#file_paths = spark_context.wholeTextFiles(path).map(lambda x: x[0]).collect()\n",
    "file_paths = spark_context.wholeTextFiles(foe e in part).map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "497d61ce-7f1d-40d7-b1ee-710783b5b820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:44:43 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 343.6 KiB, free 364.0 MiB)\n",
      "24/03/14 23:44:43 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 33.1 KiB, free 364.0 MiB)\n",
      "24/03/14 23:44:43 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on master-node:37353 (size: 33.1 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:44:43 INFO SparkContext: Created broadcast 9 from wholeTextFiles at NativeMethodAccessorImpl.java:0\n",
      "24/03/14 23:44:44 INFO BlockManagerInfo: Removed broadcast_8_piece0 on master-node:37353 in memory (size: 5.1 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:44:44 INFO BlockManagerInfo: Removed broadcast_8_piece0 on worker-node-2:33981 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/14 23:44:44 INFO BlockManagerInfo: Removed broadcast_8_piece0 on worker-node-1:35643 in memory (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/14 23:44:44 INFO FileInputFormat: Total input files to process : 10000\n",
      "24/03/14 23:44:45 INFO FileInputFormat: Total input files to process : 10000\n",
      "24/03/14 23:44:45 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 23:44:45 INFO DAGScheduler: Got job 4 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 23:44:45 INFO DAGScheduler: Final stage: ResultStage 4 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 23:44:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 23:44:45 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:44:45 INFO DAGScheduler: Submitting ResultStage 4 (PythonRDD[16] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 23:44:45 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 8.2 KiB, free 364.0 MiB)\n",
      "24/03/14 23:44:45 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 364.0 MiB)\n",
      "24/03/14 23:44:45 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on master-node:37353 (size: 5.0 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:44:45 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:44:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (PythonRDD[16] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 23:44:45 INFO YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/03/14 23:44:45 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 32) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 434776 bytes) \n",
      "24/03/14 23:44:45 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on worker-node-2:33981 (size: 5.0 KiB, free: 366.2 MiB)\n",
      "24/03/14 23:44:45 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on worker-node-2:33981 (size: 33.1 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:44:46 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 32) in 633 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/14 23:44:46 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/03/14 23:44:46 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 46467\n",
      "24/03/14 23:44:46 INFO DAGScheduler: ResultStage 4 (runJob at PythonRDD.scala:181) finished in 0.658 s\n",
      "24/03/14 23:44:46 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 23:44:46 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished\n",
      "24/03/14 23:44:46 INFO DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:181, took 0.666704 s\n",
      "24/03/14 23:44:46 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 358.3 KiB, free 363.7 MiB)\n",
      "24/03/14 23:44:46 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 32.9 KiB, free 363.6 MiB)\n",
      "24/03/14 23:44:46 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on master-node:37353 (size: 32.9 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:44:46 INFO SparkContext: Created broadcast 11 from textFile at NativeMethodAccessorImpl.java:0\n",
      "24/03/14 23:44:46 INFO FileInputFormat: Total input files to process : 1\n",
      "24/03/14 23:44:46 INFO SparkContext: Starting job: collect at /tmp/ipykernel_230216/2176338028.py:16\n",
      "24/03/14 23:44:46 INFO DAGScheduler: Got job 5 (collect at /tmp/ipykernel_230216/2176338028.py:16) with 2 output partitions\n",
      "24/03/14 23:44:46 INFO DAGScheduler: Final stage: ResultStage 5 (collect at /tmp/ipykernel_230216/2176338028.py:16)\n",
      "24/03/14 23:44:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 23:44:46 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:44:46 INFO DAGScheduler: Submitting ResultStage 5 (PythonRDD[19] at collect at /tmp/ipykernel_230216/2176338028.py:16), which has no missing parents\n",
      "24/03/14 23:44:46 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 8.3 KiB, free 363.6 MiB)\n",
      "24/03/14 23:44:46 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 363.6 MiB)\n",
      "24/03/14 23:44:46 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on master-node:37353 (size: 5.1 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:44:46 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:44:46 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (PythonRDD[19] at collect at /tmp/ipykernel_230216/2176338028.py:16) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 23:44:46 INFO YarnScheduler: Adding task set 5.0 with 2 tasks resource profile 0\n",
      "24/03/14 23:44:46 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 33) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/14 23:44:46 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 34) (worker-node-1, executor 2, partition 1, NODE_LOCAL, 7710 bytes) \n",
      "24/03/14 23:44:46 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on worker-node-1:35643 (size: 5.1 KiB, free: 366.2 MiB)\n",
      "24/03/14 23:44:46 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on worker-node-2:33981 (size: 5.1 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:44:46 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on worker-node-1:35643 (size: 32.9 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:44:46 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on worker-node-2:33981 (size: 32.9 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:44:46 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 33) (worker-node-2 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/14 23:44:46 INFO TaskSetManager: Starting task 0.1 in stage 5.0 (TID 35) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/14 23:44:46 WARN TaskSetManager: Lost task 1.0 in stage 5.0 (TID 34) (worker-node-1 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/03/14 23:44:46 INFO TaskSetManager: Starting task 1.1 in stage 5.0 (TID 36) (worker-node-1, executor 2, partition 1, NODE_LOCAL, 7710 bytes) \n",
      "24/03/14 23:44:46 INFO TaskSetManager: Lost task 0.1 in stage 5.0 (TID 35) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\n",
      "IndexError: list index out of range\n",
      ") [duplicate 1]\n",
      "24/03/14 23:44:46 INFO TaskSetManager: Starting task 0.2 in stage 5.0 (TID 37) (worker-node-1, executor 2, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/14 23:44:46 INFO TaskSetManager: Lost task 1.1 in stage 5.0 (TID 36) on worker-node-1, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\n",
      "IndexError: list index out of range\n",
      ") [duplicate 1]\n",
      "24/03/14 23:44:46 INFO TaskSetManager: Starting task 1.2 in stage 5.0 (TID 38) (worker-node-2, executor 1, partition 1, NODE_LOCAL, 7710 bytes) \n",
      "24/03/14 23:44:46 INFO TaskSetManager: Lost task 0.2 in stage 5.0 (TID 37) on worker-node-1, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\n",
      "IndexError: list index out of range\n",
      ") [duplicate 2]\n",
      "24/03/14 23:44:46 INFO TaskSetManager: Starting task 0.3 in stage 5.0 (TID 39) (worker-node-1, executor 2, partition 0, NODE_LOCAL, 7710 bytes) \n",
      "24/03/14 23:44:46 INFO TaskSetManager: Lost task 1.2 in stage 5.0 (TID 38) on worker-node-2, executor 1: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000002/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\n",
      "IndexError: list index out of range\n",
      ") [duplicate 2]\n",
      "24/03/14 23:44:46 INFO TaskSetManager: Starting task 1.3 in stage 5.0 (TID 40) (worker-node-2, executor 1, partition 1, NODE_LOCAL, 7710 bytes) \n",
      "24/03/14 23:44:46 INFO TaskSetManager: Lost task 0.3 in stage 5.0 (TID 39) on worker-node-1, executor 2: org.apache.spark.api.python.PythonException (Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\n",
      "IndexError: list index out of range\n",
      ") [duplicate 3]\n",
      "24/03/14 23:44:46 ERROR TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job\n",
      "24/03/14 23:44:46 INFO YarnScheduler: Cancelling stage 5\n",
      "24/03/14 23:44:46 INFO YarnScheduler: Killing all running tasks in stage 5: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 39) (worker-node-1 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/14 23:44:46 INFO YarnScheduler: Stage 5 was cancelled\n",
      "24/03/14 23:44:46 INFO DAGScheduler: ResultStage 5 (collect at /tmp/ipykernel_230216/2176338028.py:16) failed in 0.223 s due to Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 39) (worker-node-1 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "24/03/14 23:44:46 INFO DAGScheduler: Job 5 failed: collect at /tmp/ipykernel_230216/2176338028.py:16, took 0.227301 s\n",
      "24/03/14 23:44:46 WARN TaskSetManager: Lost task 1.3 in stage 5.0 (TID 40) (worker-node-2 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 39) (worker-node-1 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\n",
      "IndexError: list index out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/03/14 23:44:46 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 39) (worker-node-1 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m rows \u001b[38;5;241m=\u001b[39m cleaned_rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m data: Row(column1\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m3\u001b[39m], column2\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m6\u001b[39m], column3\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m28\u001b[39m], column4\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m25\u001b[39m], column5\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m8\u001b[39m], column6\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m9\u001b[39m], column7\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m10\u001b[39m], column8\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m11\u001b[39m]))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Append the rows to the common list\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m all_rows\u001b[38;5;241m.\u001b[39mextend(\u001b[43mrows\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 39) (worker-node-1 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/tmp/hadoop-hadoop/nm-local-dir/usercache/hadoop/appcache/application_1710430208522_0013/container_1710430208522_0013_01_000003/pyspark.zip/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_230216/2176338028.py\", line 13, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "file_paths = spark_context.wholeTextFiles(path).map(lambda x: x[0]).take(30)\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "for file_path in part:\n",
    "    # Read the file and clean the content\n",
    "    rdd = spark_context.textFile(file_path)\n",
    "    \n",
    "    # Apply transformations to clean the RDD content and split it into parts\n",
    "    cleaned_rdd = rdd.map(lambda line: line.replace('\\n', '').replace(' ', '').replace(',', '').split())\n",
    "    \n",
    "    # Create rows for each cleaned line in the RDD\n",
    "    rows = cleaned_rdd.map(lambda data: Row(column1=data[3], column2=data[6], column3=data[28], column4=data[25], column5=data[8], column6=data[9], column7=data[10], column8=data[11]))\n",
    "    \n",
    "    # Append the rows to the common list\n",
    "    all_rows.extend(rows.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7fdf26-2a79-4f32-a77d-1bb29c95e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"duration\", True),\n",
    "    StructField(\"end_of_fade_in\", FloatType(), True),\n",
    "    StructField(\"start_of_fade_out\", FloatType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "    StructField(\"key\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"time_signature\", FloatType(), True),\n",
    "    StructField(\"time_signature_confidence\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f51242-af28-436e-adb9-73567c5c0b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle DataFrame aus der Liste von Zeilen\n",
    "df = sqlContext.createDataFrame(all_rows, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240d7005-be56-491a-9ab1-6e6171a19597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:37:44 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 358.3 KiB, free 366.0 MiB)\n",
      "24/03/14 23:37:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.9 KiB, free 365.9 MiB)\n",
      "24/03/14 23:37:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on master-node:37353 (size: 32.9 KiB, free: 366.3 MiB)\n",
      "24/03/14 23:37:44 INFO SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'PipelinedRDD' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m content \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Erstelle Zeilen f√ºr jede Datei\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m rows \u001b[38;5;241m=\u001b[39m [Row(column1\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m5\u001b[39m], column2\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m6\u001b[39m], column3\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m28\u001b[39m], column4\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m25\u001b[39m], column5\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m23\u001b[39m], column6\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m29\u001b[39m], column7\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m30\u001b[39m], column8\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m31\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m [content]]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# F√ºge die Zeilen zur gemeinsamen Liste hinzu\u001b[39;00m\n\u001b[1;32m     12\u001b[0m all_rows\u001b[38;5;241m.\u001b[39mextend(rows\u001b[38;5;241m.\u001b[39mcollect())\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m content \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Erstelle Zeilen f√ºr jede Datei\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m rows \u001b[38;5;241m=\u001b[39m [Row(column1\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m, column2\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m6\u001b[39m], column3\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m28\u001b[39m], column4\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m25\u001b[39m], column5\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m23\u001b[39m], column6\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m29\u001b[39m], column7\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m30\u001b[39m], column8\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m31\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m [content]]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# F√ºge die Zeilen zur gemeinsamen Liste hinzu\u001b[39;00m\n\u001b[1;32m     12\u001b[0m all_rows\u001b[38;5;241m.\u001b[39mextend(rows\u001b[38;5;241m.\u001b[39mcollect())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PipelinedRDD' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "\n",
    "for file_path in part:\n",
    "    # Lese die Datei und bereinige den Inhalt\n",
    "    rdd = spark_context.textFile(file_path)\n",
    "    content = rdd.map(lambda line: line.replace('\\n', '').replace(' ', '').replace(',', ''))\n",
    "    \n",
    "    # Erstelle Zeilen f√ºr jede Datei\n",
    "    rows = [Row(column1=data[5], column2=data[6], column3=data[28], column4=data[25], column5=data[23], column6=data[29], column7=data[30], column8=data[31]) for data in [content]]\n",
    "    \n",
    "    # F√ºge die Zeilen zur gemeinsamen Liste hinzu\n",
    "    all_rows.extend(rows.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58fa953d-1217-4544-b6b4-84cdb30021fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:20:23 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/14 23:20:23 INFO DAGScheduler: Got job 380 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/14 23:20:23 INFO DAGScheduler: Final stage: ResultStage 386 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/14 23:20:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 23:20:23 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:20:23 INFO DAGScheduler: Submitting ResultStage 386 (MapPartitionsRDD[1155] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/14 23:20:23 INFO MemoryStore: Block broadcast_743 stored as values in memory (estimated size 14.7 KiB, free 347.0 MiB)\n",
      "24/03/14 23:20:23 INFO MemoryStore: Block broadcast_743_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 347.0 MiB)\n",
      "24/03/14 23:20:23 INFO BlockManagerInfo: Added broadcast_743_piece0 in memory on master-node:44443 (size: 7.0 KiB, free: 364.6 MiB)\n",
      "24/03/14 23:20:23 INFO SparkContext: Created broadcast 743 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:20:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 386 (MapPartitionsRDD[1155] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 23:20:23 INFO YarnScheduler: Adding task set 386.0 with 1 tasks resource profile 0\n",
      "24/03/14 23:20:23 INFO TaskSetManager: Starting task 0.0 in stage 386.0 (TID 747) (worker-node-1, executor 1, partition 0, PROCESS_LOCAL, 13411 bytes) \n",
      "24/03/14 23:20:23 INFO BlockManagerInfo: Added broadcast_743_piece0 in memory on worker-node-1:37441 (size: 7.0 KiB, free: 364.7 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|column1|column2|column3|column4|column5|column6|column7|column8|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|218.932|  0.247|218.932|-11.197|      1| 92.198|      4|  0.778|\n",
      "|148.035|  0.148|137.915| -9.843|      6|121.274|      4|  0.384|\n",
      "|177.475|  0.282|172.304| -9.689|      8| 100.07|      1|      0|\n",
      "|233.404|      0|217.124| -9.013|      0|119.293|      4|      0|\n",
      "|209.606|  0.066|198.699| -4.501|      2|129.738|      4|  0.562|\n",
      "|267.702|  2.264| 254.27| -9.323|      5|147.782|      3|  0.454|\n",
      "|114.782|  0.096|114.782|-17.302|      1|111.787|      1|      0|\n",
      "| 189.57|  0.319|181.023|-11.642|      4| 101.43|      3|  0.408|\n",
      "|269.818|    5.3| 258.99|-13.496|      4| 86.643|      4|  0.487|\n",
      "|266.396|  0.084|261.747| -6.697|      7|114.041|      4|  0.878|\n",
      "|218.775|  2.125|207.012|-10.021|      5|146.765|      1|      0|\n",
      "|245.211|  0.357| 227.48| -7.545|      7|117.975|      4|  0.835|\n",
      "|226.351|      0|221.553| -6.632|      9| 130.04|      4|      0|\n",
      "|191.843|   0.38|188.424|  -7.75|     10|137.334|      1|  0.319|\n",
      "|307.382|  0.612|296.658| -8.346|      3|125.197|      3|  0.211|\n",
      "|491.128|      0|486.034| -8.576|      7|119.826|      4|  0.756|\n",
      "|228.597|  0.223|217.426| -16.11|      8|127.756|      5|  0.579|\n",
      "|599.249|  1.193|591.999| -8.032|      6| 99.273|      4|  0.158|\n",
      "|290.298|  0.145|285.605| -5.271|      7|150.062|      4|  0.931|\n",
      "|165.694|  0.162|157.391| -6.787|      7|138.331|      4|  0.127|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:20:24 INFO TaskSetManager: Finished task 0.0 in stage 386.0 (TID 747) in 769 ms on worker-node-1 (executor 1) (1/1)\n",
      "24/03/14 23:20:24 INFO YarnScheduler: Removed TaskSet 386.0, whose tasks have all completed, from pool \n",
      "24/03/14 23:20:24 INFO DAGScheduler: ResultStage 386 (showString at NativeMethodAccessorImpl.java:0) finished in 0.777 s\n",
      "24/03/14 23:20:24 INFO DAGScheduler: Job 380 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 23:20:24 INFO YarnScheduler: Killing all running tasks in stage 386: Stage finished\n",
      "24/03/14 23:20:24 INFO DAGScheduler: Job 380 finished: showString at NativeMethodAccessorImpl.java:0, took 0.779313 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Erstelle DataFrame aus den gesammelten Zeilen\n",
    "schema = sqlContext.createDataFrame(all_rows)\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "schema.registerTempTable(\"combined_data_table\")\n",
    "\n",
    "# Zeige das Ergebnis\n",
    "result = sqlContext.sql(\"SELECT * FROM combined_data_table\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1552aabc-ee90-4bc9-8064-44d1360cbd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"duration\", True),\n",
    "    StructField(\"end_of_fade_in\", FloatType(), True),\n",
    "    StructField(\"start_of_fade_out\", FloatType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "    StructField(\"key\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"time_signature\", FloatType(), True),\n",
    "    StructField(\"time_signature_confidence\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b95f853-0bf6-46d7-8a28-a634a4d32824",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_ACCEPT_OBJECT_IN_TYPE] `FloatType()` can not accept object `218.932` in type `str`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Erstelle DataFrame aus den gesammelten Zeilen\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43msqlContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/context.py:486\u001b[0m, in \u001b[0;36mSQLContext.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateDataFrame\u001b[39m(  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    382\u001b[0m     data: Union[RDD[Any], Iterable[Any], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandasDataFrameLike\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m     verifySchema: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    386\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m    Py4JJavaError: ...\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:1090\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# make sure data could consumed multiple times\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m-> 1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferSchemaFromList(data, names\u001b[38;5;241m=\u001b[39mschema)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/session.py:1459\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe.<locals>.prepare\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(obj):\n\u001b[0;32m-> 1459\u001b[0m     \u001b[43mverify_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:2160\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_struct\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   2151\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLENGTH_SHOULD_BE_THE_SAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2152\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2157\u001b[0m             },\n\u001b[1;32m   2158\u001b[0m         )\n\u001b[1;32m   2159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, (_, verifier) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(obj, verifiers):\n\u001b[0;32m-> 2160\u001b[0m         \u001b[43mverifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2162\u001b[0m     d \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:2187\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2186\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m verify_nullability(obj):\n\u001b[0;32m-> 2187\u001b[0m         \u001b[43mverify_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:2181\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_default\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_default\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2180\u001b[0m     assert_acceptable_types(obj)\n\u001b[0;32m-> 2181\u001b[0m     \u001b[43mverify_acceptable_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/types.py:2006\u001b[0m, in \u001b[0;36m_make_type_verifier.<locals>.verify_acceptable_types\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mverify_acceptable_types\u001b[39m(obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2004\u001b[0m     \u001b[38;5;66;03m# subclass of them can not be fromInternal in JVM\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _acceptable_types[_type]:\n\u001b[0;32m-> 2006\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   2007\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_ACCEPT_OBJECT_IN_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2008\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   2009\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(dataType),\n\u001b[1;32m   2010\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(obj),\n\u001b[1;32m   2011\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobj_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   2012\u001b[0m             },\n\u001b[1;32m   2013\u001b[0m         )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_ACCEPT_OBJECT_IN_TYPE] `FloatType()` can not accept object `218.932` in type `str`."
     ]
    }
   ],
   "source": [
    "# Erstelle DataFrame aus den gesammelten Zeilen\n",
    "df = sqlContext.createDataFrame(all_rows, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91e7d473-5abb-4b5c-b3d4-b19015240039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:20:34 INFO DAGScheduler: Registering RDD 1162 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 6\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Got map stage job 381 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Final stage: ShuffleMapStage 387 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Submitting ShuffleMapStage 387 (MapPartitionsRDD[1162] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/14 23:20:34 INFO MemoryStore: Block broadcast_744 stored as values in memory (estimated size 16.1 KiB, free 347.0 MiB)\n",
      "24/03/14 23:20:34 INFO MemoryStore: Block broadcast_744_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 347.0 MiB)\n",
      "24/03/14 23:20:34 INFO BlockManagerInfo: Added broadcast_744_piece0 in memory on master-node:44443 (size: 8.3 KiB, free: 364.6 MiB)\n",
      "24/03/14 23:20:34 INFO SparkContext: Created broadcast 744 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 387 (MapPartitionsRDD[1162] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 23:20:34 INFO YarnScheduler: Adding task set 387.0 with 2 tasks resource profile 0\n",
      "24/03/14 23:20:34 INFO TaskSetManager: Starting task 0.0 in stage 387.0 (TID 748) (worker-node-2, executor 2, partition 0, PROCESS_LOCAL, 13400 bytes) \n",
      "24/03/14 23:20:34 INFO TaskSetManager: Starting task 1.0 in stage 387.0 (TID 749) (worker-node-1, executor 1, partition 1, PROCESS_LOCAL, 13422 bytes) \n",
      "24/03/14 23:20:34 INFO BlockManagerInfo: Added broadcast_744_piece0 in memory on worker-node-1:37441 (size: 8.3 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:34 INFO BlockManagerInfo: Added broadcast_744_piece0 in memory on worker-node-2:35017 (size: 8.3 KiB, free: 364.6 MiB)\n",
      "24/03/14 23:20:34 INFO TaskSetManager: Finished task 1.0 in stage 387.0 (TID 749) in 37 ms on worker-node-1 (executor 1) (1/2)\n",
      "24/03/14 23:20:34 INFO TaskSetManager: Finished task 0.0 in stage 387.0 (TID 748) in 59 ms on worker-node-2 (executor 2) (2/2)\n",
      "24/03/14 23:20:34 INFO YarnScheduler: Removed TaskSet 387.0, whose tasks have all completed, from pool \n",
      "24/03/14 23:20:34 INFO DAGScheduler: ShuffleMapStage 387 (count at NativeMethodAccessorImpl.java:0) finished in 0.067 s\n",
      "24/03/14 23:20:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/14 23:20:34 INFO DAGScheduler: running: Set()\n",
      "24/03/14 23:20:34 INFO DAGScheduler: waiting: Set()\n",
      "24/03/14 23:20:34 INFO DAGScheduler: failed: Set()\n",
      "24/03/14 23:20:34 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Got job 382 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Final stage: ResultStage 389 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 388)\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Submitting ResultStage 389 (MapPartitionsRDD[1165] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/14 23:20:34 INFO MemoryStore: Block broadcast_745 stored as values in memory (estimated size 12.5 KiB, free 347.0 MiB)\n",
      "24/03/14 23:20:34 INFO MemoryStore: Block broadcast_745_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 347.0 MiB)\n",
      "24/03/14 23:20:34 INFO BlockManagerInfo: Added broadcast_745_piece0 in memory on master-node:44443 (size: 5.9 KiB, free: 364.6 MiB)\n",
      "24/03/14 23:20:34 INFO SparkContext: Created broadcast 745 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 389 (MapPartitionsRDD[1165] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 23:20:34 INFO YarnScheduler: Adding task set 389.0 with 1 tasks resource profile 0\n",
      "24/03/14 23:20:34 INFO TaskSetManager: Starting task 0.0 in stage 389.0 (TID 750) (worker-node-1, executor 1, partition 0, NODE_LOCAL, 7626 bytes) \n",
      "24/03/14 23:20:34 INFO BlockManagerInfo: Added broadcast_745_piece0 in memory on worker-node-1:37441 (size: 5.9 KiB, free: 364.6 MiB)\n",
      "24/03/14 23:20:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 192.168.2.124:60916\n",
      "24/03/14 23:20:34 INFO TaskSetManager: Finished task 0.0 in stage 389.0 (TID 750) in 25 ms on worker-node-1 (executor 1) (1/1)\n",
      "24/03/14 23:20:34 INFO YarnScheduler: Removed TaskSet 389.0, whose tasks have all completed, from pool \n",
      "24/03/14 23:20:34 INFO DAGScheduler: ResultStage 389 (count at NativeMethodAccessorImpl.java:0) finished in 0.031 s\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Job 382 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 23:20:34 INFO YarnScheduler: Killing all running tasks in stage 389: Stage finished\n",
      "24/03/14 23:20:34 INFO DAGScheduler: Job 382 finished: count at NativeMethodAccessorImpl.java:0, took 0.033571 s\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in the DataFrame:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8841cb17-41ab-499c-93e9-699421e8635f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|column1|column2|column3|column4|column5|column6|column7|column8|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "|218.932|  0.247|218.932|-11.197|      1| 92.198|      4|  0.778|\n",
      "|148.035|  0.148|137.915| -9.843|      6|121.274|      4|  0.384|\n",
      "|177.475|  0.282|172.304| -9.689|      8| 100.07|      1|      0|\n",
      "|233.404|      0|217.124| -9.013|      0|119.293|      4|      0|\n",
      "|209.606|  0.066|198.699| -4.501|      2|129.738|      4|  0.562|\n",
      "|267.702|  2.264| 254.27| -9.323|      5|147.782|      3|  0.454|\n",
      "|114.782|  0.096|114.782|-17.302|      1|111.787|      1|      0|\n",
      "| 189.57|  0.319|181.023|-11.642|      4| 101.43|      3|  0.408|\n",
      "|269.818|    5.3| 258.99|-13.496|      4| 86.643|      4|  0.487|\n",
      "|266.396|  0.084|261.747| -6.697|      7|114.041|      4|  0.878|\n",
      "|218.775|  2.125|207.012|-10.021|      5|146.765|      1|      0|\n",
      "|245.211|  0.357| 227.48| -7.545|      7|117.975|      4|  0.835|\n",
      "|226.351|      0|221.553| -6.632|      9| 130.04|      4|      0|\n",
      "|191.843|   0.38|188.424|  -7.75|     10|137.334|      1|  0.319|\n",
      "|307.382|  0.612|296.658| -8.346|      3|125.197|      3|  0.211|\n",
      "|491.128|      0|486.034| -8.576|      7|119.826|      4|  0.756|\n",
      "|228.597|  0.223|217.426| -16.11|      8|127.756|      5|  0.579|\n",
      "|599.249|  1.193|591.999| -8.032|      6| 99.273|      4|  0.158|\n",
      "|290.298|  0.145|285.605| -5.271|      7|150.062|      4|  0.931|\n",
      "|165.694|  0.162|157.391| -6.787|      7|138.331|      4|  0.127|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_730_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 364.6 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_730_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 364.6 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_730_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 364.6 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_733_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 364.6 MiB)\n",
      "24/03/14 23:20:37 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_733_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_733_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO DAGScheduler: Got job 383 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/14 23:20:37 INFO DAGScheduler: Final stage: ResultStage 390 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/14 23:20:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 23:20:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:20:37 INFO DAGScheduler: Submitting ResultStage 390 (MapPartitionsRDD[1167] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/14 23:20:37 INFO MemoryStore: Block broadcast_746 stored as values in memory (estimated size 14.7 KiB, free 347.4 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_724_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_724_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_724_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO MemoryStore: Block broadcast_746_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 347.4 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Added broadcast_746_piece0 in memory on master-node:44443 (size: 7.0 KiB, free: 364.6 MiB)\n",
      "24/03/14 23:20:37 INFO SparkContext: Created broadcast 746 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:20:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 390 (MapPartitionsRDD[1167] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 23:20:37 INFO YarnScheduler: Adding task set 390.0 with 1 tasks resource profile 0\n",
      "24/03/14 23:20:37 INFO TaskSetManager: Starting task 0.0 in stage 390.0 (TID 751) (worker-node-1, executor 1, partition 0, PROCESS_LOCAL, 13411 bytes) \n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_731_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_731_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_731_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Added broadcast_746_piece0 in memory on worker-node-1:37441 (size: 7.0 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_729_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_729_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_729_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_738_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_738_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_738_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO TaskSetManager: Finished task 0.0 in stage 390.0 (TID 751) in 18 ms on worker-node-1 (executor 1) (1/1)\n",
      "24/03/14 23:20:37 INFO YarnScheduler: Removed TaskSet 390.0, whose tasks have all completed, from pool \n",
      "24/03/14 23:20:37 INFO DAGScheduler: ResultStage 390 (showString at NativeMethodAccessorImpl.java:0) finished in 0.024 s\n",
      "24/03/14 23:20:37 INFO DAGScheduler: Job 383 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 23:20:37 INFO YarnScheduler: Killing all running tasks in stage 390: Stage finished\n",
      "24/03/14 23:20:37 INFO DAGScheduler: Job 383 finished: showString at NativeMethodAccessorImpl.java:0, took 0.026688 s\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_722_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_722_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_722_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 364.7 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_715_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_715_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_715_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_707_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_707_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_707_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_719_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_719_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_719_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_726_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_726_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_726_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_734_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_734_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_734_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 364.8 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_725_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_725_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_725_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_723_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_723_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_723_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_727_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_727_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_727_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 364.9 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_735_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_735_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_735_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_711_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_711_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_711_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_728_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_728_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_728_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_732_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_732_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_732_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_744_piece0 on master-node:44443 in memory (size: 8.3 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_744_piece0 on worker-node-2:35017 in memory (size: 8.3 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_744_piece0 on worker-node-1:37441 in memory (size: 8.3 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_717_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_717_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_717_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_743_piece0 on master-node:44443 in memory (size: 7.0 KiB, free: 365.0 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_743_piece0 on worker-node-1:37441 in memory (size: 7.0 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_745_piece0 on master-node:44443 in memory (size: 5.9 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_745_piece0 on worker-node-1:37441 in memory (size: 5.9 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_709_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_709_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_709_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_736_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_736_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_736_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_713_piece0 on worker-node-2:35017 in memory (size: 33.1 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_713_piece0 on worker-node-1:37441 in memory (size: 33.1 KiB, free: 365.2 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_713_piece0 on master-node:44443 in memory (size: 33.1 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_742_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_742_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_742_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 365.2 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_740_piece0 on master-node:44443 in memory (size: 4.7 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_740_piece0 on worker-node-2:35017 in memory (size: 4.7 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:37 INFO BlockManagerInfo: Removed broadcast_740_piece0 on worker-node-1:37441 in memory (size: 4.7 KiB, free: 365.2 MiB)\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c8fbf83-5cac-4709-8307-91d008195c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchschnittswerte:\n",
      "avg(column3): 230.37028000000007\n",
      "avg(column1): 239.11645049999998\n",
      "avg(column6): 127.39284500000002\n",
      "avg(column2): 0.8018450000000003\n",
      "avg(column7): 3.415\n",
      "avg(column5): 5.445\n",
      "avg(column4): -10.243735000000001\n",
      "avg(column8): 0.5016700000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:20:41 INFO DAGScheduler: Registering RDD 1169 (collect at /tmp/ipykernel_226860/960308202.py:2) as input to shuffle 7\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Got map stage job 384 (collect at /tmp/ipykernel_226860/960308202.py:2) with 2 output partitions\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Final stage: ShuffleMapStage 391 (collect at /tmp/ipykernel_226860/960308202.py:2)\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Submitting ShuffleMapStage 391 (MapPartitionsRDD[1169] at collect at /tmp/ipykernel_226860/960308202.py:2), which has no missing parents\n",
      "24/03/14 23:20:41 INFO MemoryStore: Block broadcast_747 stored as values in memory (estimated size 40.0 KiB, free 352.5 MiB)\n",
      "24/03/14 23:20:41 INFO MemoryStore: Block broadcast_747_piece0 stored as bytes in memory (estimated size 14.7 KiB, free 352.5 MiB)\n",
      "24/03/14 23:20:41 INFO BlockManagerInfo: Added broadcast_747_piece0 in memory on master-node:44443 (size: 14.7 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:41 INFO SparkContext: Created broadcast 747 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 391 (MapPartitionsRDD[1169] at collect at /tmp/ipykernel_226860/960308202.py:2) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/14 23:20:41 INFO YarnScheduler: Adding task set 391.0 with 2 tasks resource profile 0\n",
      "24/03/14 23:20:41 INFO TaskSetManager: Starting task 0.0 in stage 391.0 (TID 752) (worker-node-1, executor 1, partition 0, PROCESS_LOCAL, 13400 bytes) \n",
      "24/03/14 23:20:41 INFO TaskSetManager: Starting task 1.0 in stage 391.0 (TID 753) (worker-node-2, executor 2, partition 1, PROCESS_LOCAL, 13422 bytes) \n",
      "24/03/14 23:20:41 INFO BlockManagerInfo: Added broadcast_747_piece0 in memory on worker-node-2:35017 (size: 14.7 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:41 INFO BlockManagerInfo: Added broadcast_747_piece0 in memory on worker-node-1:37441 (size: 14.7 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:41 INFO TaskSetManager: Finished task 1.0 in stage 391.0 (TID 753) in 21 ms on worker-node-2 (executor 2) (1/2)\n",
      "24/03/14 23:20:41 INFO TaskSetManager: Finished task 0.0 in stage 391.0 (TID 752) in 30 ms on worker-node-1 (executor 1) (2/2)\n",
      "24/03/14 23:20:41 INFO YarnScheduler: Removed TaskSet 391.0, whose tasks have all completed, from pool \n",
      "24/03/14 23:20:41 INFO DAGScheduler: ShuffleMapStage 391 (collect at /tmp/ipykernel_226860/960308202.py:2) finished in 0.043 s\n",
      "24/03/14 23:20:41 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/14 23:20:41 INFO DAGScheduler: running: Set()\n",
      "24/03/14 23:20:41 INFO DAGScheduler: waiting: Set()\n",
      "24/03/14 23:20:41 INFO DAGScheduler: failed: Set()\n",
      "24/03/14 23:20:41 INFO SparkContext: Starting job: collect at /tmp/ipykernel_226860/960308202.py:2\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Got job 385 (collect at /tmp/ipykernel_226860/960308202.py:2) with 1 output partitions\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Final stage: ResultStage 393 (collect at /tmp/ipykernel_226860/960308202.py:2)\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 392)\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Submitting ResultStage 393 (MapPartitionsRDD[1172] at collect at /tmp/ipykernel_226860/960308202.py:2), which has no missing parents\n",
      "24/03/14 23:20:41 INFO MemoryStore: Block broadcast_748 stored as values in memory (estimated size 30.4 KiB, free 352.4 MiB)\n",
      "24/03/14 23:20:41 INFO MemoryStore: Block broadcast_748_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 352.4 MiB)\n",
      "24/03/14 23:20:41 INFO BlockManagerInfo: Added broadcast_748_piece0 in memory on master-node:44443 (size: 10.4 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:41 INFO SparkContext: Created broadcast 748 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:20:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 393 (MapPartitionsRDD[1172] at collect at /tmp/ipykernel_226860/960308202.py:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 23:20:41 INFO YarnScheduler: Adding task set 393.0 with 1 tasks resource profile 0\n",
      "24/03/14 23:20:41 INFO TaskSetManager: Starting task 0.0 in stage 393.0 (TID 754) (worker-node-1, executor 1, partition 0, NODE_LOCAL, 7626 bytes) \n",
      "24/03/14 23:20:41 INFO BlockManagerInfo: Added broadcast_748_piece0 in memory on worker-node-1:37441 (size: 10.4 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:20:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 192.168.2.124:60916\n",
      "24/03/14 23:20:42 INFO TaskSetManager: Finished task 0.0 in stage 393.0 (TID 754) in 91 ms on worker-node-1 (executor 1) (1/1)\n",
      "24/03/14 23:20:42 INFO YarnScheduler: Removed TaskSet 393.0, whose tasks have all completed, from pool \n",
      "24/03/14 23:20:42 INFO DAGScheduler: ResultStage 393 (collect at /tmp/ipykernel_226860/960308202.py:2) finished in 0.098 s\n",
      "24/03/14 23:20:42 INFO DAGScheduler: Job 385 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 23:20:42 INFO YarnScheduler: Killing all running tasks in stage 393: Stage finished\n",
      "24/03/14 23:20:42 INFO DAGScheduler: Job 385 finished: collect at /tmp/ipykernel_226860/960308202.py:2, took 0.102913 s\n"
     ]
    }
   ],
   "source": [
    "# F√ºhre die angegebene Abfrage aus, um durchschnittliche Werte zu berechnen\n",
    "average_values = df.agg({'column1': 'avg', 'column2': 'avg', 'column3': 'avg', 'column4': 'avg', 'column5': 'avg', 'column6': 'avg', 'column7': 'avg', 'column8': 'avg'}).collect()[0]\n",
    "print(\"Durchschnittswerte:\")\n",
    "for col_name, avg_value in average_values.asDict().items():\n",
    "    print(f\"{col_name}: {avg_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7ab67-3b12-4a91-9de7-d1403277d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark_context.textFile('hdfs://master-node:9000/user/hadoop/MillionSongSubset_ASCI_analysis_songs/TRAAAAW128F429D538_songs.asci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0f8f6-d57a-4aae-afdf-c2624c382129",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda line: line.strip('{}').split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327c752-ccef-44fe-ad52-57828e887c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda row: [str(elem) for elem in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22a541-74d1-4986-814d-1a087a4d5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e258a-7dfd-4940-a6ae-8bb091eeea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f78bb-e817-474d-acf5-c0c33bed8740",
   "metadata": {},
   "source": [
    "# 3) H5Py file object file creation from the H5 file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e74778-0a7d-4324-8ec2-209858a66edc",
   "metadata": {},
   "source": [
    "# Debug: Only one h5File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ba988-e260-47c7-b9b3-d5d5c9b910ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file_paths = ['hdfs://master-node:9000/user/hadoop/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5']\n",
    "h5_file_path = 'hdfs://master-node:9000/user/hadoop/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37181a-309b-419c-964e-d48d275fa6d1",
   "metadata": {},
   "source": [
    "# Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6da36c-82e1-4874-af26-f0d9bcbb5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(file_path):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        song = file['analysis']['songs']\n",
    "        np_song = np.array(song[0])\n",
    "        selected_features = np_song[['danceability', 'duration', 'tempo', 'energy', 'loudness']].ravel()\n",
    "        return selected_features.tolist()\n",
    "        #print(f\"File found: {file_path}\")\n",
    "         #return file_path\n",
    "    # except FileNotFoundError:\n",
    "    #     print(f\"File not found: {file_path}\")\n",
    "    #     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc9696-8ccd-476e-8fe4-867491f32e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = spark_context.parallelize(h5_file_path)\n",
    "files_infos_rdd = file_paths.map(extract_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29e487-ef1d-46c0-829f-708203dafb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files_infos_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897df25a-b182-41a3-92a1-f61f9492ddbe",
   "metadata": {},
   "source": [
    "# 4) Create Spark Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6348bf-0f3e-455e-af23-1bf886a2e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere das Schema f√ºr den DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"danceability\", FloatType(), True),\n",
    "    StructField(\"duration\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"energy\", FloatType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51be1c-caa7-4443-8ecc-e0dece7f6603",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = sqlContext.createDataFrame(files_infos_rdd.flatMap(lambda x: x), schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da17ea8-b186-406f-bf62-f36affa66f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige den DataFrame-Inhalt\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75edaf8-3e1f-4173-8f65-b8852bb3b27e",
   "metadata": {},
   "source": [
    "# Calculation (Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d4735-196d-40f3-87be-b41dcdabfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_values = spark_df.agg({'danceability': 'avg', 'duration': 'avg', 'tempo': 'avg', 'energy': 'avg', 'loudness': 'avg'}).collect()[0]\n",
    "print(\"Average Values::\")\n",
    "for col_name, avg_value in average_values.asDict().items():\n",
    "    print(f\"{col_name}: {avg_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ca167-67d5-4b4e-857c-b11709b1698e",
   "metadata": {},
   "source": [
    "# STOP SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23825d7-4cce-458e-82bc-b4dc6995cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39436e22-7c9e-4b0d-ab08-95942846a58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
