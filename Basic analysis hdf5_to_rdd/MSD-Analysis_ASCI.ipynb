{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a2ed10a-10d1-4734-bbdf-1d54a99fd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTHON\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pyspark\n",
    "\n",
    "#SPARK\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "\n",
    "import sys\n",
    "\n",
    "# Append the path to Pydoop to sys.path\n",
    "#sys.path.append(\"/usr/local/lib/python3.8/dist-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3096f2-39bc-4025-9c2c-a216255bdf3f",
   "metadata": {},
   "source": [
    "Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6fb6d6aa-4b8d-4c65-8dc1-a61c5d94018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark Session - 2 CORE - imporvement CONFIGURATION!!\n",
    "# spark_session = SparkSession.builder\\\n",
    "#     .master(\"local[2]\")\\\n",
    "#     .appName(\"pseudo_spark_nora\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Set Hadoop configuration directory\n",
    "#os.environ['HADOOP_CONF_DIR'] = '/path/to/hadoop/conf'\n",
    "#spark_session.stop()\n",
    "#os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"HDFS_Connection_Test_nora\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# num executors \n",
    "# executor memory RAM \n",
    "\n",
    "\n",
    "# spark_session = SparkSession.builder \\\n",
    "#     .appName(\"HDFS_Connection_Test_nora\") \\\n",
    "#     .master(\"yarn\") \\\n",
    "#     .config(\"spark.executor.instances\", \"2\") \\\n",
    "#     .config(\"spark.hadoop.fs.default.name\", \"hdfs://master-node:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://master-node:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.dfs.server.namenode.class\", \"org.apache.hadoop.hdfs.server.namenode.NameNode\") \\\n",
    "#     .config(\"spark.hadoop.conf\", \"org.apache.hadoop.hdfs.HdfsConfiguration\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "#.config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "#.config(\"spark.cores.max\", 4)\\\n",
    "\n",
    "\n",
    "#.config(\"spark.jars.packages\", \"LLNL:spark-hdf5:0.0.4\") \\\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"INFO\")\n",
    "\n",
    "sqlContext = SQLContext(spark_session.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44667861-4446-4677-adf9-5acf17142cc4",
   "metadata": {},
   "source": [
    "File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f72586dd-41ed-4710-8c15-6cbbe47debcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use Spark to list files in HDFS directory\n",
    "# hdfs_files = spark_session._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()) \\\n",
    "#     .listStatus(spark_session._jvm.org.apache.hadoop.fs.Path(directory_path))\n",
    "#path = 'hdfs://master-node:9000/user/hadoop/MillionSongSubset_ASCI_analysis_songs/TRAAAAW128F429D538_songs.asci'\n",
    "path = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd1391e3-6cfe-4623-bc8e-d21ac0212b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1175] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = spark_context.parallelize(path)\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc5b8a3d-e994-4f86-8feb-e66f5105e5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:29:46 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 23:29:46 INFO DAGScheduler: Got job 387 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 23:29:46 INFO DAGScheduler: Final stage: ResultStage 395 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 23:29:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 23:29:46 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:29:46 INFO DAGScheduler: Submitting ResultStage 395 (PythonRDD[1176] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 23:29:46 INFO MemoryStore: Block broadcast_750 stored as values in memory (estimated size 5.9 KiB, free 352.4 MiB)\n",
      "24/03/14 23:29:46 INFO MemoryStore: Block broadcast_750_piece0 stored as bytes in memory (estimated size 3.8 KiB, free 352.4 MiB)\n",
      "24/03/14 23:29:46 INFO BlockManagerInfo: Added broadcast_750_piece0 in memory on master-node:44443 (size: 3.8 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:29:46 INFO SparkContext: Created broadcast 750 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:29:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 395 (PythonRDD[1176] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 23:29:46 INFO YarnScheduler: Adding task set 395.0 with 1 tasks resource profile 0\n",
      "24/03/14 23:29:46 INFO TaskSetManager: Starting task 0.0 in stage 395.0 (TID 756) (worker-node-1, executor 1, partition 0, PROCESS_LOCAL, 7708 bytes) \n",
      "24/03/14 23:29:46 INFO BlockManagerInfo: Added broadcast_750_piece0 in memory on worker-node-1:37441 (size: 3.8 KiB, free: 365.1 MiB)\n",
      "24/03/14 23:29:46 INFO TaskSetManager: Finished task 0.0 in stage 395.0 (TID 756) in 53 ms on worker-node-1 (executor 1) (1/1)\n",
      "24/03/14 23:29:46 INFO YarnScheduler: Removed TaskSet 395.0, whose tasks have all completed, from pool \n",
      "24/03/14 23:29:46 INFO DAGScheduler: ResultStage 395 (runJob at PythonRDD.scala:181) finished in 0.059 s\n",
      "24/03/14 23:29:46 INFO DAGScheduler: Job 387 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 23:29:46 INFO YarnScheduler: Killing all running tasks in stage 395: Stage finished\n",
      "24/03/14 23:29:46 INFO DAGScheduler: Job 387 finished: runJob at PythonRDD.scala:181, took 0.061541 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'h'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_first = file_paths.first()\n",
    "file_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5a877e2-6301-4dfe-8716-8dd0f4bd4d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:33:27 INFO MemoryStore: Block broadcast_756 stored as values in memory (estimated size 344.0 KiB, free 364.4 MiB)\n",
      "24/03/14 23:33:27 INFO MemoryStore: Block broadcast_756_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 364.4 MiB)\n",
      "24/03/14 23:33:27 INFO BlockManagerInfo: Added broadcast_756_piece0 in memory on master-node:44443 (size: 33.3 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:33:27 INFO SparkContext: Created broadcast 756 from wholeTextFiles at NativeMethodAccessorImpl.java:0\n",
      "24/03/14 23:33:27 INFO FileInputFormat: Total input files to process : 10000\n",
      "24/03/14 23:33:28 INFO FileInputFormat: Total input files to process : 10000\n",
      "24/03/14 23:33:28 INFO SparkContext: Starting job: runJob at PythonRDD.scala:181\n",
      "24/03/14 23:33:28 INFO DAGScheduler: Got job 390 (runJob at PythonRDD.scala:181) with 1 output partitions\n",
      "24/03/14 23:33:28 INFO DAGScheduler: Final stage: ResultStage 398 (runJob at PythonRDD.scala:181)\n",
      "24/03/14 23:33:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/14 23:33:28 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/14 23:33:28 INFO DAGScheduler: Submitting ResultStage 398 (PythonRDD[1187] at RDD at PythonRDD.scala:53), which has no missing parents\n",
      "24/03/14 23:33:28 INFO MemoryStore: Block broadcast_757 stored as values in memory (estimated size 8.2 KiB, free 364.4 MiB)\n",
      "24/03/14 23:33:28 INFO MemoryStore: Block broadcast_757_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 364.4 MiB)\n",
      "24/03/14 23:33:28 INFO BlockManagerInfo: Added broadcast_757_piece0 in memory on master-node:44443 (size: 5.0 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:33:28 INFO SparkContext: Created broadcast 757 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/14 23:33:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 398 (PythonRDD[1187] at RDD at PythonRDD.scala:53) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/14 23:33:28 INFO YarnScheduler: Adding task set 398.0 with 1 tasks resource profile 0\n",
      "24/03/14 23:33:28 INFO TaskSetManager: Starting task 0.0 in stage 398.0 (TID 759) (worker-node-2, executor 2, partition 0, NODE_LOCAL, 434776 bytes) \n",
      "24/03/14 23:33:28 INFO BlockManagerInfo: Added broadcast_757_piece0 in memory on worker-node-2:35017 (size: 5.0 KiB, free: 366.2 MiB)\n",
      "24/03/14 23:33:28 INFO BlockManagerInfo: Added broadcast_756_piece0 in memory on worker-node-2:35017 (size: 33.3 KiB, free: 366.2 MiB)\n",
      "24/03/14 23:33:28 INFO TaskSetManager: Finished task 0.0 in stage 398.0 (TID 759) in 319 ms on worker-node-2 (executor 2) (1/1)\n",
      "24/03/14 23:33:28 INFO YarnScheduler: Removed TaskSet 398.0, whose tasks have all completed, from pool \n",
      "24/03/14 23:33:28 INFO DAGScheduler: ResultStage 398 (runJob at PythonRDD.scala:181) finished in 0.329 s\n",
      "24/03/14 23:33:28 INFO DAGScheduler: Job 390 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/14 23:33:28 INFO YarnScheduler: Killing all running tasks in stage 398: Stage finished\n",
      "24/03/14 23:33:28 INFO DAGScheduler: Job 390 finished: runJob at PythonRDD.scala:181, took 0.332867 s\n"
     ]
    }
   ],
   "source": [
    "#file_paths = spark_context.wholeTextFiles(path).map(lambda x: x[0]).collect()\n",
    "file_paths = spark_context.wholeTextFiles(path).map(lambda x: x[0]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0b6a04f-a40d-457d-b318-eb6f8f3eb378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 23:33:34 INFO MemoryStore: Block broadcast_758 stored as values in memory (estimated size 358.7 KiB, free 364.1 MiB)\n",
      "24/03/14 23:33:34 INFO MemoryStore: Block broadcast_758_piece0 stored as bytes in memory (estimated size 33.1 KiB, free 364.0 MiB)\n",
      "24/03/14 23:33:34 INFO BlockManagerInfo: Added broadcast_758_piece0 in memory on master-node:44443 (size: 33.1 KiB, free: 366.1 MiB)\n",
      "24/03/14 23:33:34 INFO SparkContext: Created broadcast 758 from textFile at <unknown>:0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'PipelinedRDD' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m content \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Erstelle Zeilen für jede Datei\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m rows \u001b[38;5;241m=\u001b[39m [Row(column1\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m5\u001b[39m], column2\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m6\u001b[39m], column3\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m28\u001b[39m], column4\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m25\u001b[39m], column5\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m23\u001b[39m], column6\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m29\u001b[39m], column7\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m30\u001b[39m], column8\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m31\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m [content]]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Füge die Zeilen zur gemeinsamen Liste hinzu\u001b[39;00m\n\u001b[1;32m     12\u001b[0m all_rows\u001b[38;5;241m.\u001b[39mextend(rows\u001b[38;5;241m.\u001b[39mcollect())\n",
      "Cell \u001b[0;32mIn[43], line 9\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m content \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Erstelle Zeilen für jede Datei\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m rows \u001b[38;5;241m=\u001b[39m [Row(column1\u001b[38;5;241m=\u001b[39m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m, column2\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m6\u001b[39m], column3\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m28\u001b[39m], column4\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m25\u001b[39m], column5\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m23\u001b[39m], column6\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m29\u001b[39m], column7\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m30\u001b[39m], column8\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;241m31\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m [content]]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Füge die Zeilen zur gemeinsamen Liste hinzu\u001b[39;00m\n\u001b[1;32m     12\u001b[0m all_rows\u001b[38;5;241m.\u001b[39mextend(rows\u001b[38;5;241m.\u001b[39mcollect())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'PipelinedRDD' object is not subscriptable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 00:02:40 INFO BlockManagerInfo: Removed broadcast_756_piece0 on master-node:44443 in memory (size: 33.3 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:02:40 INFO BlockManagerInfo: Removed broadcast_756_piece0 on worker-node-2:35017 in memory (size: 33.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 00:02:40 INFO BlockManagerInfo: Removed broadcast_757_piece0 on master-node:44443 in memory (size: 5.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 00:02:40 INFO BlockManagerInfo: Removed broadcast_757_piece0 on worker-node-2:35017 in memory (size: 5.0 KiB, free: 366.2 MiB)\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Lese die Datei und bereinige den Inhalt\n",
    "    rdd = spark_context.textFile(file_path)\n",
    "    content = rdd.map(lambda line: line.replace('\\n', '').replace(' ', '').replace(',', ''))\n",
    "    \n",
    "    # Erstelle Zeilen für jede Datei\n",
    "    rows = [Row(column1=data[5], column2=data[6], column3=data[28], column4=data[25], column5=data[23], column6=data[29], column7=data[30], column8=data[31]) for data in [content]]\n",
    "    \n",
    "    # Füge die Zeilen zur gemeinsamen Liste hinzu\n",
    "    all_rows.extend(rows.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240d7005-be56-491a-9ab1-6e6171a19597",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Lese die Datei und bereinige den Inhalt\n",
    "    rdd = spark_context.textFile(file_path)\n",
    "    content = rdd.map(lambda line: line.replace('\\n', '').replace(' ', '').replace(',', '')).collect()\n",
    "    \n",
    "    # Erstelle Zeilen für jede Datei\n",
    "    rows = [Row(column1=data[5], column2=data[6], column3=data[28], column4=data[25], column5=data[23], column6=data[29], column7=data[30], column8=data[31]) for data in [content]]\n",
    "    \n",
    "    # Füge die Zeilen zur gemeinsamen Liste hinzu\n",
    "    all_rows.extend(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa953d-1217-4544-b6b4-84cdb30021fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle DataFrame aus den gesammelten Zeilen\n",
    "schema = sqlContext.createDataFrame(all_rows)\n",
    "\n",
    "# Register the DataFrame as a table\n",
    "schema.registerTempTable(\"combined_data_table\")\n",
    "\n",
    "# Zeige das Ergebnis\n",
    "result = sqlContext.sql(\"SELECT * FROM combined_data_table\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552aabc-ee90-4bc9-8064-44d1360cbd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"duration\", True),\n",
    "    StructField(\"end_of_fade_in\", FloatType(), True),\n",
    "    StructField(\"start_of_fade_out\", FloatType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "    StructField(\"key\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"time_signature\", FloatType(), True),\n",
    "    StructField(\"time_signature_confidence\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95f853-0bf6-46d7-8a28-a634a4d32824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle DataFrame aus den gesammelten Zeilen\n",
    "df = sqlContext.createDataFrame(all_rows, schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7d473-5abb-4b5c-b3d4-b19015240039",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in the DataFrame:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8841cb17-41ab-499c-93e9-699421e8635f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8fbf83-5cac-4709-8307-91d008195c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Führe die angegebene Abfrage aus, um durchschnittliche Werte zu berechnen\n",
    "average_values = df.agg({'column1': 'avg', 'column2': 'avg', 'column3': 'avg', 'column4': 'avg', 'column5': 'avg', 'column6': 'avg', 'column7': 'avg', 'column8': 'avg'}).collect()[0]\n",
    "print(\"Durchschnittswerte:\")\n",
    "for col_name, avg_value in average_values.asDict().items():\n",
    "    print(f\"{col_name}: {avg_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7ab67-3b12-4a91-9de7-d1403277d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark_context.textFile('hdfs://master-node:9000/user/hadoop/MillionSongSubset_ASCI_analysis_songs/TRAAAAW128F429D538_songs.asci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b0f8f6-d57a-4aae-afdf-c2624c382129",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda line: line.strip('{}').split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327c752-ccef-44fe-ad52-57828e887c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda row: [str(elem) for elem in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22a541-74d1-4986-814d-1a087a4d5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e258a-7dfd-4940-a6ae-8bb091eeea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f78bb-e817-474d-acf5-c0c33bed8740",
   "metadata": {},
   "source": [
    "# 3) H5Py file object file creation from the H5 file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e74778-0a7d-4324-8ec2-209858a66edc",
   "metadata": {},
   "source": [
    "# Debug: Only one h5File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683ba988-e260-47c7-b9b3-d5d5c9b910ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file_paths = ['hdfs://master-node:9000/user/hadoop/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5']\n",
    "h5_file_path = 'hdfs://master-node:9000/user/hadoop/MillionSongSubset/A/A/A/TRAAAAW128F429D538.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37181a-309b-419c-964e-d48d275fa6d1",
   "metadata": {},
   "source": [
    "# Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6da36c-82e1-4874-af26-f0d9bcbb5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(file_path):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        song = file['analysis']['songs']\n",
    "        np_song = np.array(song[0])\n",
    "        selected_features = np_song[['danceability', 'duration', 'tempo', 'energy', 'loudness']].ravel()\n",
    "        return selected_features.tolist()\n",
    "        #print(f\"File found: {file_path}\")\n",
    "         #return file_path\n",
    "    # except FileNotFoundError:\n",
    "    #     print(f\"File not found: {file_path}\")\n",
    "    #     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc9696-8ccd-476e-8fe4-867491f32e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = spark_context.parallelize(h5_file_path)\n",
    "files_infos_rdd = file_paths.map(extract_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29e487-ef1d-46c0-829f-708203dafb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files_infos_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897df25a-b182-41a3-92a1-f61f9492ddbe",
   "metadata": {},
   "source": [
    "# 4) Create Spark Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6348bf-0f3e-455e-af23-1bf886a2e451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere das Schema für den DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"danceability\", FloatType(), True),\n",
    "    StructField(\"duration\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"energy\", FloatType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51be1c-caa7-4443-8ecc-e0dece7f6603",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = sqlContext.createDataFrame(files_infos_rdd.flatMap(lambda x: x), schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da17ea8-b186-406f-bf62-f36affa66f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zeige den DataFrame-Inhalt\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75edaf8-3e1f-4173-8f65-b8852bb3b27e",
   "metadata": {},
   "source": [
    "# Calculation (Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17d4735-196d-40f3-87be-b41dcdabfafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_values = spark_df.agg({'danceability': 'avg', 'duration': 'avg', 'tempo': 'avg', 'energy': 'avg', 'loudness': 'avg'}).collect()[0]\n",
    "print(\"Average Values::\")\n",
    "for col_name, avg_value in average_values.asDict().items():\n",
    "    print(f\"{col_name}: {avg_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ca167-67d5-4b4e-857c-b11709b1698e",
   "metadata": {},
   "source": [
    "# STOP SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23825d7-4cce-458e-82bc-b4dc6995cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39436e22-7c9e-4b0d-ab08-95942846a58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
