{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2ed10a-10d1-4734-bbdf-1d54a99fd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTHON\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pyspark\n",
    "\n",
    "#SPARK\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "\n",
    "import sys\n",
    "\n",
    "# Append the path to Pydoop to sys.path\n",
    "#sys.path.append(\"/usr/local/lib/python3.8/dist-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3096f2-39bc-4025-9c2c-a216255bdf3f",
   "metadata": {},
   "source": [
    "Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb6d6aa-4b8d-4c65-8dc1-a61c5d94018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 03:00:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Spark Session - 2 CORE - imporvement CONFIGURATION!!\n",
    "# spark_session = SparkSession.builder\\\n",
    "#     .master(\"local[2]\")\\\n",
    "#     .appName(\"pseudo_spark_nora\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Set Hadoop configuration directory\n",
    "#os.environ['HADOOP_CONF_DIR'] = '/path/to/hadoop/conf'\n",
    "#spark_session.stop()\n",
    "#os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"HDFS_Connection_Test_nora\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# num executors \n",
    "# executor memory RAM \n",
    "\n",
    "\n",
    "# spark_session = SparkSession.builder \\\n",
    "#     .appName(\"HDFS_Connection_Test_nora\") \\\n",
    "#     .master(\"yarn\") \\\n",
    "#     .config(\"spark.executor.instances\", \"2\") \\\n",
    "#     .config(\"spark.hadoop.fs.default.name\", \"hdfs://master-node:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://master-node:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.dfs.server.namenode.class\", \"org.apache.hadoop.hdfs.server.namenode.NameNode\") \\\n",
    "#     .config(\"spark.hadoop.conf\", \"org.apache.hadoop.hdfs.HdfsConfiguration\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "#.config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "#.config(\"spark.cores.max\", 4)\\\n",
    "\n",
    "\n",
    "#.config(\"spark.jars.packages\", \"LLNL:spark-hdf5:0.0.4\") \\\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"INFO\")\n",
    "\n",
    "sqlContext = SQLContext(spark_session.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44667861-4446-4677-adf9-5acf17142cc4",
   "metadata": {},
   "source": [
    "File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f72586dd-41ed-4710-8c15-6cbbe47debcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAAW128F429D538.asci'\n",
    "#path = 'hdfs://master-node:9000/user/hadoop/MillionSongSubset_ASCI_analysis_songs/'\n",
    "\n",
    "# path_0 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_0/'\n",
    "# path_1 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_1/'\n",
    "# path_2 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_2/'\n",
    "# path_3 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_3/'\n",
    "# path_4 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_4/'\n",
    "# path_5 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_5/'\n",
    "# paths = [path_0, path_1, path_2, path_3, path_4, path_5]\n",
    "\n",
    "path = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a18b7ee5-e775-4b03-b698-f04b07e67300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:08:59 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 344.0 KiB, free 364.6 MiB)\n",
      "24/03/15 03:08:59 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 364.6 MiB)\n",
      "24/03/15 03:08:59 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on master-node:46797 (size: 33.3 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:08:59 INFO SparkContext: Created broadcast 17 from wholeTextFiles at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "file_contents = spark_context.wholeTextFiles(path).map(lambda x: x[1].replace('\\n', '').replace('{', '').replace('}', '').replace(' ', '').split(', '))\n",
    "#file_contents = spark_context.wholeTextFiles(','.join(paths)).map(lambda x: x[1].replace('\\n', '').replace('{', '').replace('}', '').replace(' ', '').split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73413731-3fd0-4616-ae10-27d312e66938",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_contents = file_contents.map(lambda x: x[0].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bf4e196-d127-41c3-b062-be8829f49686",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_elements = split_file_contents.map(lambda x: [float(x[i]) for i in [3, 4, 26, 23, 24, 25, 27, 28, 29]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93b68749-d9c2-4845-9ada-f7e7ade94385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_elements.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b137ba3-8fbe-4981-a12e-60cb40cef006",
   "metadata": {},
   "source": [
    "## Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d58c04d-b0a3-46c0-8786-5d0c5f659237",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"duration\", FloatType(), True),\n",
    "    StructField(\"end_of_fade_in\", FloatType(), True),\n",
    "    StructField(\"start_of_fade_out\", FloatType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "    StructField(\"mode\", FloatType(), True),\n",
    "    StructField(\"mode_confidence\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"time_signature\", FloatType(), True),\n",
    "    StructField(\"time_signature_confidence\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57bb06ef-8815-4fc5-84e7-5b7d530ae7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame(selected_elements, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0aff43dd-0ef5-4755-b28e-aff56a10e23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_16_piece0 on master-node:46797 in memory (size: 11.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_16_piece0 on worker-node-2:41937 in memory (size: 11.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on master-node:46797 in memory (size: 6.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on worker-node-2:41937 in memory (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on worker-node-2:41937 in memory (size: 12.8 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on worker-node-1:45621 in memory (size: 12.8 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on master-node:46797 in memory (size: 12.8 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_14_piece0 on master-node:46797 in memory (size: 11.6 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_14_piece0 on worker-node-2:41937 in memory (size: 11.6 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_15_piece0 on master-node:46797 in memory (size: 19.2 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_15_piece0 on worker-node-2:41937 in memory (size: 19.2 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_15_piece0 on worker-node-1:45621 in memory (size: 19.2 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO FileInputFormat: Total input files to process : 10000\n",
      "24/03/15 03:09:08 INFO FileInputFormat: Total input files to process : 10000\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Registering RDD 58 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Got map stage job 14 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[58] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 03:09:08 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 29.3 KiB, free 364.8 MiB)\n",
      "24/03/15 03:09:08 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 364.8 MiB)\n",
      "24/03/15 03:09:08 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on master-node:46797 (size: 12.8 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:08 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[58] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/15 03:09:08 INFO YarnScheduler: Adding task set 19.0 with 2 tasks resource profile 0\n",
      "24/03/15 03:09:08 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 434765 bytes) \n",
      "24/03/15 03:09:08 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 20) (worker-node-1, executor 2, partition 1, PROCESS_LOCAL, 434765 bytes) \n",
      "24/03/15 03:09:08 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on worker-node-2:41937 (size: 12.8 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:08 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on worker-node-1:45621 (size: 12.8 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:08 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on worker-node-2:41937 (size: 33.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:08 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on worker-node-1:45621 (size: 33.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:10:42 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 94354 ms on worker-node-2 (executor 1) (1/2)\n",
      "[Stage 19:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:12:17 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 20) in 188572 ms on worker-node-1 (executor 2) (2/2)\n",
      "24/03/15 03:12:17 INFO YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:12:17 INFO DAGScheduler: ShuffleMapStage 19 (count at NativeMethodAccessorImpl.java:0) finished in 188.591 s\n",
      "24/03/15 03:12:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/15 03:12:17 INFO DAGScheduler: running: Set()\n",
      "24/03/15 03:12:17 INFO DAGScheduler: waiting: Set()\n",
      "24/03/15 03:12:17 INFO DAGScheduler: failed: Set()\n",
      "24/03/15 03:12:17 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Got job 15 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Final stage: ResultStage 21 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[61] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 03:12:17 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 12.5 KiB, free 364.8 MiB)\n",
      "24/03/15 03:12:17 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 364.8 MiB)\n",
      "24/03/15 03:12:17 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on master-node:46797 (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:12:17 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[61] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 03:12:17 INFO YarnScheduler: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "24/03/15 03:12:17 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21) (worker-node-1, executor 2, partition 0, NODE_LOCAL, 7626 bytes) \n",
      "24/03/15 03:12:17 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on worker-node-1:45621 (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:12:17 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 192.168.2.124:49960\n",
      "24/03/15 03:12:17 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 21) in 56 ms on worker-node-1 (executor 2) (1/1)\n",
      "24/03/15 03:12:17 INFO YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:12:17 INFO DAGScheduler: ResultStage 21 (count at NativeMethodAccessorImpl.java:0) finished in 0.068 s\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 03:12:17 INFO YarnScheduler: Killing all running tasks in stage 21: Stage finished\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Job 15 finished: count at NativeMethodAccessorImpl.java:0, took 0.075696 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_rows = df.count()\n",
    "print(\"Number of rows in the DataFrame:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8dce02ca-1e86-4451-8368-dd59fedc72b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:12:17 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Got job 16 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Final stage: ResultStage 22 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[63] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 03:12:17 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 28.4 KiB, free 364.7 MiB)\n",
      "24/03/15 03:12:17 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 11.6 KiB, free 364.7 MiB)\n",
      "24/03/15 03:12:17 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on master-node:46797 (size: 11.6 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:12:17 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:12:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[63] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 03:12:17 INFO YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "24/03/15 03:12:17 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 434776 bytes) \n",
      "24/03/15 03:12:17 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on worker-node-2:41937 (size: 11.6 KiB, free: 366.1 MiB)\n",
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "|duration|end_of_fade_in|start_of_fade_out|loudness|mode|mode_confidence|  tempo|time_signature|time_signature_confidence|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "| 218.932|         0.247|          218.932| -11.197| 0.0|          0.636| 92.198|           4.0|                    0.778|\n",
      "| 148.035|         0.148|          137.915|  -9.843| 0.0|           0.43|121.274|           4.0|                    0.384|\n",
      "| 177.475|         0.282|          172.304|  -9.689| 1.0|          0.565| 100.07|           1.0|                      0.0|\n",
      "| 233.404|           0.0|          217.124|  -9.013| 1.0|          0.749|119.293|           4.0|                      0.0|\n",
      "| 209.606|         0.066|          198.699|  -4.501| 1.0|          0.371|129.738|           4.0|                    0.562|\n",
      "| 267.702|         2.264|           254.27|  -9.323| 1.0|          0.557|147.782|           3.0|                    0.454|\n",
      "| 114.782|         0.096|          114.782| -17.302| 1.0|            0.0|111.787|           1.0|                      0.0|\n",
      "|  189.57|         0.319|          181.023| -11.642| 0.0|           0.16| 101.43|           3.0|                    0.408|\n",
      "| 269.818|           5.3|           258.99| -13.496| 1.0|          0.652| 86.643|           4.0|                    0.487|\n",
      "| 266.396|         0.084|          261.747|  -6.697| 0.0|          0.473|114.041|           4.0|                    0.878|\n",
      "| 218.775|         2.125|          207.012| -10.021| 0.0|          0.485|146.765|           1.0|                      0.0|\n",
      "| 245.211|         0.357|           227.48|  -7.545| 1.0|          0.686|117.975|           4.0|                    0.835|\n",
      "| 226.351|           0.0|          221.553|  -6.632| 1.0|          0.305| 130.04|           4.0|                      0.0|\n",
      "| 191.843|          0.38|          188.424|   -7.75| 0.0|          0.198|137.334|           1.0|                    0.319|\n",
      "| 307.382|         0.612|          296.658|  -8.346| 1.0|          0.533|125.197|           3.0|                    0.211|\n",
      "| 491.128|           0.0|          486.034|  -8.576| 1.0|          0.829|119.826|           4.0|                    0.756|\n",
      "| 228.597|         0.223|          217.426|  -16.11| 1.0|          0.516|127.756|           5.0|                    0.579|\n",
      "| 599.249|         1.193|          591.999|  -8.032| 1.0|          0.346| 99.273|           4.0|                    0.158|\n",
      "| 290.298|         0.145|          285.605|  -5.271| 1.0|          0.756|150.062|           4.0|                    0.931|\n",
      "| 165.694|         0.162|          157.391|  -6.787| 1.0|          0.568|138.331|           4.0|                    0.127|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:12:18 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 1118 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 03:12:18 INFO YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:12:18 INFO DAGScheduler: ResultStage 22 (showString at NativeMethodAccessorImpl.java:0) finished in 1.133 s\n",
      "24/03/15 03:12:18 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 03:12:18 INFO YarnScheduler: Killing all running tasks in stage 22: Stage finished\n",
      "24/03/15 03:12:18 INFO DAGScheduler: Job 16 finished: showString at NativeMethodAccessorImpl.java:0, took 1.139434 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75edaf8-3e1f-4173-8f65-b8852bb3b27e",
   "metadata": {},
   "source": [
    "# Calculation (Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81a4b687-0b00-4244-8daa-c20c0aeff80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:12:18 INFO DAGScheduler: Registering RDD 65 (collect at /tmp/ipykernel_300399/2441391651.py:1) as input to shuffle 6\n",
      "24/03/15 03:12:18 INFO DAGScheduler: Got map stage job 17 (collect at /tmp/ipykernel_300399/2441391651.py:1) with 2 output partitions\n",
      "24/03/15 03:12:18 INFO DAGScheduler: Final stage: ShuffleMapStage 23 (collect at /tmp/ipykernel_300399/2441391651.py:1)\n",
      "24/03/15 03:12:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 03:12:18 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:12:18 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[65] at collect at /tmp/ipykernel_300399/2441391651.py:1), which has no missing parents\n",
      "24/03/15 03:12:18 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 50.9 KiB, free 364.7 MiB)\n",
      "24/03/15 03:12:18 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 364.6 MiB)\n",
      "24/03/15 03:12:18 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on master-node:46797 (size: 19.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:12:18 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:12:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[65] at collect at /tmp/ipykernel_300399/2441391651.py:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/15 03:12:18 INFO YarnScheduler: Adding task set 23.0 with 2 tasks resource profile 0\n",
      "24/03/15 03:12:18 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 434765 bytes) \n",
      "24/03/15 03:12:18 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 24) (worker-node-1, executor 2, partition 1, PROCESS_LOCAL, 434765 bytes) \n",
      "24/03/15 03:12:18 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on worker-node-1:45621 (size: 19.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:12:18 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on worker-node-2:41937 (size: 19.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:12:23 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 24) in 5307 ms on worker-node-1 (executor 2) (1/2)\n",
      "24/03/15 03:12:24 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 5806 ms on worker-node-2 (executor 1) (2/2)\n",
      "24/03/15 03:12:24 INFO YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:12:24 INFO DAGScheduler: ShuffleMapStage 23 (collect at /tmp/ipykernel_300399/2441391651.py:1) finished in 5.823 s\n",
      "24/03/15 03:12:24 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/15 03:12:24 INFO DAGScheduler: running: Set()\n",
      "24/03/15 03:12:24 INFO DAGScheduler: waiting: Set()\n",
      "24/03/15 03:12:24 INFO DAGScheduler: failed: Set()\n",
      "24/03/15 03:12:24 INFO SparkContext: Starting job: collect at /tmp/ipykernel_300399/2441391651.py:1\n",
      "24/03/15 03:12:24 INFO DAGScheduler: Got job 18 (collect at /tmp/ipykernel_300399/2441391651.py:1) with 1 output partitions\n",
      "24/03/15 03:12:24 INFO DAGScheduler: Final stage: ResultStage 25 (collect at /tmp/ipykernel_300399/2441391651.py:1)\n",
      "24/03/15 03:12:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 24)\n",
      "24/03/15 03:12:24 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:12:24 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[68] at collect at /tmp/ipykernel_300399/2441391651.py:1), which has no missing parents\n",
      "24/03/15 03:12:24 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 32.7 KiB, free 364.6 MiB)\n",
      "24/03/15 03:12:24 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 364.6 MiB)\n",
      "24/03/15 03:12:24 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on master-node:46797 (size: 11.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:12:24 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:12:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[68] at collect at /tmp/ipykernel_300399/2441391651.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 03:12:24 INFO YarnScheduler: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "24/03/15 03:12:24 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7626 bytes) \n",
      "24/03/15 03:12:24 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on worker-node-2:41937 (size: 11.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:12:24 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 192.168.2.40:56940\n",
      "24/03/15 03:12:24 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 35 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 03:12:24 INFO YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:12:24 INFO DAGScheduler: ResultStage 25 (collect at /tmp/ipykernel_300399/2441391651.py:1) finished in 0.046 s\n",
      "24/03/15 03:12:24 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 03:12:24 INFO YarnScheduler: Killing all running tasks in stage 25: Stage finished\n",
      "24/03/15 03:12:24 INFO DAGScheduler: Job 18 finished: collect at /tmp/ipykernel_300399/2441391651.py:1, took 0.053157 s\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "average_values = df.agg({'duration': 'avg', 'end_of_fade_in': 'avg', 'start_of_fade_out': 'avg', \n",
    "                         'loudness': 'avg', 'mode': 'avg', 'mode_confidence': 'avg', \n",
    "                         'tempo': 'avg', 'time_signature': 'avg', \n",
    "                         'time_signature_confidence': 'avg'}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09134280-7b4c-4130-a0c4-deb68bbf3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = pd.DataFrame([average_values.asDict()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e409464d-8f5b-4757-a28f-fe16b1eaef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in the DataFrame:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0bb5aa6b-0e6b-4025-930d-613a9f1ac207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg(duration)</th>\n",
       "      <th>avg(tempo)</th>\n",
       "      <th>avg(end_of_fade_in)</th>\n",
       "      <th>avg(start_of_fade_out)</th>\n",
       "      <th>avg(time_signature)</th>\n",
       "      <th>avg(time_signature_confidence)</th>\n",
       "      <th>avg(mode_confidence)</th>\n",
       "      <th>avg(mode)</th>\n",
       "      <th>avg(loudness)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>238.507519</td>\n",
       "      <td>122.915449</td>\n",
       "      <td>0.758616</td>\n",
       "      <td>229.975465</td>\n",
       "      <td>3.5648</td>\n",
       "      <td>0.509937</td>\n",
       "      <td>0.477784</td>\n",
       "      <td>0.6911</td>\n",
       "      <td>-10.485669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg(duration)  avg(tempo)  avg(end_of_fade_in)  avg(start_of_fade_out)  \\\n",
       "0     238.507519  122.915449             0.758616              229.975465   \n",
       "\n",
       "   avg(time_signature)  avg(time_signature_confidence)  avg(mode_confidence)  \\\n",
       "0               3.5648                        0.509937              0.477784   \n",
       "\n",
       "   avg(mode)  avg(loudness)  \n",
       "0     0.6911     -10.485669  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ca167-67d5-4b4e-857c-b11709b1698e",
   "metadata": {},
   "source": [
    "# STOP SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23825d7-4cce-458e-82bc-b4dc6995cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores\n",
    "spark_context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
