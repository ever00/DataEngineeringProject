{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2ed10a-10d1-4734-bbdf-1d54a99fd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTHON\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pyspark\n",
    "\n",
    "#SPARK\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "\n",
    "import sys\n",
    "\n",
    "# Append the path to Pydoop to sys.path\n",
    "#sys.path.append(\"/usr/local/lib/python3.8/dist-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3096f2-39bc-4025-9c2c-a216255bdf3f",
   "metadata": {},
   "source": [
    "Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb6d6aa-4b8d-4c65-8dc1-a61c5d94018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 03:00:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Spark Session - 2 CORE - imporvement CONFIGURATION!!\n",
    "# spark_session = SparkSession.builder\\\n",
    "#     .master(\"local[2]\")\\\n",
    "#     .appName(\"pseudo_spark_nora\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Set Hadoop configuration directory\n",
    "#os.environ['HADOOP_CONF_DIR'] = '/path/to/hadoop/conf'\n",
    "#spark_session.stop()\n",
    "#os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"HDFS_Connection_Test_nora\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# num executors \n",
    "# executor memory RAM \n",
    "\n",
    "\n",
    "# spark_session = SparkSession.builder \\\n",
    "#     .appName(\"HDFS_Connection_Test_nora\") \\\n",
    "#     .master(\"yarn\") \\\n",
    "#     .config(\"spark.executor.instances\", \"2\") \\\n",
    "#     .config(\"spark.hadoop.fs.default.name\", \"hdfs://master-node:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://master-node:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.dfs.server.namenode.class\", \"org.apache.hadoop.hdfs.server.namenode.NameNode\") \\\n",
    "#     .config(\"spark.hadoop.conf\", \"org.apache.hadoop.hdfs.HdfsConfiguration\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "#.config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "#.config(\"spark.cores.max\", 4)\\\n",
    "\n",
    "\n",
    "#.config(\"spark.jars.packages\", \"LLNL:spark-hdf5:0.0.4\") \\\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"INFO\")\n",
    "\n",
    "sqlContext = SQLContext(spark_session.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44667861-4446-4677-adf9-5acf17142cc4",
   "metadata": {},
   "source": [
    "File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f72586dd-41ed-4710-8c15-6cbbe47debcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAAW128F429D538.asci'\n",
    "#path = 'hdfs://master-node:9000/user/hadoop/MillionSongSubset_ASCI_analysis_songs/'\n",
    "\n",
    "# path_0 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_0/'\n",
    "# path_1 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_1/'\n",
    "# path_2 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_2/'\n",
    "# path_3 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_3/'\n",
    "# path_4 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_4/'\n",
    "# path_5 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_5/'\n",
    "# paths = [path_0, path_1, path_2, path_3, path_4, path_5]\n",
    "\n",
    "path = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a18b7ee5-e775-4b03-b698-f04b07e67300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:08:59 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 344.0 KiB, free 364.6 MiB)\n",
      "24/03/15 03:08:59 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 33.3 KiB, free 364.6 MiB)\n",
      "24/03/15 03:08:59 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on master-node:46797 (size: 33.3 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:08:59 INFO SparkContext: Created broadcast 17 from wholeTextFiles at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "file_contents = spark_context.wholeTextFiles(path).map(lambda x: x[1].replace('\\n', '').replace('{', '').replace('}', '').replace(' ', '').split(', '))\n",
    "#file_contents = spark_context.wholeTextFiles(','.join(paths)).map(lambda x: x[1].replace('\\n', '').replace('{', '').replace('}', '').replace(' ', '').split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73413731-3fd0-4616-ae10-27d312e66938",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_contents = file_contents.map(lambda x: x[0].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bf4e196-d127-41c3-b062-be8829f49686",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_elements = split_file_contents.map(lambda x: [float(x[i]) for i in [3, 4, 26, 23, 24, 25, 27, 28, 29]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93b68749-d9c2-4845-9ada-f7e7ade94385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_elements.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b137ba3-8fbe-4981-a12e-60cb40cef006",
   "metadata": {},
   "source": [
    "## Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1d58c04d-b0a3-46c0-8786-5d0c5f659237",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"duration\", FloatType(), True),\n",
    "    StructField(\"end_of_fade_in\", FloatType(), True),\n",
    "    StructField(\"start_of_fade_out\", FloatType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "    StructField(\"mode\", FloatType(), True),\n",
    "    StructField(\"mode_confidence\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"time_signature\", FloatType(), True),\n",
    "    StructField(\"time_signature_confidence\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57bb06ef-8815-4fc5-84e7-5b7d530ae7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark_session.createDataFrame(selected_elements, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aff43dd-0ef5-4755-b28e-aff56a10e23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_16_piece0 on master-node:46797 in memory (size: 11.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_16_piece0 on worker-node-2:41937 in memory (size: 11.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on master-node:46797 in memory (size: 6.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on worker-node-2:41937 in memory (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on worker-node-2:41937 in memory (size: 12.8 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on worker-node-1:45621 in memory (size: 12.8 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on master-node:46797 in memory (size: 12.8 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_14_piece0 on master-node:46797 in memory (size: 11.6 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_14_piece0 on worker-node-2:41937 in memory (size: 11.6 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_15_piece0 on master-node:46797 in memory (size: 19.2 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_15_piece0 on worker-node-2:41937 in memory (size: 19.2 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO BlockManagerInfo: Removed broadcast_15_piece0 on worker-node-1:45621 in memory (size: 19.2 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:07 INFO FileInputFormat: Total input files to process : 10000\n",
      "24/03/15 03:09:08 INFO FileInputFormat: Total input files to process : 10000\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Registering RDD 58 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 5\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Got map stage job 14 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[58] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 03:09:08 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 29.3 KiB, free 364.8 MiB)\n",
      "24/03/15 03:09:08 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 364.8 MiB)\n",
      "24/03/15 03:09:08 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on master-node:46797 (size: 12.8 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:08 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:09:08 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[58] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/15 03:09:08 INFO YarnScheduler: Adding task set 19.0 with 2 tasks resource profile 0\n",
      "24/03/15 03:09:08 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 434765 bytes) \n",
      "24/03/15 03:09:08 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 20) (worker-node-1, executor 2, partition 1, PROCESS_LOCAL, 434765 bytes) \n",
      "24/03/15 03:09:08 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on worker-node-2:41937 (size: 12.8 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:08 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on worker-node-1:45621 (size: 12.8 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:08 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on worker-node-2:41937 (size: 33.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:09:08 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on worker-node-1:45621 (size: 33.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:10:42 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 94354 ms on worker-node-2 (executor 1) (1/2)\n",
      "[Stage 19:=============================>                            (1 + 1) / 2]\r"
     ]
    }
   ],
   "source": [
    "num_rows = df.count()\n",
    "print(\"Number of rows in the DataFrame:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dce02ca-1e86-4451-8368-dd59fedc72b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75edaf8-3e1f-4173-8f65-b8852bb3b27e",
   "metadata": {},
   "source": [
    "# Calculation (Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4b687-0b00-4244-8daa-c20c0aeff80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_values = df.agg({'duration': 'avg', 'end_of_fade_in': 'avg', 'start_of_fade_out': 'avg', \n",
    "                         'loudness': 'avg', 'mode': 'avg', 'mode_confidence': 'avg', \n",
    "                         'tempo': 'avg', 'time_signature': 'avg', \n",
    "                         'time_signature_confidence': 'avg'}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09134280-7b4c-4130-a0c4-deb68bbf3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = pd.DataFrame([average_values.asDict()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409464d-8f5b-4757-a28f-fe16b1eaef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in the DataFrame:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5aa6b-0e6b-4025-930d-613a9f1ac207",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ca167-67d5-4b4e-857c-b11709b1698e",
   "metadata": {},
   "source": [
    "# STOP SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23825d7-4cce-458e-82bc-b4dc6995cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# release the cores\n",
    "spark_context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
