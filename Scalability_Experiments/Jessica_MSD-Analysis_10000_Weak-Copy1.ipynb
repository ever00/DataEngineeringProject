{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2ed10a-10d1-4734-bbdf-1d54a99fd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTHON\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pyspark\n",
    "\n",
    "#SPARK\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "\n",
    "import sys\n",
    "\n",
    "# Append the path to Pydoop to sys.path\n",
    "#sys.path.append(\"/usr/local/lib/python3.8/dist-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3096f2-39bc-4025-9c2c-a216255bdf3f",
   "metadata": {},
   "source": [
    "Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb6d6aa-4b8d-4c65-8dc1-a61c5d94018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(num_workers, cpu):\n",
    "    # Spark_session with different number of workers, cpu and file_size\n",
    "    spark_session = SparkSession.builder \\\n",
    "        .appName(\"Iteration:3_15_000_Files_workers:\"+ str(num_workers) + \"_cpu:\" + str(cpu)) \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.executor.instances\", num_workers) \\\n",
    "        .config(\"spark.executor.cores\", cpu) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"WARN\")\n",
    "    sqlContext = SQLContext(spark_context)\n",
    "    \n",
    "    return spark_session, spark_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4dd9501-d031-4076-9eb7-8189eaf538f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_computations(spark_session, spark_context):\n",
    "    path_0 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_0/'\n",
    "    path_1 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_1/'\n",
    "    path_2 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_2/'\n",
    "    path_3 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_3/'\n",
    "    path_4 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_4/'\n",
    "    #path_5 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_5/'\n",
    "    path = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI/'\n",
    "    paths = [path_0, path_1, path_2, path_3, path_4, path]\n",
    "    \n",
    "    # path\n",
    "    #file_contents = spark_context.wholeTextFiles(path).map(lambda x: x[1].replace('\\n', '').replace('{', '').replace('}', '').replace(' ', '').split(', '))\n",
    "    # paths\n",
    "    file_contents = spark_context.wholeTextFiles(','.join(paths)).map(lambda x: x[1].replace('\\n', '').replace('{', '').replace('}', '').replace(' ', '').split(', '))\n",
    "\n",
    "    split_file_contents = file_contents.map(lambda x: x[0].split(','))\n",
    "    selected_elements = split_file_contents.map(lambda x: [float(x[i]) for i in [3, 4, 26, 23, 24, 25, 27, 28, 29]])\n",
    "    \n",
    "    \n",
    "    schema = StructType([\n",
    "    StructField(\"duration\", FloatType(), True),\n",
    "    StructField(\"end_of_fade_in\", FloatType(), True),\n",
    "    StructField(\"start_of_fade_out\", FloatType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "    StructField(\"mode\", FloatType(), True),\n",
    "    StructField(\"mode_confidence\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"time_signature\", FloatType(), True),\n",
    "    StructField(\"time_signature_confidence\", FloatType(), True)\n",
    "    ])\n",
    "\n",
    "    df = spark_session.createDataFrame(selected_elements, schema).cache()\n",
    "\n",
    "    num_rows = df.count()\n",
    "    print(\"Number of rows in the DataFrame:\", num_rows)\n",
    "\n",
    "    df.show()\n",
    "\n",
    "    average_values = df.agg({'duration': 'avg', 'end_of_fade_in': 'avg', 'start_of_fade_out': 'avg', \n",
    "                         'loudness': 'avg', 'mode': 'avg', 'mode_confidence': 'avg', \n",
    "                         'tempo': 'avg', 'time_signature': 'avg', \n",
    "                         'time_signature_confidence': 'avg'}).collect()[0]\n",
    "    \n",
    "    avg_df = pd.DataFrame([average_values.asDict()])\n",
    "\n",
    "    print(\"Number of rows in the DataFrame:\", num_rows)\n",
    "\n",
    "    avg_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ee21d5-769c-42e4-b3a7-eff968b22bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 16:28:47 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "24/03/15 16:30:13 ERROR TransportClient: Failed to send RPC RPC 4995510330001399207 to /192.168.2.40:54884: io.netty.channel.StacklessClosedChannelException\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "24/03/15 16:30:13 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 1 at RPC address 192.168.2.124:57706, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Failed to send RPC RPC 4995510330001399207 to /192.168.2.40:54884: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:590)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:557)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:492)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:636)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:629)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:118)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:999)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:860)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:877)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:940)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:569)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "24/03/15 16:30:13 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_8_1 !\n",
      "24/03/15 16:30:13 ERROR YarnScheduler: Lost executor 1 on worker-node-1: Executor Process Lost\n",
      "24/03/15 16:30:13 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 1) (worker-node-1 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Executor Process Lost\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 15015\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "|duration|end_of_fade_in|start_of_fade_out|loudness|mode|mode_confidence|  tempo|time_signature|time_signature_confidence|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "| 218.932|         0.247|          218.932| -11.197| 0.0|          0.636| 92.198|           4.0|                    0.778|\n",
      "| 148.035|         0.148|          137.915|  -9.843| 0.0|           0.43|121.274|           4.0|                    0.384|\n",
      "| 177.475|         0.282|          172.304|  -9.689| 1.0|          0.565| 100.07|           1.0|                      0.0|\n",
      "| 233.404|           0.0|          217.124|  -9.013| 1.0|          0.749|119.293|           4.0|                      0.0|\n",
      "| 209.606|         0.066|          198.699|  -4.501| 1.0|          0.371|129.738|           4.0|                    0.562|\n",
      "| 267.702|         2.264|           254.27|  -9.323| 1.0|          0.557|147.782|           3.0|                    0.454|\n",
      "| 114.782|         0.096|          114.782| -17.302| 1.0|            0.0|111.787|           1.0|                      0.0|\n",
      "|  189.57|         0.319|          181.023| -11.642| 0.0|           0.16| 101.43|           3.0|                    0.408|\n",
      "| 269.818|           5.3|           258.99| -13.496| 1.0|          0.652| 86.643|           4.0|                    0.487|\n",
      "| 266.396|         0.084|          261.747|  -6.697| 0.0|          0.473|114.041|           4.0|                    0.878|\n",
      "| 218.775|         2.125|          207.012| -10.021| 0.0|          0.485|146.765|           1.0|                      0.0|\n",
      "| 245.211|         0.357|           227.48|  -7.545| 1.0|          0.686|117.975|           4.0|                    0.835|\n",
      "| 226.351|           0.0|          221.553|  -6.632| 1.0|          0.305| 130.04|           4.0|                      0.0|\n",
      "| 191.843|          0.38|          188.424|   -7.75| 0.0|          0.198|137.334|           1.0|                    0.319|\n",
      "| 307.382|         0.612|          296.658|  -8.346| 1.0|          0.533|125.197|           3.0|                    0.211|\n",
      "| 491.128|           0.0|          486.034|  -8.576| 1.0|          0.829|119.826|           4.0|                    0.756|\n",
      "| 228.597|         0.223|          217.426|  -16.11| 1.0|          0.516|127.756|           5.0|                    0.579|\n",
      "| 599.249|         1.193|          591.999|  -8.032| 1.0|          0.346| 99.273|           4.0|                    0.158|\n",
      "| 290.298|         0.145|          285.605|  -5.271| 1.0|          0.756|150.062|           4.0|                    0.931|\n",
      "| 165.694|         0.162|          157.391|  -6.787| 1.0|          0.568|138.331|           4.0|                    0.127|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows in the DataFrame: 15015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 16:33:31 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 15015\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "|duration|end_of_fade_in|start_of_fade_out|loudness|mode|mode_confidence|  tempo|time_signature|time_signature_confidence|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "| 218.932|         0.247|          218.932| -11.197| 0.0|          0.636| 92.198|           4.0|                    0.778|\n",
      "| 148.035|         0.148|          137.915|  -9.843| 0.0|           0.43|121.274|           4.0|                    0.384|\n",
      "| 177.475|         0.282|          172.304|  -9.689| 1.0|          0.565| 100.07|           1.0|                      0.0|\n",
      "| 233.404|           0.0|          217.124|  -9.013| 1.0|          0.749|119.293|           4.0|                      0.0|\n",
      "| 209.606|         0.066|          198.699|  -4.501| 1.0|          0.371|129.738|           4.0|                    0.562|\n",
      "| 267.702|         2.264|           254.27|  -9.323| 1.0|          0.557|147.782|           3.0|                    0.454|\n",
      "| 114.782|         0.096|          114.782| -17.302| 1.0|            0.0|111.787|           1.0|                      0.0|\n",
      "|  189.57|         0.319|          181.023| -11.642| 0.0|           0.16| 101.43|           3.0|                    0.408|\n",
      "| 269.818|           5.3|           258.99| -13.496| 1.0|          0.652| 86.643|           4.0|                    0.487|\n",
      "| 266.396|         0.084|          261.747|  -6.697| 0.0|          0.473|114.041|           4.0|                    0.878|\n",
      "| 218.775|         2.125|          207.012| -10.021| 0.0|          0.485|146.765|           1.0|                      0.0|\n",
      "| 245.211|         0.357|           227.48|  -7.545| 1.0|          0.686|117.975|           4.0|                    0.835|\n",
      "| 226.351|           0.0|          221.553|  -6.632| 1.0|          0.305| 130.04|           4.0|                      0.0|\n",
      "| 191.843|          0.38|          188.424|   -7.75| 0.0|          0.198|137.334|           1.0|                    0.319|\n",
      "| 307.382|         0.612|          296.658|  -8.346| 1.0|          0.533|125.197|           3.0|                    0.211|\n",
      "| 491.128|           0.0|          486.034|  -8.576| 1.0|          0.829|119.826|           4.0|                    0.756|\n",
      "| 228.597|         0.223|          217.426|  -16.11| 1.0|          0.516|127.756|           5.0|                    0.579|\n",
      "| 599.249|         1.193|          591.999|  -8.032| 1.0|          0.346| 99.273|           4.0|                    0.158|\n",
      "| 290.298|         0.145|          285.605|  -5.271| 1.0|          0.756|150.062|           4.0|                    0.931|\n",
      "| 165.694|         0.162|          157.391|  -6.787| 1.0|          0.568|138.331|           4.0|                    0.127|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows in the DataFrame: 15015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 16:34:43 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 15015\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "|duration|end_of_fade_in|start_of_fade_out|loudness|mode|mode_confidence|  tempo|time_signature|time_signature_confidence|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "| 218.932|         0.247|          218.932| -11.197| 0.0|          0.636| 92.198|           4.0|                    0.778|\n",
      "| 148.035|         0.148|          137.915|  -9.843| 0.0|           0.43|121.274|           4.0|                    0.384|\n",
      "| 177.475|         0.282|          172.304|  -9.689| 1.0|          0.565| 100.07|           1.0|                      0.0|\n",
      "| 233.404|           0.0|          217.124|  -9.013| 1.0|          0.749|119.293|           4.0|                      0.0|\n",
      "| 209.606|         0.066|          198.699|  -4.501| 1.0|          0.371|129.738|           4.0|                    0.562|\n",
      "| 267.702|         2.264|           254.27|  -9.323| 1.0|          0.557|147.782|           3.0|                    0.454|\n",
      "| 114.782|         0.096|          114.782| -17.302| 1.0|            0.0|111.787|           1.0|                      0.0|\n",
      "|  189.57|         0.319|          181.023| -11.642| 0.0|           0.16| 101.43|           3.0|                    0.408|\n",
      "| 269.818|           5.3|           258.99| -13.496| 1.0|          0.652| 86.643|           4.0|                    0.487|\n",
      "| 266.396|         0.084|          261.747|  -6.697| 0.0|          0.473|114.041|           4.0|                    0.878|\n",
      "| 218.775|         2.125|          207.012| -10.021| 0.0|          0.485|146.765|           1.0|                      0.0|\n",
      "| 245.211|         0.357|           227.48|  -7.545| 1.0|          0.686|117.975|           4.0|                    0.835|\n",
      "| 226.351|           0.0|          221.553|  -6.632| 1.0|          0.305| 130.04|           4.0|                      0.0|\n",
      "| 191.843|          0.38|          188.424|   -7.75| 0.0|          0.198|137.334|           1.0|                    0.319|\n",
      "| 307.382|         0.612|          296.658|  -8.346| 1.0|          0.533|125.197|           3.0|                    0.211|\n",
      "| 491.128|           0.0|          486.034|  -8.576| 1.0|          0.829|119.826|           4.0|                    0.756|\n",
      "| 228.597|         0.223|          217.426|  -16.11| 1.0|          0.516|127.756|           5.0|                    0.579|\n",
      "| 599.249|         1.193|          591.999|  -8.032| 1.0|          0.346| 99.273|           4.0|                    0.158|\n",
      "| 290.298|         0.145|          285.605|  -5.271| 1.0|          0.756|150.062|           4.0|                    0.931|\n",
      "| 165.694|         0.162|          157.391|  -6.787| 1.0|          0.568|138.331|           4.0|                    0.127|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows in the DataFrame: 15015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 16:35:25 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 15015\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "|duration|end_of_fade_in|start_of_fade_out|loudness|mode|mode_confidence|  tempo|time_signature|time_signature_confidence|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "| 218.932|         0.247|          218.932| -11.197| 0.0|          0.636| 92.198|           4.0|                    0.778|\n",
      "| 148.035|         0.148|          137.915|  -9.843| 0.0|           0.43|121.274|           4.0|                    0.384|\n",
      "| 177.475|         0.282|          172.304|  -9.689| 1.0|          0.565| 100.07|           1.0|                      0.0|\n",
      "| 233.404|           0.0|          217.124|  -9.013| 1.0|          0.749|119.293|           4.0|                      0.0|\n",
      "| 209.606|         0.066|          198.699|  -4.501| 1.0|          0.371|129.738|           4.0|                    0.562|\n",
      "| 267.702|         2.264|           254.27|  -9.323| 1.0|          0.557|147.782|           3.0|                    0.454|\n",
      "| 114.782|         0.096|          114.782| -17.302| 1.0|            0.0|111.787|           1.0|                      0.0|\n",
      "|  189.57|         0.319|          181.023| -11.642| 0.0|           0.16| 101.43|           3.0|                    0.408|\n",
      "| 269.818|           5.3|           258.99| -13.496| 1.0|          0.652| 86.643|           4.0|                    0.487|\n",
      "| 266.396|         0.084|          261.747|  -6.697| 0.0|          0.473|114.041|           4.0|                    0.878|\n",
      "| 218.775|         2.125|          207.012| -10.021| 0.0|          0.485|146.765|           1.0|                      0.0|\n",
      "| 245.211|         0.357|           227.48|  -7.545| 1.0|          0.686|117.975|           4.0|                    0.835|\n",
      "| 226.351|           0.0|          221.553|  -6.632| 1.0|          0.305| 130.04|           4.0|                      0.0|\n",
      "| 191.843|          0.38|          188.424|   -7.75| 0.0|          0.198|137.334|           1.0|                    0.319|\n",
      "| 307.382|         0.612|          296.658|  -8.346| 1.0|          0.533|125.197|           3.0|                    0.211|\n",
      "| 491.128|           0.0|          486.034|  -8.576| 1.0|          0.829|119.826|           4.0|                    0.756|\n",
      "| 228.597|         0.223|          217.426|  -16.11| 1.0|          0.516|127.756|           5.0|                    0.579|\n",
      "| 599.249|         1.193|          591.999|  -8.032| 1.0|          0.346| 99.273|           4.0|                    0.158|\n",
      "| 290.298|         0.145|          285.605|  -5.271| 1.0|          0.756|150.062|           4.0|                    0.931|\n",
      "| 165.694|         0.162|          157.391|  -6.787| 1.0|          0.568|138.331|           4.0|                    0.127|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of rows in the DataFrame: 15015\n"
     ]
    }
   ],
   "source": [
    "# Measure time taken for computations with different number of workers\n",
    "for num_workers in [1, 2]:\n",
    "    for cpu in [1, 2]:\n",
    "        spark_session, spark_context =  create_spark_session(num_workers, cpu)\n",
    "        run_computations(spark_session, spark_context)\n",
    "        \n",
    "        spark_session.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ca167-67d5-4b4e-857c-b11709b1698e",
   "metadata": {},
   "source": [
    "# STOP SPARK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
