{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2ed10a-10d1-4734-bbdf-1d54a99fd33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTHON\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pyspark\n",
    "\n",
    "#SPARK\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "\n",
    "import sys\n",
    "\n",
    "# Append the path to Pydoop to sys.path\n",
    "#sys.path.append(\"/usr/local/lib/python3.8/dist-packages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3096f2-39bc-4025-9c2c-a216255bdf3f",
   "metadata": {},
   "source": [
    "Spark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb6d6aa-4b8d-4c65-8dc1-a61c5d94018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/15 03:21:00 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "/home/hadoop/.local/lib/python3.8/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Spark Session - 2 CORE - imporvement CONFIGURATION!!\n",
    "# spark_session = SparkSession.builder\\\n",
    "#     .master(\"local[2]\")\\\n",
    "#     .appName(\"pseudo_spark_nora\")\\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# Set Hadoop configuration directory\n",
    "#os.environ['HADOOP_CONF_DIR'] = '/path/to/hadoop/conf'\n",
    "#spark_session.stop()\n",
    "#os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "\n",
    "spark_session = SparkSession.builder \\\n",
    "    .appName(\"Analyse_1000Files\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# num executors \n",
    "# executor memory RAM \n",
    "\n",
    "\n",
    "# spark_session = SparkSession.builder \\\n",
    "#     .appName(\"HDFS_Connection_Test_nora\") \\\n",
    "#     .master(\"yarn\") \\\n",
    "#     .config(\"spark.executor.instances\", \"2\") \\\n",
    "#     .config(\"spark.hadoop.fs.default.name\", \"hdfs://master-node:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://master-node:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\") \\\n",
    "#     .config(\"spark.hadoop.dfs.server.namenode.class\", \"org.apache.hadoop.hdfs.server.namenode.NameNode\") \\\n",
    "#     .config(\"spark.hadoop.conf\", \"org.apache.hadoop.hdfs.HdfsConfiguration\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "#.config(\"spark.dynamicAllocation.enabled\", False)\\\n",
    "#.config(\"spark.cores.max\", 4)\\\n",
    "\n",
    "\n",
    "#.config(\"spark.jars.packages\", \"LLNL:spark-hdf5:0.0.4\") \\\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"INFO\")\n",
    "\n",
    "sqlContext = SQLContext(spark_session.sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44667861-4446-4677-adf9-5acf17142cc4",
   "metadata": {},
   "source": [
    "File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72586dd-41ed-4710-8c15-6cbbe47debcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI/TRAAAAW128F429D538.asci'\n",
    "#path = 'hdfs://master-node:9000/user/hadoop/MillionSongSubset_ASCI_analysis_songs/'\n",
    "\n",
    "# path_0 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_0/'\n",
    "# path_1 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_1/'\n",
    "# path_2 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_2/'\n",
    "# path_3 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_3/'\n",
    "# path_4 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_4/'\n",
    "# path_5 = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI_SUB/subfolder_5/'\n",
    "# paths = [path_0, path_1, path_2, path_3, path_4, path_5]\n",
    "\n",
    "path = 'hdfs://master-node:9000/user/hadoop/MSD_ASCI/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a18b7ee5-e775-4b03-b698-f04b07e67300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:21:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 343.6 KiB, free 366.0 MiB)\n",
      "24/03/15 03:21:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 33.1 KiB, free 365.9 MiB)\n",
      "24/03/15 03:21:18 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on master-node:36759 (size: 33.1 KiB, free: 366.3 MiB)\n",
      "24/03/15 03:21:18 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0\n"
     ]
    }
   ],
   "source": [
    "file_contents = spark_context.wholeTextFiles(path).map(lambda x: x[1].replace('\\n', '').replace('{', '').replace('}', '').replace(' ', '').split(', '))\n",
    "#file_contents = spark_context.wholeTextFiles(','.join(paths)).map(lambda x: x[1].replace('\\n', '').replace('{', '').replace('}', '').replace(' ', '').split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73413731-3fd0-4616-ae10-27d312e66938",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file_contents = file_contents.map(lambda x: x[0].split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf4e196-d127-41c3-b062-be8829f49686",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_elements = split_file_contents.map(lambda x: [float(x[i]) for i in [3, 4, 26, 23, 24, 25, 27, 28, 29]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93b68749-d9c2-4845-9ada-f7e7ade94385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_elements.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b137ba3-8fbe-4981-a12e-60cb40cef006",
   "metadata": {},
   "source": [
    "## Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d58c04d-b0a3-46c0-8786-5d0c5f659237",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"duration\", FloatType(), True),\n",
    "    StructField(\"end_of_fade_in\", FloatType(), True),\n",
    "    StructField(\"start_of_fade_out\", FloatType(), True),\n",
    "    StructField(\"loudness\", FloatType(), True),\n",
    "    StructField(\"mode\", FloatType(), True),\n",
    "    StructField(\"mode_confidence\", FloatType(), True),\n",
    "    StructField(\"tempo\", FloatType(), True),\n",
    "    StructField(\"time_signature\", FloatType(), True),\n",
    "    StructField(\"time_signature_confidence\", FloatType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57bb06ef-8815-4fc5-84e7-5b7d530ae7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:21:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/03/15 03:21:19 INFO SharedState: Warehouse path is 'file:/home/hadoop/MSD_Project/DataEngineeringProject/Scalability_Experiments/spark-warehouse'.\n",
      "24/03/15 03:21:19 INFO ServerInfo: Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/03/15 03:21:19 INFO ServerInfo: Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/03/15 03:21:19 INFO ServerInfo: Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/03/15 03:21:19 INFO ServerInfo: Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "24/03/15 03:21:19 INFO ServerInfo: Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n"
     ]
    }
   ],
   "source": [
    "df = spark_session.createDataFrame(selected_elements, schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aff43dd-0ef5-4755-b28e-aff56a10e23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:21:22 INFO CodeGenerator: Code generated in 346.844432 ms\n",
      "24/03/15 03:21:23 INFO FileInputFormat: Total input files to process : 10000\n",
      "24/03/15 03:21:24 INFO FileInputFormat: Total input files to process : 10000\n",
      "24/03/15 03:21:24 INFO DAGScheduler: Got job 0 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/03/15 03:21:24 INFO DAGScheduler: Final stage: ResultStage 0 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 03:21:24 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 03:21:24 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:21:24 INFO DAGScheduler: Submitting ResultStage 0 (*(1) Scan ExistingRDD[duration#0,end_of_fade_in#1,start_of_fade_out#2,loudness#3,mode#4,mode_confidence#5,tempo#6,time_signature#7,time_signature_confidence#8]\n",
      " MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 03:21:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 30.4 KiB, free 365.9 MiB)\n",
      "24/03/15 03:21:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 365.9 MiB)\n",
      "24/03/15 03:21:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on master-node:36759 (size: 12.7 KiB, free: 366.3 MiB)\n",
      "24/03/15 03:21:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:21:24 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (*(1) Scan ExistingRDD[duration#0,end_of_fade_in#1,start_of_fade_out#2,loudness#3,mode#4,mode_confidence#5,tempo#6,time_signature#7,time_signature_confidence#8]\n",
      " MapPartitionsRDD[8] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/15 03:21:24 INFO YarnScheduler: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "24/03/15 03:21:24 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 434776 bytes) \n",
      "24/03/15 03:21:24 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (worker-node-1, executor 2, partition 1, PROCESS_LOCAL, 434776 bytes) \n",
      "24/03/15 03:21:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on worker-node-2:41755 (size: 12.7 KiB, free: 366.3 MiB)\n",
      "24/03/15 03:21:25 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on worker-node-1:34595 (size: 12.7 KiB, free: 366.3 MiB)\n",
      "24/03/15 03:21:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on worker-node-2:41755 (size: 33.1 KiB, free: 366.3 MiB)\n",
      "24/03/15 03:21:26 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on worker-node-1:34595 (size: 33.1 KiB, free: 366.3 MiB)\n",
      "24/03/15 03:21:33 INFO BlockManagerInfo: Added rdd_8_0 in memory on worker-node-2:41755 (size: 177.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:33 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 8912 ms on worker-node-2 (executor 1) (1/2)\n",
      "24/03/15 03:21:33 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 47949\n",
      "24/03/15 03:21:34 INFO BlockManagerInfo: Added rdd_8_1 in memory on worker-node-1:34595 (size: 177.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:34 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 9692 ms on worker-node-1 (executor 2) (2/2)\n",
      "24/03/15 03:21:34 INFO DAGScheduler: ResultStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 9.867 s\n",
      "24/03/15 03:21:34 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:21:34 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 03:21:34 INFO YarnScheduler: Killing all running tasks in stage 0: Stage finished\n",
      "24/03/15 03:21:34 INFO CodeGenerator: Code generated in 19.376417 ms\n",
      "24/03/15 03:21:34 INFO DAGScheduler: Registering RDD 13 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "24/03/15 03:21:34 INFO DAGScheduler: Got map stage job 1 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions\n",
      "24/03/15 03:21:34 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 03:21:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 03:21:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:21:34 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 03:21:34 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 37.2 KiB, free 365.9 MiB)\n",
      "24/03/15 03:21:34 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 365.8 MiB)\n",
      "24/03/15 03:21:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on master-node:36759 (size: 16.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:21:34 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:21:34 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/15 03:21:34 INFO YarnScheduler: Adding task set 1.0 with 2 tasks resource profile 0\n",
      "24/03/15 03:21:34 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (worker-node-1, executor 2, partition 1, PROCESS_LOCAL, 434765 bytes) \n",
      "24/03/15 03:21:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 3) (worker-node-2, executor 1, partition 0, PROCESS_LOCAL, 434765 bytes) \n",
      "24/03/15 03:21:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on worker-node-1:34595 (size: 16.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:34 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on worker-node-2:41755 (size: 16.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:35 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 3) in 322 ms on worker-node-2 (executor 1) (1/2)\n",
      "24/03/15 03:21:35 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 340 ms on worker-node-1 (executor 2) (2/2)\n",
      "24/03/15 03:21:35 INFO DAGScheduler: ShuffleMapStage 1 (count at NativeMethodAccessorImpl.java:0) finished in 0.371 s\n",
      "24/03/15 03:21:35 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/15 03:21:35 INFO DAGScheduler: running: Set()\n",
      "24/03/15 03:21:35 INFO DAGScheduler: waiting: Set()\n",
      "24/03/15 03:21:35 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:21:35 INFO DAGScheduler: failed: Set()\n",
      "24/03/15 03:21:35 INFO CodeGenerator: Code generated in 16.242566 ms\n",
      "24/03/15 03:21:35 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Got job 2 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Final stage: ResultStage 3 (count at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 03:21:35 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.5 KiB, free 365.8 MiB)\n",
      "24/03/15 03:21:35 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 365.8 MiB)\n",
      "24/03/15 03:21:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on master-node:36759 (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:21:35 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[16] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 03:21:35 INFO YarnScheduler: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/03/15 03:21:35 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7626 bytes) \n",
      "24/03/15 03:21:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on master-node:36759 in memory (size: 16.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:21:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on worker-node-1:34595 in memory (size: 16.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on worker-node-2:41755 (size: 6.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on worker-node-2:41755 in memory (size: 16.1 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:35 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 192.168.2.40:50090\n",
      "24/03/15 03:21:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on master-node:36759 in memory (size: 12.7 KiB, free: 366.3 MiB)\n",
      "24/03/15 03:21:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on worker-node-2:41755 in memory (size: 12.7 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:35 INFO BlockManagerInfo: Removed broadcast_1_piece0 on worker-node-1:34595 in memory (size: 12.7 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:35 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 228 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 03:21:35 INFO YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:21:35 INFO DAGScheduler: ResultStage 3 (count at NativeMethodAccessorImpl.java:0) finished in 0.260 s\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 03:21:35 INFO YarnScheduler: Killing all running tasks in stage 3: Stage finished\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Job 2 finished: count at NativeMethodAccessorImpl.java:0, took 0.302157 s\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 10000\n"
     ]
    }
   ],
   "source": [
    "num_rows = df.count()\n",
    "print(\"Number of rows in the DataFrame:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dce02ca-1e86-4451-8368-dd59fedc72b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:21:35 INFO CodeGenerator: Code generated in 32.586267 ms\n",
      "24/03/15 03:21:35 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[21] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/03/15 03:21:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 38.0 KiB, free 365.9 MiB)\n",
      "24/03/15 03:21:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 15.3 KiB, free 365.9 MiB)\n",
      "24/03/15 03:21:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on master-node:36759 (size: 15.3 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:21:35 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[21] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 03:21:35 INFO YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/03/15 03:21:35 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5) (worker-node-2, executor 1, partition 0, PROCESS_LOCAL, 434776 bytes) \n",
      "24/03/15 03:21:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on worker-node-2:41755 (size: 15.3 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:35 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 217 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 03:21:35 INFO YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:21:35 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.232 s\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 03:21:35 INFO YarnScheduler: Killing all running tasks in stage 4: Stage finished\n",
      "24/03/15 03:21:35 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.243678 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "|duration|end_of_fade_in|start_of_fade_out|loudness|mode|mode_confidence|  tempo|time_signature|time_signature_confidence|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "| 218.932|         0.247|          218.932| -11.197| 0.0|          0.636| 92.198|           4.0|                    0.778|\n",
      "| 148.035|         0.148|          137.915|  -9.843| 0.0|           0.43|121.274|           4.0|                    0.384|\n",
      "| 177.475|         0.282|          172.304|  -9.689| 1.0|          0.565| 100.07|           1.0|                      0.0|\n",
      "| 233.404|           0.0|          217.124|  -9.013| 1.0|          0.749|119.293|           4.0|                      0.0|\n",
      "| 209.606|         0.066|          198.699|  -4.501| 1.0|          0.371|129.738|           4.0|                    0.562|\n",
      "| 267.702|         2.264|           254.27|  -9.323| 1.0|          0.557|147.782|           3.0|                    0.454|\n",
      "| 114.782|         0.096|          114.782| -17.302| 1.0|            0.0|111.787|           1.0|                      0.0|\n",
      "|  189.57|         0.319|          181.023| -11.642| 0.0|           0.16| 101.43|           3.0|                    0.408|\n",
      "| 269.818|           5.3|           258.99| -13.496| 1.0|          0.652| 86.643|           4.0|                    0.487|\n",
      "| 266.396|         0.084|          261.747|  -6.697| 0.0|          0.473|114.041|           4.0|                    0.878|\n",
      "| 218.775|         2.125|          207.012| -10.021| 0.0|          0.485|146.765|           1.0|                      0.0|\n",
      "| 245.211|         0.357|           227.48|  -7.545| 1.0|          0.686|117.975|           4.0|                    0.835|\n",
      "| 226.351|           0.0|          221.553|  -6.632| 1.0|          0.305| 130.04|           4.0|                      0.0|\n",
      "| 191.843|          0.38|          188.424|   -7.75| 0.0|          0.198|137.334|           1.0|                    0.319|\n",
      "| 307.382|         0.612|          296.658|  -8.346| 1.0|          0.533|125.197|           3.0|                    0.211|\n",
      "| 491.128|           0.0|          486.034|  -8.576| 1.0|          0.829|119.826|           4.0|                    0.756|\n",
      "| 228.597|         0.223|          217.426|  -16.11| 1.0|          0.516|127.756|           5.0|                    0.579|\n",
      "| 599.249|         1.193|          591.999|  -8.032| 1.0|          0.346| 99.273|           4.0|                    0.158|\n",
      "| 290.298|         0.145|          285.605|  -5.271| 1.0|          0.756|150.062|           4.0|                    0.931|\n",
      "| 165.694|         0.162|          157.391|  -6.787| 1.0|          0.568|138.331|           4.0|                    0.127|\n",
      "+--------+--------------+-----------------+--------+----+---------------+-------+--------------+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:21:37 INFO CodeGenerator: Code generated in 30.818988 ms\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75edaf8-3e1f-4173-8f65-b8852bb3b27e",
   "metadata": {},
   "source": [
    "# Calculation (Average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81a4b687-0b00-4244-8daa-c20c0aeff80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:21:37 INFO CodeGenerator: Code generated in 77.477609 ms\n",
      "24/03/15 03:21:37 INFO DAGScheduler: Registering RDD 26 (collect at /tmp/ipykernel_302089/2441391651.py:1) as input to shuffle 1\n",
      "24/03/15 03:21:37 INFO DAGScheduler: Got map stage job 4 (collect at /tmp/ipykernel_302089/2441391651.py:1) with 2 output partitions\n",
      "24/03/15 03:21:37 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (collect at /tmp/ipykernel_302089/2441391651.py:1)\n",
      "24/03/15 03:21:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/03/15 03:21:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:21:37 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[26] at collect at /tmp/ipykernel_302089/2441391651.py:1), which has no missing parents\n",
      "24/03/15 03:21:37 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 59.6 KiB, free 365.8 MiB)\n",
      "24/03/15 03:21:37 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 21.7 KiB, free 365.8 MiB)\n",
      "24/03/15 03:21:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on master-node:36759 (size: 21.7 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:21:37 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:21:37 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[26] at collect at /tmp/ipykernel_302089/2441391651.py:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/03/15 03:21:37 INFO YarnScheduler: Adding task set 5.0 with 2 tasks resource profile 0\n",
      "24/03/15 03:21:37 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6) (worker-node-1, executor 2, partition 1, PROCESS_LOCAL, 434765 bytes) \n",
      "24/03/15 03:21:37 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 7) (worker-node-2, executor 1, partition 0, PROCESS_LOCAL, 434765 bytes) \n",
      "24/03/15 03:21:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on worker-node-2:41755 (size: 21.7 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on worker-node-1:34595 (size: 21.7 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:37 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 7) in 149 ms on worker-node-2 (executor 1) (1/2)\n",
      "24/03/15 03:21:38 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 257 ms on worker-node-1 (executor 2) (2/2)\n",
      "24/03/15 03:21:38 INFO YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:21:38 INFO DAGScheduler: ShuffleMapStage 5 (collect at /tmp/ipykernel_302089/2441391651.py:1) finished in 0.278 s\n",
      "24/03/15 03:21:38 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/03/15 03:21:38 INFO DAGScheduler: running: Set()\n",
      "24/03/15 03:21:38 INFO DAGScheduler: waiting: Set()\n",
      "24/03/15 03:21:38 INFO DAGScheduler: failed: Set()\n",
      "24/03/15 03:21:38 INFO CodeGenerator: Code generated in 58.85462 ms\n",
      "24/03/15 03:21:38 INFO SparkContext: Starting job: collect at /tmp/ipykernel_302089/2441391651.py:1\n",
      "24/03/15 03:21:38 INFO DAGScheduler: Got job 5 (collect at /tmp/ipykernel_302089/2441391651.py:1) with 1 output partitions\n",
      "24/03/15 03:21:38 INFO DAGScheduler: Final stage: ResultStage 7 (collect at /tmp/ipykernel_302089/2441391651.py:1)\n",
      "24/03/15 03:21:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\n",
      "24/03/15 03:21:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/03/15 03:21:38 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[29] at collect at /tmp/ipykernel_302089/2441391651.py:1), which has no missing parents\n",
      "24/03/15 03:21:38 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 32.7 KiB, free 365.8 MiB)\n",
      "24/03/15 03:21:38 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 365.7 MiB)\n",
      "24/03/15 03:21:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on master-node:36759 (size: 11.1 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:21:38 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585\n",
      "24/03/15 03:21:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[29] at collect at /tmp/ipykernel_302089/2441391651.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/03/15 03:21:38 INFO YarnScheduler: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "24/03/15 03:21:38 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 8) (worker-node-2, executor 1, partition 0, NODE_LOCAL, 7626 bytes) \n",
      "24/03/15 03:21:38 INFO BlockManagerInfo: Removed broadcast_3_piece0 on worker-node-2:41755 in memory (size: 6.0 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:38 INFO BlockManagerInfo: Removed broadcast_3_piece0 on master-node:36759 in memory (size: 6.0 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:21:38 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on worker-node-2:41755 (size: 11.1 KiB, free: 366.0 MiB)\n",
      "24/03/15 03:21:38 INFO BlockManagerInfo: Removed broadcast_5_piece0 on master-node:36759 in memory (size: 21.7 KiB, free: 366.2 MiB)\n",
      "24/03/15 03:21:38 INFO BlockManagerInfo: Removed broadcast_5_piece0 on worker-node-1:34595 in memory (size: 21.7 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:38 INFO BlockManagerInfo: Removed broadcast_5_piece0 on worker-node-2:41755 in memory (size: 21.7 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 192.168.2.40:50090\n",
      "24/03/15 03:21:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on master-node:36759 in memory (size: 15.3 KiB, free: 366.3 MiB)\n",
      "24/03/15 03:21:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on worker-node-2:41755 in memory (size: 15.3 KiB, free: 366.1 MiB)\n",
      "24/03/15 03:21:38 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 8) in 93 ms on worker-node-2 (executor 1) (1/1)\n",
      "24/03/15 03:21:38 INFO YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/03/15 03:21:38 INFO DAGScheduler: ResultStage 7 (collect at /tmp/ipykernel_302089/2441391651.py:1) finished in 0.134 s\n",
      "24/03/15 03:21:38 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/03/15 03:21:38 INFO YarnScheduler: Killing all running tasks in stage 7: Stage finished\n",
      "24/03/15 03:21:38 INFO DAGScheduler: Job 5 finished: collect at /tmp/ipykernel_302089/2441391651.py:1, took 0.142882 s\n"
     ]
    }
   ],
   "source": [
    "average_values = df.agg({'duration': 'avg', 'end_of_fade_in': 'avg', 'start_of_fade_out': 'avg', \n",
    "                         'loudness': 'avg', 'mode': 'avg', 'mode_confidence': 'avg', \n",
    "                         'tempo': 'avg', 'time_signature': 'avg', \n",
    "                         'time_signature_confidence': 'avg'}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09134280-7b4c-4130-a0c4-deb68bbf3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = pd.DataFrame([average_values.asDict()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e409464d-8f5b-4757-a28f-fe16b1eaef7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows in the DataFrame:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb5aa6b-0e6b-4025-930d-613a9f1ac207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg(duration)</th>\n",
       "      <th>avg(tempo)</th>\n",
       "      <th>avg(end_of_fade_in)</th>\n",
       "      <th>avg(start_of_fade_out)</th>\n",
       "      <th>avg(time_signature)</th>\n",
       "      <th>avg(time_signature_confidence)</th>\n",
       "      <th>avg(mode_confidence)</th>\n",
       "      <th>avg(mode)</th>\n",
       "      <th>avg(loudness)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>238.507519</td>\n",
       "      <td>122.915449</td>\n",
       "      <td>0.758616</td>\n",
       "      <td>229.975465</td>\n",
       "      <td>3.5648</td>\n",
       "      <td>0.509937</td>\n",
       "      <td>0.477784</td>\n",
       "      <td>0.6911</td>\n",
       "      <td>-10.485669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   avg(duration)  avg(tempo)  avg(end_of_fade_in)  avg(start_of_fade_out)  \\\n",
       "0     238.507519  122.915449             0.758616              229.975465   \n",
       "\n",
       "   avg(time_signature)  avg(time_signature_confidence)  avg(mode_confidence)  \\\n",
       "0               3.5648                        0.509937              0.477784   \n",
       "\n",
       "   avg(mode)  avg(loudness)  \n",
       "0     0.6911     -10.485669  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ca167-67d5-4b4e-857c-b11709b1698e",
   "metadata": {},
   "source": [
    "# STOP SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c23825d7-4cce-458e-82bc-b4dc6995cf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/15 03:21:38 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/03/15 03:21:38 INFO SparkUI: Stopped Spark web UI at http://master-node:4040\n",
      "24/03/15 03:21:38 INFO YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "24/03/15 03:21:38 INFO YarnClientSchedulerBackend: Shutting down all executors\n",
      "24/03/15 03:21:38 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "24/03/15 03:21:38 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped\n",
      "24/03/15 03:21:38 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/03/15 03:21:38 INFO MemoryStore: MemoryStore cleared\n",
      "24/03/15 03:21:38 INFO BlockManager: BlockManager stopped\n",
      "24/03/15 03:21:38 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/03/15 03:21:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/03/15 03:21:38 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "# release the cores\n",
    "spark_context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
